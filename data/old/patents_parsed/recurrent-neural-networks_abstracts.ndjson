["Each neural element of a column-structured recurrent neural network generates an output from input data and recurrent data provided from a context layer of a corresponding column. One or more candidates for an estimated value is obtained, and an occurrence probability is computed using an internal state by solving an estimation equation determined by the internal state output from the neural network. A candidate having the highest occurrence probability is an estimated value for unknown data. Thus, the internal state of the recurrent neural network is explicitly associated with the estimated value for data, and a data change can be efficiently estimated.", "Time-series trend estimating system and method using column-structured recurrent neural network "]
["A teaching method for a recurrent neural network having hidden, output and input neurons calculates weighting errors over a limited number of propagations of the network. This process permits the use of conventional teaching sets, such as are used with feedforward networks, to be used with recurrent networks. The teaching outputs are substituted for the computed activations of the output neurons in the forward propagation and error correction stages. Back propagated error from the last propagation is assumed to be zero for the hidden neurons. A method of reducing drift of the network with respect to a modeled process is also described and a forced cycling method to eliminate the time lag between network input and output.", "Recurrent neural networks teaching system "]
["Based on the encoding of deterministic finite-state automata (DFA) in discrete-time, second-order recurrent neural networks, an algorithm constructs an augmented recurrent neural network that encodes a FFA and recognizes a given fuzzy regular language with arbitrary accuracy.", "Deterministic encoding of fuzzy finite state automata in continuous recurrent neural networks "]
["A system for converting an analog signal into a digital data stream includes a recurrent network with a plurality of converter circuits that individually receive the same analog signal as input. The circuits then generate a plurality of spike outputs that exhibit characteristics of the analog signal. Interconnecting feedback loops from each circuit output to the input of neighboring circuits queues the plurality of spike outputs to thereby self-organize the network. A digital clock is then used to establish predetermined time intervals for counting the spike outputs to create the digital data stream.", "Analog to digital conversion using recurrent neural networks "]
["A neural network apparatus and method for use in applications such as in a voltage/reactive-power controller in which a neuro control-object simulator and a neuro controller pre-learn so as to make input-output relations of the controller match the input-output relations of a control unit and so as to make input-output relations of the simulator match input-output relations of a control object. The controller re-learns so as to make the output of the simulator match an input corresponding to a desired output of the control object. After re-learning, the controller controls the control-object.", "Control method using neural networks and a voltage/reactive-power controller for a power system using the control method "]
["An electronic engine control (EEC) module executes both open loop and closed loop neural network processes to control the air/fuel mixture ratio of a vehicle engine to hold the fuel mixture at stoichiometry. The open loop neural network provides transient air/fuel control to provide a base stoichiometric air/fuel mixture ratio signal in response to throttle position under current engine speed and load conditions. The base air/fuel mixture ratio signal from the open loop network is additively combined with a closed loop trimming signal which varies the air/fuel mixture ratio in response to variations in the sensed exhaust gas oxygen level. Each neural network function is defined by a unitary data structure which defines the network architecture, including the number of node layers, the number of nodes per layer, and the interconnections between nodes. In addition, the data structure holds weight values which determine the manner in which network signals are combined. The network definition data structures are created by a network training system which utilizes an external training processor which employs gradient methods to derive network weight values in accordance with a cost function which quantitatively defines system objectives and an identification network which is pretrained to provide gradient signals representative of the behavior of the physical plant. The training processor executes training cycles asynchronously with the operation of the EEC module in a representative test vehicle.", "Trained Neural network air/fuel control system "]
["The present invention is concerned with a signal processing system having a learning function pursuant to the back-propagation learning rule by the neural network, in which the learning rate is dynamically changed as a function of input values to effect high-speed stable learning. The signal processing system of the present invention is so arranged that, by executing signal processing for the input signals by the recurrent network formed by units each corresponding to a neuron, the features of the sequential time series pattern such as voice signals fluctuating on the time axis can be extracted through learning the coupling state of the recurrent network. The present invention is also concerned with a learning processing system adapted to cause the signal processing section formed by a neural network to undergo signal processing pursuant to the back-propagation learning rule, wherein the local minimum state in the course of the learning processing may be avoided by learning the coefficient of coupling strength while simultaneously increasing the number of the unit of the intermediate layer.", "Recurrent neural network with variable size intermediate layer "]
["A method and system for is disclosed for speech synthesis using deep neural networks. A neural network may be trained to map input phonetic transcriptions of training-time text strings into sequences of acoustic feature vectors, which yield predefined speech waveforms when processed by a signal generation module. The training-time text strings may correspond to written transcriptions of speech carried in the predefined speech waveforms. Subsequent to training, a run-time text string may be translated to a run-time phonetic transcription, which may include a run-time sequence of phonetic-context descriptors, each of which contains a phonetic speech unit, data indicating phonetic context, and data indicating time duration of the respective phonetic speech unit. The trained neural network may then map the run-time sequence of the phonetic-context descriptors to run-time predicted feature vectors, which may in turn be translated into synthesized speech by the signal generation module.", "Speech synthesis using deep neural networks "]
["Methods are developed on a digital computer for performing work order scheduling activity in a dynamic factory floor environment, in a manner which enables scheduling heuristic knowledge from a scheduler to be encoded through an adaptive learning process, thus eliminating the need to define these rules explicitly. A sequential assignment paradigm incrementally builds up a final schedule from a partial schedule, assigning each work order to appropriate resources in turns, taking advantage of the parallel processing capability of neural networks by selecting the most appropriate resource combination (i.e. schedule generation) for each work order under simultaneous interaction of multiple scheduling constraints.", "Neural network system and method for factory floor scheduling "]
["A recurrent, neural network-based fuzzy logic system includes in a rule base layer and a membership function layer neurons which each have a recurrent architecture with an output-to-input feedback path including a time delay element and a neural weight. Further included is a recurrent, neural network-based fuzzy logic rule generator wherein a neural network receives and fuzzifies input data and provides data corresponding to fuzzy logic membership functions and recurrent fuzzy logic rules.", "Recurrent neural network-based fuzzy logic system and method "]
["A method and an apparatus are disclosed for processing a measurement process to estimate a signal process. The method synthesizes realizations of a signal process and a measurement process into a primary filter for estimating the signal process and, if required, an ancillary filter for providing the primary filter's estimation error statistics. Both the primary and the ancillary filters are made out of artificial recurrent neural networks (RNNs). Their implementation results in the filtering apparatus. The synthesis is performed through training RNNs. The weights/parameters and initial dynamic state of an RNN are determined by minimizing a training criterion by the variation of the same. The training criterion, which is constructed on the basis of a selected estimation error criterion, incorporates the aforementioned realizations. An alternative way to determine the initial dynamic state of an RNN is to simply set it equal to a canonical initial dynamic state. After adequate training, both the primary and the ancillary filters are recursive filters optimal for the given respective RNN architectures with the lagged feedbacks carrying the optimal conditional statistics. If appropriate RNN paradigms and estimation error criteria are selected, the primary and the ancillary filters of such paradigms are proven to approximate the respective optimal filters in performance (with respect to the selected estimation error criteria) to any desired degree of accuracy, provided that the RNNs that constitute the primary and ancillary filters are of sufficient sizes.", "Optimal filtering by recurrent neural networks "]
["A gas turbine control system includes a controller that is coupled to actuator systems that govern operation of the gas turbine. The controller includes a processor for generating respective actuator control signals in correspondence with a plurality of turbine operating condition signals; the controller includes at least one neural network estimator that is trained to generate an estimated turbine operating condition signal. The neural network estimator typically has one or more hidden neuron layers that are coupled together in a feedforward structure, a recurrent neural network architecture. The estimated turbine operating condition signal generated by the neural network estimator typically, but not necessarily, represents a turbine internal cycle operating parameter for which the turbine has no corresponding operating parameter sensor.", "Controller with neural network for estimating gas turbine internal cycle parameters "]
["Any deterministic finite-state automata (DFA) can be implemented in a sparse recurrent neural network (RNN) with second-order weights and sigmoidal discriminant functions. Construction algorithms can be extended to fault-tolerant DFA implementations such that faults in an analog implementation of neurons or weights do not affect the desired network performance. The weights are replicated k times for k-1 fault tolerance. Alternatively, the independent network is replicated 2k+1 times and the majority of the outputs is used for a k fault tolerance. In a further alternative solution, a single network with k\u03b7 neurons uses a \"n choose k\"encoding algorithm for k fault tolerance.", "Fault-tolerant implementation of finite-state automata in recurrent neural networks "]
["A recurrent, neural network-based fuzzy logic system includes neurons in a rule base layer which each have a recurrent architecture with an output-to-input feedback path including a time delay element and a neural weight. Further included is a neural network-based, fuzzy logic finite state machine wherein the neural network-based, fuzzy logic system has a recurrent architecture with an output-to-input feedback path including at least a time delay element. Still further included is a recurrent, neural network-based fuzzy logic rule generator wherein a neural network receives and fuzzifies input data and provides data corresponding to fuzzy logic membership functions and recurrent fuzzy logic rules.", "Recurrent neural network-based fuzzy logic system "]
["Deep recurrent neural networks applied to speech recognition. The deep recurrent neural networks (RNNs) are preferably implemented by stacked long short-term memory bidirectional RNNs. The RNNs are trained using end-to-end training with suitable regularization.", "System and method for speech recognition using deep recurrent neural networks "]
["A neural network based universal time series prediction system for financial securities includes a pipelined recurrent ANN architecutre having a plurality of identical modules to first adjust internal weights and biases in response to a first training set representing a nonlinear financial time series of samples of a financial quantity and a target value, and then determine and store an estimated prediction error of the ANN in order to adjust short time stock price predictions in accordance with the stored prediction error. The prediction system is also designed to output upper and lower prediction bounds within a confidence region.", "Artificial neural network based universal time series "]
[null, "Adaptive control method of dissolved oxygen (DO) based on recurrent neural network (RNN) model "]
["A method and apparatus is provided for processing a measurement process to estimate a signal process, even if the signal and/or measurement processes have large and/or expanding ranges. The method synthesizes training data comprising realizations of the signal and measurement processes into a primary filter for estimating the signal process and, if required, an ancillary filter for providing the primary filter's estimation error statistics. The primary and ancillary filters each comprise an artificial recurrent neural network (RNN) and at least one range extender or reducer. Their implementation results in the filtering apparatus. Many types of range extender and reducer are disclosed, which have different degrees of effectiveness and computational cost. For a neural filter under design, range extenders and/or reducers are selected from those types jointly with the architecture of the RNN in consideration of the filtering accuracy, the RNN size and the computational cost of each selected range extender and reducer so as to maximize the cost effectiveness of the neural filter. The aforementioned synthesis is performed through training RNNs together with range extenders and/or reducers.", "Optimal filtering by neural networks with range extenders and/or reducers "]
["Neural networks for optimal estimation (including prediction) and/or control involve an execution step and a learning step, and are characterized by the learning step being performed by neural computations. The set of learning rules cause the circuit's connection strengths to learn to approximate the optimal estimation and/or control function that minimizes estimation error and/or a measure of control cost. The classical Kalman filter and the classical Kalman optimal controller are important examples of such an optimal estimation and/or control function. The circuit uses only a stream of noisy measurements to infer relevant properties of the external dynamical system, learn the optimal estimation and/or control function, and apply its learning of this optimal function to input data streams in an online manner. In this way, the circuit simultaneously learns and generates estimates and/or control output signals that are optimal, given the network's current state of learning.", "Neural networks for prediction and control "]
["A learning system is provided, which includes network storage means for storing a network including a plurality of nodes, each of which holds a dynamics; and learning means for self-organizationally updating the dynamics of the network on the basis of measured time-series data.", "Apparatus and method for embedding recurrent neural networks into the nodes of a self-organizing map "]
[null, "FPGA implementation method based on LS-SVM classification and recurrence learning recurrence neural network "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for identifying the language of a spoken utterance. One of the methods includes receiving input features of an utterance; and processing the input features using an acoustic model that comprises one or more convolutional neural network (CNN) layers, one or more long short-term memory network (LSTM) layers, and one or more fully connected neural network layers to generate a transcription for the utterance.", "Convolutional, long short-term memory, fully connected deep neural networks "]
["An apparatus based on an n-variable unlimited recurrent adjustable network (URANn) comprises: two layers, each layer having the same number (n) of neuron elements; linear neuron elements xi constituting a first layer; nonlinear artificial neuron elements yj having respective temperature-dependent parameters Tj and constituting a second layer. Each linear and nonlinear neuron element of the first and second layers is connected using a feedforward connection part, a recurrent connection part, and an auto connection part. A nonlinear oscillation apparatus having the recurrent neural network is generally operated in accordance with the equation (1) described below: ##EQU1##", "Apparatus based on n-variable unlimited recurrent adjustable network "]
["The invention relates to a method for computer-assisted learning of one or more neural networks based on a time series of data comprising first values at subsequent time steps. Furthermore, the invention relates to a method for computer-assisted prediction of values of a time series based on one or more neural networks. According to the invention, a rate of change of an initial time series of data is calculated, thus resulting in derivative values. Those derivative values are subjected to Empirical Mode Decomposition which is a well-known technique. The modes extracted by this Empirical Mode Decomposition as well as some time-lagged values of the time series are used as inputs for the neural networks whereas the output of those networks are future values of the time series which are to be predicted. Those networks are trained and used for prediction of future values of the time series. The invention provides good prediction results because derivative values are used as inputs and also past values are considered as inputs. The invention may be used for predicting values of any time series and is particularly for predicting the dynamics of a technical system, e.g. for predicting vibrations occurring in a technical system.", "A method for computer-assisted learning of one or more neural networks "]
["A recognition apparatus and method using a neural network is provided. A neuron-like element stores a value of its inner condition. The neuron-like element also updates a values of its internal status on the basis of an output from the neuron-like element itself, outputs from other neuron-like elements and an external input, and an output value generator a value of its internal status into an external output. Accordingly, the neuron-like element itself can retain the history of input data. This enables the time series data, such as speech, to be processed without providing any special devices in the neural network.", "Recognition apparatus using neural network, and learning method therefor "]
["A electronic engine control (EEC) module executes a neural network processing program to control the idle speed of an internal combustion engine by controlling the bypass air (throttle duty cycle) and the engine's ignition timing. The neural network is defined by a unitary data structure which defmes the network architecture, including the number of node layers, the number of nodes per layer, and the interconnections between nodes. To achieve idle speed control, the neural network processes input signals indicating the current operating state of the engine, including engine speed, the intake mass air flow rate, a desired engine speed, engine temperature, and other variables which influence engine speed, including loads imposed by power steering and air conditioning systems. The network definition data structure holds weight values which determine the manner in which network signals, including the input signals, are combined. The network definition data structures are created by a network training system which utilizes an external training processor which employ dynamic gradient methods to derive network weight values in accordance with a cost function which quantitatively defines system objectives and an identification network which is pretined to provide gradient signals representative of the behavior of the physical plant. The training processor executes training cycles asynchronously with the operation of the EEC module in a representative test vehicle.", "Trained neural network engine idle speed control system "]
["An apparatus for transforming a voice signal of a talker into a voice signal having characteristics of a different person provides apparatus for separating the talker's voice signal into a plurality of voice parameters including frequency components, a neural network for transforming at least some of the separated frequency components into those characteristic of the different person, and apparatus for combining the voice parameters for reconstituting the talker's voice signal having characteristics of the different person.", "Apparatus for transforming voice using neural networks "]
["Sound processing techniques using recurrent neural networks are described. In one or more implementations, temporal dependencies are captured in sound data that are modeled through use of a recurrent neural network (RNN). The captured temporal dependencies are employed as part of feature extraction performed using nonnegative matrix factorization (NMF). One or more sound processing techniques are performed on the sound data based at least in part on the feature extraction.", "Non-negative Matrix Factorization Regularized by Recurrent Neural Networks for Audio Processing "]
[null, "Electrocardiograph detection method based on quantum simple recursion neural network "]
["Methods, computer-readable media, and systems are provided for machine learning in a simultaneous recurrent neural network. One embodiment of the invention provides a method including initializing one or more weight in the network, initializing parameters of an extended Kalman filter, setting a Jacobian matrix to an empty matrix, augmenting the Jacobian matrix for each of a plurality of training patterns, adjusting the one or more weights using the extended Kalman filter formulas, and calculating a network output for one or more testing patterns.", "Methods of improved learning in simultaneous recurrent neural networks "]
["A method for identifying engine combustion failure of an internal combustion engine having a plurality of cylinders, a crankshaft and a crankshaft position sensor includes the steps of operating the internal combustion engine to rotate the crankshaft, measuring rotational quantities of the crankshaft corresponding to events created by each of the plurality of cylinders during operation of the internal combustion engine, correcting the rotational quantities measured to remove periodic position irregularities to generate a corrected temporal signal, generating an acceleration signal of the crankshaft using the corrected temporal signals, and identifying combustion failures as a function of the acceleration signal. A time-lagged recurrent neural network utilizes the acceleration signal, along with other engine parameters to identify the cylinder-specific misfire events.", "Method for identifying misfire events of an internal combustion engine "]
["Technologies pertaining to slot filling are described herein. A deep neural network, a recurrent neural network, and/or a spatio-temporally deep neural network are configured to assign labels to words in a word sequence set forth in natural language. At least one label is a semantic label that is assigned to at least one word in the word sequence.", "Assignment of semantic labels to a sequence of words using neural network architectures "]
["A system is described herein which uses a neural network having an input layer that accepts an input vector and a feature vector. The input vector represents at least part of input information, such as, but not limited to, a word or phrase in a sequence of input words. The feature vector provides supplemental information pertaining to the input information. The neural network produces an output vector based on the input vector and the feature vector. In one implementation, the neural network is a recurrent neural network. Also described herein are various applications of the system, including a machine translation application.", "Feature-Augmented Neural Networks and Applications of Same "]
["Disclosed is a an integrated circuit method and system for generating a compiler to map a code set to object code capable of being executed on an operating system platform. The integrated circuit is encoded with logic including at least one neural network. The at least one neural network in the integrated circuit is trained to convert the code set to object code. The at least one trained neural network is then used to convert the code set to object code.", "Method and system for converting code to executable code using neural networks implemented in a very large scale integration (VLSI) integrated circuit "]
["A method for the supervised teaching of a recurrent neutral network (RNN) is disclosed. A typical embodiment of the method utilizes a large (50 units or more), randomly initialized RNN with a globally stable dynamics. During the training period, the output units of this RNN are teacher-forced to follow the desired output signal. During this period, activations from all hidden units are recorded. At the end of the teaching period, these recorded data are used as input for a method which computes new weights of those connections that feed into the output units. The method is distinguished from existing training methods for RNNs through the following characteristics: (1) Only the weights of connections to output units are changed by learning\u2014existing methods for teaching recurrent networks adjust all network weights. (2) The internal dynamics of large networks are used as a \u201creservoir\u201d of dynamical components which are not changed, but only newly combined by the learning procedure\u2014existing methods use small networks, whose internal dynamics are themselves completely re-shaped through learning.", "Method for supervised teaching of a recurrent artificial neural network "]
["Systems and methods for training networks are provided. A method for training networks comprises receiving an input from each of a plurality of neural networks differing from each other in at least one of architecture, input modality, and feature type, connecting the plurality of neural networks through a common output layer, or through one or more common hidden layers and a common output layer to result in a joint network, and training the joint network.", "Method and system for joint training of hybrid neural networks for acoustic modeling in automatic speech recognition "]
["A method of automating the calibration of lookup tables containing correction values to be used in an on-board vehicle system is disclosed. The method includes training a neural network to model engine behavior by outputting cylinder specific crankshaft acceleration correction values in response to any engine speed and load input conditions. The correction values generated are stored in a memory device. The training takes place off-board the vehicle, using a data set previously obtained from operating a representative engine under normal operating conditions.", "Method of generation correction tables for misfire detection using neural networks "]
["A electronic engine control (EEC) module executes a generic neural network processing program to perform one or more neural network control funtions. Each neural network funtion is defined by a unitary data structure which defines the network architecture, including the number of node layers, the number of nodes per layer, and the interconnections between nodes. In addition, the data structure holds weight values which determine the manner in which network signals are combined. The network definition data structures are created by a network training system which utilizes an external training processor which employs gradient methods to derive network weight values in accordance with a cost function which quantitatively defines system objectives and an identification network which is pretrained to provide gradient signals representative the behavior of the physical plant. The training processor executes training cycles asynchronously with the operation of the EEC module in a representative test vehicle.", "Generic neural network training and processing system "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for recognizing speech using neural networks. One of the methods includes receiving an audio input; processing the audio input using an acoustic model to generate a respective phoneme score for each of a plurality of phoneme labels; processing one or more of the phoneme scores using an inverse pronunciation model to generate a respective grapheme score for each of a plurality of grapheme labels; and processing one or more of the grapheme scores using a language model to generate a respective text label score for each of a plurality of text labels.", "Recognizing speech using neural networks "]
["Disclosed is a system, method, and program for generating a compiler to map a code set to object code capable of being executed on an operating system platform. At least one neural network is trained to convert the code set to object code. The at least one trained neural network can then be used to convert the code set to the object code.", "Method, system, and program for converting code to executable code using neural networks implemented in a software program "]
["A chaotic recurrent neural network includes N chaotic neural networks for receiving an external input and the outputs of N-1 chaotic neural networks among said N chaotic neural networks and performing an operation according to the following dynamic equation ##EQU1## wherein Wij is a synapse connection coefficient of the feedback input from the \"j\"th neuron to the \"i\"th neuron, Xi (t) is the output of the \"i\"th neuron at time t, and \u03b3i, \u03b1 and and k are a time-delaying constant, a non-negative parameter and a refractory time attenuation constant, respectively, and wherein Zi (t) represents Xi (t) when i belongs to the neuron group I and represents ai (t) when i belongs to the external input group E. Also, a learning algorithm for the chaotic recurrent neural network increases its learning efficiency.", "Chaotic recurrent neural network and learning method therefor "]
[null, "Device for analyzing time sequence based on recurrent neural network and its method "]
["An intrusion detection system (IDS) that uses application monitors for detecting application-based attacks against computer systems. The IDS implements application monitors in the form of a software program to learn and monitor the behavior of system programs in order to detect attacks against computer hosts. The application monitors implement machine learning algorithms to provide a mechanism for learning from previously observed behavior in order to recognize future attacks that it has not seen before. The application monitors include temporal locality algorithms to increased the accuracy of the IDS. The IDS of the present invention may comprise a string-matching program, a neural network, or a time series prediction algorithm for learning normal application behavior and for detecting anomalies.", "Computer intrusion detection system and method based on application monitoring "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating parse trees for input text segments. One of the methods includes obtaining an input text segment, processing the input text segment using a first long short term memory (LSTM) neural network to convert the input text segment into an alternative representation for the input text segment, and processing the alternative representation for the input text segment using a second LSTM neural network to generate a linearized representation of a parse tree for the input text segment.", "Generating parse trees of text segments using neural networks "]
["This control method is suitable to the control of voltage reactive-power of a power system. The method comprises a step of connecting a neuro controller in parallel to a control unit and connecting a neural network control-object simulator for simulating the object in parallel to the object, a step of letting the neural networks of the neuro controller and of the simulator go through a pre-learning phase so as to make input-output relations of the controller match those of the control unit, and input-output relations of the simulator match those of the object, a step of connecting the output of the controller to the input of the simulator, a step of letting the neural network of the controller go through a learning phase so as to make the output of the simulator match the input of the controller, and a step of letting the controller control the object after completing the learning phase.", "A control method using neural networks and a voltage/reactive power controller for a power system "]
["An \u201cInterestingness Modeler\u201d uses deep neural networks to learn deep semantic models (DSM) of \u201cinterestingness.\u201d The DSM, consisting of two branches of deep neural networks or their convolutional versions, identifies and predicts target documents that would interest users reading source documents. The learned model observes, identifies, and detects naturally occurring signals of interestingness in click transitions between source and target documents derived from web browser logs. Interestingness is modeled with deep neural networks that map source-target document pairs to feature vectors in a latent space, trained on document transitions in view of a \u201ccontext\u201d and optional \u201cfocus\u201d of source and target documents. Network parameters are learned to minimize distances between source documents and their corresponding \u201cinteresting\u201d targets in that space. The resulting interestingness model has applicable uses, including, but not limited to, contextual entity searches, automatic text highlighting, prefetching documents of likely interest, automated content recommendation, automated advertisement placement, etc.", "Modeling interestingness with deep neural networks "]
["A system and method of detecting an aberrant message is provided. An ordered set of words within the message is detected. The set of words found within the message is linked to a corresponding set of expected words, the set of expected words having semantic attributes. A set of grammatical structures represented in the message is detected, based on the ordered set of words and the semantic attributes of the corresponding set of expected words. A cognitive noise vector comprising a quantitative measure of a deviation between grammatical structures represented in the message and an expected measure of grammatical structures for a message of the type is then determined. The cognitive noise vector may be processed by higher levels of the neural network and/or an external processor.", "Intelligent control with hierarchical stacked neural networks "]
[null, "Hardware circuit of recursive neural network of LS-SVM classification and returning study and implementing method "]
["A \u201cvirtual string\u201d is generated for synthesizing sound produced by plucked-string instruments using recurrent neural networks. The disclosed recurrent neural network, called a Scattering Recurrent Network (SRN), is based on the physics of waves traveling in the string. Vibration measured from a plucked string is used as the training data for the SRN. The trained SRN is a virtual model capable of generating tones similar to the tones generated by the physical string. As with a real string, the \u201cvirtual string\u201d corresponding to the SRN responds differently to different types of string \u201cplucking\u201d motions.", "Method and apparatus of synthesizing plucked string instruments using recurrent neural networks "]
["In a learning method for training a recurrent neural network having a number of inputs and a number of outputs with at least one output being connected via a return line to an input, the return line is separated during training of the neural network, thereby freeing the input connected to the return line for use as an additional input during training, together with the other inputs. The additional input values, which must be estimated or predicted for supply to the thus-produced additional training inputs, are generated by treating each additional input value to be generated as a missing value in the time series of input quantities. Error distribution densities for the additional input values are calculated on the basis of the known values from the time series and their known or predetermined error distribution density, and samples are taken from this error distribution density according to the Monte Carlo method. These each lead to an estimated or predicted value whose average is introduced for the additional input value to be predicted. The method can be employed for the operation as well as for the training of the neural network, and is suitable for use in all known fields of utilization of neural networks.", "Learning method for a neural network "]
["In a method and arrangement for the neural modelling of a dynamic system with non-linear stochastic behavior wherein only a few measured values of the influencing variable are available and the remaining values of the time series are modelled, a combination of a non-linear computerized recurrent neural predictive network and a linear error model are employed to produce a prediction with the application of maximum likelihood adaption rules. The computerized recurrent neural network can be trained with the assistance of the real-time recurrent learning rule, and the linear error model is trained with the assistance of the error model adaption rule that is implemented on the basis of forward-backward Kalman equations. This model is utilized in order to predict values of the glucose-insulin metabolism of a diabetes patient.", "Method and arrangement for the neural modelling of a dynamic system with non-linear stochastic behavior "]
["A method comprising capturing current and past data frames of a vehicle scenery with an automotive imaging sensor, and predicting, by means of a recurrent neural network, the future position of a moving object in the vehicle scenery based on the current and past data frames.", "Method and electronic device "]
["Techniques for reconstructing a signal encoded with a time encoding machine (TEM) using a recurrent neural network including receiving a TEM-encoded signal, processing the TEM-encoded signal, and reconstructing the TEM-encoded signal with a recurrent neural network.", "Encoding and decoding machine with recurrent neural networks "]
["The overall invention categorizes patients with suspected acute myocardial infarction (AMI) with regard to a) AMI/non-AMI; b) infarct size (e.g. Major/Minor); c) time since onset of infarction; and d) non-AMI with/without minor myocardial damage (MMD). Generally, the above categorization is based on frequent timed blood sampling and measurement of selected biochemical markers of AMI with different rates of appearance in circulating blood. The computations are performed by using specially designed artificial neural networks. According to a first main aspect of the invention, early, i.e. generally within 3 hours from admission of the patient, detection/exclusion of acute myocardial infarction is provided. Furthermore, early prediction of the infarct size and early estimation of the time from onset are also provided.", "Detection/exclusion of acute myocardial infarction using neural network analysis of measurements of biochemical markers "]
["A neural network (100) has an input layer, a hidden layer, and an output layer. The neural network stores weight values which operate on data input at the input layer to generate output data at the output layer. An error computing unit (87) receives the output data and compares it with desired output data from a learning data storage unit (105) to calculate error values representing the difference. An error gradient computing unit (81) calculates an error gradient, i.e. rate and direction of error change. A ratio computing unit (82) computes a ratio or percentage of a prior conjugate vector and combines the ratio with the error gradient. A conjugate vector computing unit (83) generates a present line search conjugate vector from the error gradient value and a previously calculated line search gradient vector. A line search computing unit (95) includes a weight computing unit (88) which calculates a weight correction value. The weight correction value is compared (18) with a preselected maximum or upper limit correction value (\u03ba). The line search computing unit (95) limits adjustment of the weight values stored in the neural network in accordance with the maximum weight correction value.", "Learning method and apparatus for neural networks and simulator with neural network "]
["A neural network IC 31 includes n dedicated processing elements (PEs) 62, an output register 66 for storing the PEs' outputs so that they are immediately accessible to all of the PEs, a number of output circuits 78 that are connected to selected PEs to provide binary outputs, and a timing circuit 74. Each of the PEs includes a weight memory 90 for storing input, output and bias weight arrays, a first in first out (FIFO) memory 88 for storing input data, a dot product circuit 92 and an activation circuit 94. The dot product circuit computes a dot product of the input weight array and the contents of the FIFO memory, a dot product of the output weight array and the contents of the output register, a dot product of the bias value and a constant, and sums the three results. The activation circuit maps the output of the dot product circuit through an activation function to produce the PE's output. The inclusion of a memory 90 that stores both input and output weight arrays in conjunction with the output register 66 allows the PEs to be configured to implement arbitrary feed-forward and recurrent neural network architectures.", "Configurable neural network integrated circuit "]
["A method and apparatus for supervised neural learning of time dependent trajectories exploits the concepts of adjoint operators to enable computation of the gradient of an objective functional with respect to the various parameters of the network architecture in a highly efficient manner. Specifically, it combines the advantage of dramatic reductions in computational complexity inherent in adjoint methods with the ability to solve two adjoint systems of equations together forward in time. Not only is a large amount of computation and storage saved, but the handling of real-time applications becomes also possible. The invention has been applied it to two examples of representative complexity which have recently been analyzed in the open literature and demonstrated that a circular trajectory can be learned in approximately 200 iterations compared to the 12000 reported in the literature. A figure eight trajectory was achieved in under 500 iterations compared to 20000 previously required. The trajectories computed using our new method are much closer to the target trajectories than was reported in previous studies.", "Neural network training by integration of adjoint systems of equations forward in time "]
["A method of processing information is provided. The method involves receiving a message; processing the message with a trained artificial neural network based processor, having at least one set of outputs which represent information in a non-arbitrary organization of actions based on an architecture of the artificial neural network based processor and the training; representing as a noise vector at least one data pattern in the message which is incompletely represented in the non-arbitrary organization of actions; analyzing the noise vector distinctly from the trained artificial neural network; searching at least one database; and generating an output in dependence on said analyzing and said searching.", "Intelligent control with hierarchical stacked neural networks "]
["A method of training neural systems and estimating regression coefficients of regression models with respect to an error criterion is disclosed. If the error criterion is a risk-averting error cri- terion, the invented method performs the training/estimation by starting with a small value of the risk-sensitivity index of the risk-averting error criterion and gradually increasing it to ensure numerical feasibility. If the error criterion is a risk-neutral error criterion such as a standard sum- of-squares error criterion, the invented method performs the training/estimation first with respect to a risk-averting error criterion associated with the risk-neutral error criterion. If the result is not satisfactory for the risk-neutral error criterion, further training/estimation is performed either by continuing risk-averting training/estimation with decreasing values of the associated risk-averting error criterion or by training/estimation with respect to the given risk-neutral error criterion or by both.", "Risk-averting method of training neural networks and estimating regression models "]
["The present invention relates to an artificial neural network (ANN) representation for system dynamics models (SDMs) and its applications in model construction and policy design. It first shows that, by a special design of the mapping scheme, a given flow diagram (FD) (i.e., traditional representation) can be transformed into a corresponding model in the representation of partial recurrent networks (PRNs) that will correctly behave like the one it mimics. The present invention shows the equivalence of the two types of representations, both structurally and mathematically. With the additional representation, an automatic learning method that can assist in the construction of SDMs is proposed, which starts from an initial skeleton of a PRN (mapping from an initial FD), identifies the cause-effect relationships within the SDM by neural learning, and then converts it back to the corresponding FD. The composite approach makes model construction simpler and more systematic. Similarly, by assigning an intended behavior pattern as a set of training examples for a given SDM, it can learn a new system structure with the PRN representation; the differences between the original and new structures lead to considerations of policy design. Besides, one can also allow the learning process to restart after some period of using a model so that it has a chance to evolve and adapt to temporal changes in the environment. This touches an area that has not yet been well solved; i.e., feedback to a system might change not only its behavior but also the internal system structure since, for example, a social system is usually organic.", "Neural network representation for system dynamics models, and its applications "]
["An engine diagnostic system includes a bit-serial based recurrent neuroprocessor for processing data from an internal combustion engine in order to diagnose misfires in real-time and reduces the number of neurons required to perform the task by time multiplexing groups of neurons from a candidate pool of neurons to achieve the successive hidden layers of the recurrent network topology.", "Automotive engine misfire detection system including a bit-serial based recurrent neuroprocessor "]
["A system for forecasting predicted thermal loads for a building comprises a thermal condition forecaster for forecasting weather conditions to be compensated by a building environmental control system and a thermal load predictor for modeling building environmental management system components to generate a predicted thermal load for a building for maintaining a set of environmental conditions. The thermal load predictor of the present invention is a neural network and, preferably, the neural network is a recurrent neural network that generates the predicted thermal load from short-term data. The recurrent neural network is trained by inputting building thermal mass data and building occupancy data for actual weather conditions and comparing the predicted thermal load generated by the recurrent neural network to the actual thermal load measured at the building. Training error is attributed to weights of the neurons processing the building thermal mass data and building occupancy data. Iteratively adjusting these weights to minimize the error optimizes the design of the recurrent neural network for these non-weather inputs.", "System and method for predicting building thermal loads "]
["Methods and systems for recognizing speech include receiving information reflecting the speech, determining at least one broad-class of the received information, classifying the received information based on the determined broad-class, selecting a model based on the classification of the received information, and recognizing the speech using the selected model and the received information.", "Speech recognition involving a neural network "]
[null, "Failure recognition method and system based on neural network self-learning "]
["A method and system for implementing a neuro-controller. One example of a neuro-controller is a brain-like stochastic search. Another example is a neuro-controller for controlling a hypersonic aircraft. Using a variety of learning techniques, the method and system provide adaptable control of external devices (e.g., airplanes, plants, factories, and financial systems).", "Neural networks for intelligent control "]
["A method for a computer-aided control of a technical system is provided. The method involves use of a cooperative learning method and artificial neural networks. In this context, feed-forward networks are linked to one another such that the architecture as a whole meets an optimality criterion. The network approximates the rewards observed to the expected rewards as an appraiser. In this way, exclusively observations which have actually been made are used in optimum fashion to determine a quality function. In the network, the optimum action in respect of the quality function is modeled by a neural network, the neural network supplying the optimum action selection rule for the given control problem. The method is specifically used to control a gas turbine.", "Method for computer-aided control and/or regulation using neural networks "]
["A system for bearinqs-only contact state estimation in response to target bearing and ownship speed and course information provided for a plurality of observation legs at successive points in time, includes a plurality of neural networks and a data fusion circuit. Each of the neural networks generates range-normalized parameter estimate information for one of the observation legs in response to target bearing and ownship speed and course information for an associated one of the observation legs, provided thereto at each point in time and information generated for the previous point in time. The data fusion system receives the range-normalized parameter estimate information from the neural networks and generates the contact state estimate in response thereto.", "System for bearing-only contact state estimation using recurrent neural networks "]
["A function for compensating an error between a teacher signal and an output signal with a weight value is defined. A multilayered neural network is changed so that the function becomes minimum. Thus, the multilayered neural network can be adaptively controlled.", "System for controlling an object and medium using neural networks "]
[null, "Deep long-term and short-term memory recurrent neural network acoustic model establishing method based on selective attention principles "]
["Described herein are systems and methods that exploit hierarchical Recurrent Neural Networks (RNNs) to tackle the video captioning problem; that is, generating one or multiple sentences to describe a realistic video. Embodiments of the hierarchical framework comprise a sentence generator and a paragraph generator. In embodiments, the sentence generator produces one simple short sentence that describes a specific short video interval. In embodiments, it exploits both temporal- and spatial-attention mechanisms to selectively focus on visual elements during generation. In embodiments, the paragraph generator captures the inter-sentence dependency by taking as input the sentential embedding produced by the sentence generator, combining it with the paragraph history, and outputting the new initial state for the sentence generator.", "Systems and methods for video paragraph captioning using hierarchical recurrent neural networks "]
["Using a recurrent neural network (RNN) that has been trained to a satisfactory level of performance, highly discriminative features can be extracted by running a sample through the RNN, and then extracting a final hidden state hi, where i is the number of instructions of the sample. This resulting feature vector may then be concatenated with the other hand-engineered features, and a larger classifier may then be trained on hand-engineered as well as automatically determined features. Related apparatus, systems, techniques and articles are also described.", "Recurrent neural networks for malware analysis "]
["A method and apparatus for the prediction of time series data, specifically, the prediction of a foreign currency exchange rate. The method disclosed transforms the time series data into a difference of a series, compresses the transformed data using a log transformation, converts the compressed data into symbols, and subsequently trains one or more neural networks on the symbols such that a prediction is generated. Alternative embodiments demonstrate the conversion by a self-organizing map and training by a recurrent neural network.", "Method and apparatus for foreign exchange rate time series prediction and classification "]
["A method for constructing and training a discrete-time recurrent neural network for predicting network inputs is provided. A main recurrent neural network is constructed, formed from a plurality of nodes. Each node hosts a local recurrent neural network formed of a plurality of connected units. The units are connected by weighted connections. A local shadow recurrent neural network is constructed on each node. The local shadow recurrent neural network is a copy of the local recurrent neural network on the respective node, however with certain restrictions on its connection with other nodes. The main recurrent neural network is trained to determine the weights of each connection on each node to provide a local output on each node correlating to a prediction of the local input on the respective node. The training includes, for each discrete time step and on each node: feeding a local input to the local recurrent neural network to cause local network activations; feeding a training input to the local shadow recurrent neural network and applying learning rules to determine connection weights on the local shadow recurrent neural network. The determined connection weights from the local shadow network are copied to the local network.", "Construction and training of a recurrent neural network "]
["In an automatic external defibrillator (AED) having a ventricular fibrillation detector, the ventricular fibrillation detector may generally be defined as a filter containing both an adaptive non-linear section and a linear section. The non-linear section is preferably a complex-domain neural network that can be trained to differentiate between various rhythm patterns and produce linear data for input to the linear section. The linear section is preferably an ongoing, continuous operation based on a sliding window of a predetermined time period, e.g., a tapped time-delay filter. In combination the non-linear section and linear section of the filter operate to detect and extract artifacts from a patient's ECG signal in a substantially accurate fashion so that the determination to deliver a defibrillation pulse may be accurately made.", "Automatic external defibrillator having a ventricular fibrillation detector "]
["An artificial intelligence system is used with a conglomeration of fluorescence data to provide a method of improving recognition of an unknown from its spectral pattern. Customized neural network systems allow the ultimate organization and resourceful use of assumption-free variables already existing in a total scanning fluorescence database for a much more comprehensive, discrete and accurate differentiation and matching of spectra than is possible with human memory. The invention provides increased speed of fingerprinting analysis, accuracy and reliability together with a decreased learning curve and heightened objectivity for the analysis.", "Interpretation of fluorescence fingerprints of crude oils and other hydrocarbon mixtures using neural networks "]
["There are equipped a first recurrent neural network formed by connecting plural nodes so as to have a loop in which the output from one node is input to another node in accordance with a predetermined coupling weight coefficient. Meanwhile, the output of at least one node is fed back to the node concerned or another node, and an optimizing unit for determining the optimum solution of the coupling weight coefficient in the first recurrent neural network based on a learning rule using a hereditary algorithm. In this case, the first recurrent neural network outputs a first parameter indicating a motion state of a vehicle based on predetermined input information, thereby functioning as a vehicle motion model.", "Vehicle motion model generating device and method for generating vehicle motion model "]
["The invention relates to a method for controlling a turbine which is characterized by a hidden state at each point in time of the control. The dynamic behavior of the turbine is modeled using a recurrent neural network comprising a recurrent hidden layer. The recurrent hidden layer is formed by vectors of neurons which describe the hidden state of the turbine at the points in time of the control. For each point in time, two vectors are connected chronologically to a first connection which bridges one point in time, and two vectors are additionally connected chronologically to a second connection which bridges at least two points in time. Short-term effects can be corrected by means of the first connections, and long-term effects can be corrected by means of the second connections. Emissions and occurring dynamics can be minimized in the turbine by means of the latter. The invention further relates to a control device and a turbine comprising such a control device.", "Controlling a Turbine with a Recurrent Neural Network "]
["An adaptive neural system (ANS) disclosed herein comprises a processor and an adaptor. The processor includes mainly a neural network whose adjustable weights are divided into nonadaptively and adaptively adjustable weights. The nonadaptively adjustable weights are determined through minimizing or reducing a nonadaptive training criterion in an off-line nonadaptive training. Being constructed with a priori training data, the nonadaptive training criterion is a function of the nonadaptively adjustable weights and the diversity variables associated with typical values of the environmental parameter. During an operation of the adaptive neural system, only the adaptively adjustable weights are adjusted on-line to adapt to the unknown environmental parameter. This adaptive training is achieved by minimizing or reducing an adaptive training criterion. The nonadaptive training allows the ANS to make full advantage of a priori information about the ANS's operating environment and helps the ANS focus on learning about and adapting to the unknown environmental parameter during the adaptive training. In many applications, the adaptively adjustable weights can be selected, without adversely affecting the ANS's performance, such that they appear quadratically in the adaptive training criterion. In this case, the adaptive training criterion has no undesirable local minima and the existing fast algorithms for adaptive linear filters are applicable to the adaptive training.", "Nonadaptively trained adaptive neural systems "]
["A spoken language understanding (SLU) system receives a sequence of words corresponding to one or more spoken utterances of a user, which is passed through a spoken language understanding module to produce a sequence of intentions. The sequence of words are passed through a first subnetwork of a multi-scale recurrent neural network (MSRNN), and the sequence of intentions are passed through a second subnetwork of the multi-scale recurrent neural network (MSRNN). Then, the outputs of the first subnetwork and the second subnetwork are combined to predict a goal of the user.", "Method for using a Multi-Scale Recurrent Neural Network with Pretraining for Spoken Language Understanding Tasks "]
["This invention provides a data processing apparatus which can store and recall more complicated time-series data than those processed in related art technologies. In the data processing apparatus, a recurrent neural network (RNN) of higher layer generates long-period parameter and supplies it to an input layer of RNN of lower layer via a computing block. The RNN uses this input as a parameter and computes short-period input.", "Data processing apparatus and method, recording medium, and program "]
["Methods, and systems, including computer programs encoded on computer storage media for generating data items. A method includes reading a glimpse from a data item using a decoder hidden state vector of a decoder for a preceding time step, providing, as input to a encoder, the glimpse and decoder hidden state vector for the preceding time step for processing, receiving, as output from the encoder, a generated encoder hidden state vector for the time step, generating a decoder input from the generated encoder hidden state vector, providing the decoder input to the decoder for processing, receiving, as output from the decoder, a generated a decoder hidden state vector for the time step, generating a neural network output update from the decoder hidden state vector for the time step, and combining the neural network output update with a current neural network output to generate an updated neural network output.", "Recurrent neural networks for data item generation "]
["A neural network can be used to determine edit operations for normalizing an electronic communication. For example, an electronic representation of multiple characters that form a noncanonical communication can be received. It can be determined that the noncanonical communication is mapped to at least two canonical terms in a database. A recurrent neural network can be used to determine one or more edit operations usable to convert the noncanonical communication into a normalized version of the noncanonical communication. In some examples, the one or more edit operations can include inserting a character into the noncanonical communication, deleting the character from the noncanonical communication, or replacing the character with another character in the noncanonical communication. The noncanonical communication can be transformed into the normalized version of the noncanonical communication by performing the one or more edit operations.", "Determining edit operations for normalizing electronic communications using a neural network "]
["The technology relates to converting text to speech utilizing recurrent neural networks (RNNs). The recurrent neural networks may be implemented as multiple modules for determining properties of the text. In embodiments, a part-of-speech RNN module, letter-to-sound RNN module, a linguistic prosody tagger RNN module, and a context awareness and semantic mining RNN module may all be utilized. The properties from the RNN modules are processed by a hyper-structure RNN module that determine the phonetic properties of the input text based on the outputs of the other RNN modules. The hyper-structure RNN module may generate a generation sequence that is capable of being converting to audible speech by a speech synthesizer. The generation sequence may also be optimized by a global optimization module prior to being synthesized into audible speech.", "Hyper-structure recurrent neural networks for text-to-speech "]
["A spot welder comprises a neural network for processing, in real time, current and voltage energizing a weld in progress. The neural network generates a predicted time of optimal weld strength and/or nugget size for the weld in progress. A controller terminates the weld in progress at the predicted time. A method for controlling a spot welder comprises the steps of: sensing in real time current and voltage energizing a spot weld in progress; predicting a time of optimal weld strength and/or nugget size with a neural network responsive to the sensed current and voltage; and, terminating the weld in progress at the predicted time. A sensor for electromotive forces (EMF) induced by the spot welder can generate a signal for canceling out a large fraction of EMF components in at least one or both of the current and voltage signals. EMF components are substantially precluded in the current signal if the current sensor uses a buried shunt. Termination of the weld in progress at the predicted time is prevented when the predicted time precedes a predetermined minimum weld duration. The weld in progress is terminated at a predetermined maximum weld duration when the predicted time is after the predetermined maximum weld duration.", "Neural network control of spot welding "]
["A controller for a plant having a fixed-weight recurrent neural network with at least one external input signal representative of a desired condition of the plant and actual condition of the plant, and an output connected as a control signal to the plant. The fixed recurrent neural network includes a set of nodes with fixed weight interconnections between the nodes and at least one feedback input interconnecting an output from at least one of the nodes to an input of at least one node. These nodes collectively determine the value of the output from the neural network as a function of the input signal and the feedback input. The controller also includes an adaptive neural network having a plurality of nodes with variable weight interconnections between the nodes. A cost input from the plant is connected to the adaptive neural network while an output from the adaptive neural network is coupled as a processed feedback signal to nodes of the fixed-weight recurrent neural network.", "Fixed-weight recurrent neural network controller with fixed long-term and adaptive short-term memory "]
[null, "Electromechanical device neural network failure trend prediction method "]
["Systems, methods and media are provided for training a snippet extractor to create snippets based on information extracted from published descriptions. In one example, a computer-implemented method includes creating, based on a non-RNN (Recurrent Neural Network) extraction technique performed on the published descriptions, a plurality of base models, each base model including one or more sample description summaries; evaluating the base models using an evaluation technique; selecting an optimum base model; developing a classification model using RNN extraction, the classification model based on description summaries contained in the optimum base model; and using the classification model to train the snippet extractor by machine learning.", "Snippet extractor: recurrent neural networks for text summarization at industry scale "]
["A circuit element of a multi-dimensional dynamic adaptive neural network array (DANNA) may comprise a neuron/synapse select input functional to select the circuit element to function as one of a neuron and a synapse. In one embodiment of a DANNA array of such circuit elements, (wherein a circuit element may be digital), a destination neuron may be connected to a first neuron by a first synapse in one dimension a second destination neuron may be connected to the first neuron by a second synapse in a second dimension to form linked columns and rows of neuron/synapse circuit elements. In one embodiment, the rows and columns of circuit elements have read registers that are linked together by signal lines and clocked and controlled so as to output columnar data to an output register when a neuron/synapse data value is stored in the read register.", "Method and apparatus for providing real-time monitoring of an artifical neural network "]
["In a virtual element selection device for use in growing, in an electronic device, virtual life composed of various parts or elements, an indication is given from a user and is converted by a neural network into a new element of the virtual life. The new element is substituted for an old one to be displayed on a display device. Using the neural network makes it possible to select the new element which reflects the user's intention.", "Virtual element selection device and method using a neural network "]
["A speech recognition apparatus using a neural network. A neuron-like element according to the present invention has a means for storing a value of the inner condition thereof, a means for updating a value of internal status on the basis of an output from the neuron-like element itself, outputs from other neuron-like elements and an external input, and an output value generating means for converting a value of internal status into an external output. Accordingly, the neuron-like element itself can retain the history of input data. This enables the time series data, such as speech to be processed without providing any special means in the neural network.", "Speech recognition apparatus using neural network, and learning method therefor "]
["A neural network radar processor (10) comprises a multilayer perceptron neural network (100.1) comprising an input layer (102), a second layer (122), and at least a third layer (124), wherein each layer has a plurality of nodes (108), and respective subsets of nodes (108) of the second (122) and third (124) layers are interconnected so as to form mutually exclusive subnetworks (120). In-phase and quadrature phase time series from a sampled down-converted FMCW radar signal (19) are applied to the input layer, and the neural network (100) is trained so that the nodes of the output layer (106) are responsive to targets in corresponding range cells, and different subnetworks (120) are responsive to respectively different non-overlapping sets of target ranges. The neural network is trained with signals that are germane to an FMCW radar, including a wide range of target scenarios as well as leakage signals, DC bias signals, and background clutter signals.", "Neural network radar processor "]
[null, "Continuous voice recognition method based on deep long and short term memory recurrent neural network "]
["A method for predicting a canonical form for an input text sequence includes predicting the canonical form with a neural network model. The model includes an encoder, which generates a first representation of the input text sequence based on a representation of n-grams in the text sequence and a second representation of the input text sequence generated by a first neural network. The model also includes a decoder which sequentially predicts terms of the canonical form based on the first and second representations and a predicted prefix of the canonical form. The canonical form can be used, for example, to query a knowledge base or to generate a next utterance in a discourse.", "Semantic parsing using deep neural networks for predicting canonical forms "]
["A method for computer-assisted modeling of a technical system is disclosed. At multiple different operating points, the technical system is described by a first state vector with first state variable(s) and by a second state vector with second state variable(s). A neural network comprising a special form of a feed-forward network is used for the computer-assisted modeling of said system. The feed-forward network includes at least one bridging connector that connects a neural layer with an output layer, thereby bridging at least one hidden layer, which allows the training of networks with multiple hidden layers in a simple manner with known learning methods, e.g., the gradient descent method. The method may be used for modeling a gas turbine system, in which a neural network trained using the method may be used to estimate or predict nitrogen oxide or carbon monoxide emissions or parameters relating to combustion chamber vibrations.", "Method For The Computer-Assisted Modeling Of A Technical System "]
["An all-optical, continuous-time, recurrent neural network is disclosed which is capable of executing a broad class of energy-minimizing neural net algorithms. The network is a resonator which contains a saturable, two-beam amplifier; two volume holograms; and a linear, two-beam amplifier. The saturable amplifier permits, through the use of a spatially patterned signal beam, the realization of a two-dimensional optical neuron array; the two volume holograms provide adaptive, global network interconnectivity; and the linear amplifier supplies sufficient resonator gain to permit convergent operation of the network.", "Continuous-time optical neural network "]
["A method and an apparatus for the rapid learning of nonlinear mappings and topological transformations using a dynamically reconfigurable artificial neural network is presented. This fully-recurrent Adaptive Neuron Model (ANM) network has been applied to the highly degenerative inverse kinematics problem in robotics, and its performance evaluation is bench-marked. Once trained, the resulting neuromorphic architecture was implemented in custom analog neural network hardware and the parameters capturing the functional transformation downloaded onto the system. This neuroprocessor, capable of 109 ops/sec, was interfaced directly to a three degree of freedom Heathkit robotic manipulator. Calculation of the hardware feed-forward pass for this mapping was benchmarked at \u224810 \u03bcsec.", "Adaptive neuron model--an architecture for the rapid learning of nonlinear topological transformations "]
["A signal extraction system for extracting one or more signal components from an input signal including a plurality of signal components. This system is equipped with a neural network arithmetic section designed to process information through the use of a recurrent neural network. The neural network arithmetic section extracts one or more signal components, for example, a speech signal component and a noise signal component from an input signal including a plurality of signal components such as a speech and noises and outputs the extracted signal components. Owing to the presence of this neural network arithmetic section, the signal extraction becomes possible with a high accuracy.", "Signal extraction system, system and method for speech restoration, learning method for neural network model, constructing method of neural network model, and signal processing system "]
["A method for remotely monitoring and identifying Functional Activities performed by a test subject, the method including the steps of (a) extracting a given type of signals from each of a predetermined set of muscles of a control subject during the performance thereby of a given Functional Activity; (b) processing the signals to provide therefor given parameters affected by the given Functional Activity; (c) storing the data obtained in step (b); and (d) repeating steps (a)-(c) for a plurality of other control subjects. The stored data then is utilized to establish a normative data base indicative of the given Functional Activity and further steps include (f) extracting the given type of signals from each of at least some of the predetermined set of muscles of a test subject during a given monitoring period; and (g) comparing the data base to the signals of step (f) to identify during the monitoring period portions thereof during which the test subject was performing the given Functional Activity.", "System and method for remotely monitoring functional activities "]
["A speech synthesis method based on recurrent neural networks specifically comprises the following steps of: acquiring context information of a text to be synthesized; generating an acoustic statistical parameter sequence according to the context information of the text; according to the acoustic statistical parameter sequence generated from the context information, using a recurrent neural network to generate an acoustic parameter sequence of a speech to be synthesized; and synthesizing the speech according to the acoustic parameter sequence of the speech to be synthesized. Compared with traditional statistical parameter speech synthesis methods, the method gives the synthesized speech better naturalness and has good real-time property.", "Speech synthesis method based on recurrent neural networks "]
["Learning-type motion control is performed using a hierarchical recurrent neural network. A motion pattern provided through human teaching work is automatically time-serially segmented with the hierarchical recurrent neural network, and the motion control of a machine body is carried out with a combination of the segmented data, whereby various motion patterns can be produced. With the time-serial segmentation, local time-serial patterns and an overall pattern as a combination of the local time-serial patterns are produced. For those motion patterns, indices for static stability and dynamic stability of the machine body, e.g., ZMP stability criteria, are satisfied and hence control stability is ensured.", "Legged mobile robot and its motion teaching method, and storage medium "]
["A method of address translation of images and filters to virtual matrices to perform a convolution by matrix multiplication includes receiving an image and a filter. Each image and filter has a memory address. The method also includes mapping the memory addresses to virtual matrix addresses based on a calculated linearized image and a calculated linearized filter. The method further includes converting data in the virtual matrix to a predefined internal format. The method still further includes convolving the image by matrix multiplication of the data in the predefined internal format based on the virtual matrix addresses.", "Convolution matrix multiply with callback for deep tiling for deep convolutional neural networks "]
["A robust neural system for robust processing is disclosed for averting unacceptable or disastrous processing performances. This robust neural system either comprises a neural network or comprises a neural network and at least one range transformer. At least one adjustable weight of the robust neural system is a nonlinear weight of the neural work determined in a nonadaptive training of the robust neural system with respect to a nonadaptive risk-sensitive training criterion.", "Robust neutral systems "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating representations of input sequences. One of the methods includes obtaining an input sequence, the input sequence comprising a plurality of inputs arranged according to an input order; processing the input sequence using a first long short term memory (LSTM) neural network to convert the input sequence into an alternative representation for the input sequence; and processing the alternative representation for the input sequence using a second LSTM neural network to generate a target sequence for the input sequence, the target sequence comprising a plurality of outputs arranged according to an output order.", "Generating representations of input sequences using neural networks "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating representations of input sequences. One of the methods includes receiving a grapheme sequence, the grapheme sequence comprising a plurality of graphemes arranged according to an input order; processing the sequence of graphemes using a long short-term memory (LSTM) neural network to generate an initial phoneme sequence from the grapheme sequence, the initial phoneme sequence comprising a plurality of phonemes arranged according to an output order; and generating a phoneme representation of the grapheme sequence from the initial phoneme sequence generated by the LSTM neural network, wherein generating the phoneme representation comprises removing, from the initial phoneme sequence, phonemes in one or more positions in the output order.", "Generating representations of input sequences using neural networks "]
["Recurrent neural networks are powerful tools for handling incomplete data problems in machine learning thanks to their significant generative capabilities. However, the computational demand for algorithms to work in real time applications requires specialized hardware and software solutions. We disclose a method for adding recurrent processing capabilities into a feedforward network without sacrificing much from computational efficiency. We assume a mixture model and generate samples of the last hidden layer according to the class decisions of the output layer, modify the hidden layer activity using the samples, and propagate to lower layers. For an incomplete data problem, the iterative procedure emulates feedforward-feedback loop, filling-in the missing hidden layer activity with meaningful representations.", "Method for pseudo-recurrent processing of data using a feedforward neural network architecture "]
[null, "Behavior identification method based on recurrent neural network and human skeleton movement sequences "]
["A method for the computer-aided learning of a recurrent neural network for modeling a dynamic system which is characterized at respective times by an observable vector with one or more observables as entries is provided. The neural network includes both a causal network with a flow of information that is directed forwards in time and a retro-causal network with a flow of information which is directed backwards in time. The states of the dynamic system are characterized by first state vectors in the causal network and by second state vectors in the retro-causal network, wherein the state vectors each contain observables for the dynamic system and also hidden states of the dynamic system. Both networks are linked to one another by a combination of the observables from the relevant first and second state vectors and are learned on the basis of training date including known observables vectors.", "Method for the computer-aided learning of a recurrent neural network for modeling a dynamic system "]
["Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for a neural network system. In one aspect, a neural network system includes a recurrent neural network that is configured to, for each time step of a predetermined number of time steps, receive a set of latent variables for the time step and process the latent variables to update a hidden state of the recurrent neural network; and a generative subsystem that is configured to, for each time step, generate the set of latent variables for the time step and provide the set of latent variables as input to the recurrent neural network; update a hidden canvas using the updated hidden state of the recurrent neural network; and, for a last time step, generate an output image using the updated hidden canvas for the last time step.", "Generative neural networks "]
["The invention relates to the analysis and prognosis of the state of a combustion chamber using a recurrent, neuronal network. According to the invention, the recurrent, neuronal network is trained with training data from states of combustion chambers, and thus provided with a prognosis capacity. The state of a combustion chamber is predicted using the trained, recurrent, neuronal network.", "Method for the prognosis of the state of a combustion chamber using a recurrent, neuronal network "]
[null, "A method for computer-aided learning of a recurrent neural network "]
["A method and apparatus for vascular disease detection and characterization using a recurrent neural network (RNN) is disclosed. A plurality of 2D cross-section image patches are extracted from a 3D computed tomography angiography (CTA) image, each extracted at a respective sampling point along a vessel centerline of a vessel of interest in the 3D CTA image. Vascular abnormalities in the vessel of interest are detected and characterized by classifying each of the sampling points along the vessel centerline based on the plurality of 2D cross-section image patches using a trained RNN.", "Method and system for vascular disease detection using recurrent neural networks "]
["The invention relates to a method for the computer-aided learning of a recurrent neural network for modelling a dynamic system which is characterized at respective times by an observables vector comprising one or more observables as entries. The recurrent neuronal network comprises a first causal network that couples first vectors of the dynamic system at successive points in time via a weight matrix. In the context of learning the recurrent neuronal network, a so-called teacher-forcing is carried out for each first state vector of the causal network using a differential vector. The method according to the invention is characterized in that the recurrent neuronal network comprises also at least a second causal network which is constructed analogous to the first causal network and in which second state vectors are coupled to each other by means of the same weight matrix as in the first causal network. However, the at least one second causal network differs from the first causal network in that teacher-forcing is only carried out for a part of the second state vectors. In this way, an improved learning of the neuronal network is achieved so that the learned neuronal network provides better long-term assessments of the correspondingly modelled dynamic system. The method is particularly suitable for modelling the temporal development of energy prices and/or raw material prices. Similarly, the method can be used for modelling observables for any technical systems, such as, for example, gas turbines and/or wind power installations.", "Method for the computer-aided learning of a recurrent neuronal network for modelling a dynamic system "]
["An information processing apparatus, includes a lower time series data generation unit having a plurality of recurrent neural networks which learn predetermined time series data, and generating prediction time series data. An upper time series data generation unit has recurrent neural networks which learn error time series data that is time series data of errors raised at the time of the learning by the respective plural recurrent neural networks of the lower time series data generation unit. Generation of prediction error time series data that is time series data of prediction errors; and a conversion unit that performs nonlinear conversion for the prediction errors generated by the upper time series data generation unit. The lower time series data generation unit outputs the prediction time series data according to the prediction errors.", "Information processing apparatus, method, and program using recurrent neural networks "]
[null, "Typhoon prediction system by using evolution strategy and bilinear recurrent neural network "]
["A device for simulating human creativity employing a neural network trained to produce input-output maps within some predetermined knowledge domain, an apparatus for subjecting the neural network to perturbations that produce changes in the predetermined knowledge domain, the neural network having an optional output for feeding the outputs of the neural network to a second neural network that evaluates and selects outputs based on training within the second neural network. The device may also include a reciprocal feed back connection from the output of the second neural network to the first neural network to further influence and change what takes place in the aforesaid neural network.", "Device for the autonomous generation of useful information "]
[null, "Ammonia concentration predicting method based on recursion self-organization neural network "]
["A method and apparatus for data communication in an oil well environment, wherein the method comprises detecting an acoustic signal transmitted along an acoustic channel, the acoustic signal being distorted from transmission through the acoustic channel, generating a transmitted data signal in response to the acoustic signal, inputting the transmitted data signal to an adaptive equalizer and adaptively equalizing the transmitted data signal to produce an equalized data signal related to the transmitted data signal by a mathematical function. The detecting step may include positioning an acoustic receiver in a communication unit along the acoustic channel. The communication unit may be positioned downhole and the adaptive equalizer may be positioned remotely relative to the communication unit or may be placed in the communication unit. The adaptive equalizer may be a frequency domain filter, a neural net adaptive equalizer or a nonlinear recurrent neural net equalizer. The acoustic signal may comprise a plurality of discrete transmissions which may be a training sequence for training the adaptive equalizer and may comprise a first discrete transmission transmitted repeatedly.", "Adaptive acoustic channel equalizer & tuning method "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for implementing long-short term memory layers with compressed gating functions. One of the systems includes a first long short-term memory (LSTM) layer, wherein the first LSTM layer is configured to, for each of the plurality of time steps, generate a new layer state and a new layer output by applying a plurality of gates to a current layer input, a current layer state, and a current layer output, each of the plurality of gates being configured to, for each of the plurality of time steps, generate a respective intermediate gate output vector by multiplying a gate input vector and a gate parameter matrix. The gate parameter matrix for at least one of the plurality of gates is a structured matrix or is defined by a compressed parameter matrix and a projection matrix.", "Compressed recurrent neural network models "]
["A system eliminates alignment processing and performs TTS functionality using a new neural architecture. The neural architecture includes an encoder and a decoder. The encoder receives an input and encodes it into vectors. The encoder applies a sequence of transformations to the input and generates a vector representing the entire sentence. The decoder takes the encoding and outputs an audio file, which can include compressed audio frames.", "Sequence to sequence transformations for speech synthesis via recurrent neural networks "]
["A biometric identification apparatus and method using bio signals and an artificial neural network, are provided. The biometric identification apparatus includes: a periodic signal extraction unit which extracts one or more periodic signals from an input bio signal; a template calculation unit which calculates a template value using the extracted periodic signals; a template storage unit which stores a plurality of template values corresponding to a plurality of living bodies; and a reading unit which reads the template value that is most approximate to the template value calculated by the template calculation unit from the template storage unit. Accordingly, it is possible to identify a living body by taking into consideration all of the characteristics of bio signals detected from the living body.", "Biometric identification apparatus and method using bio signals and artificial neural network "]
["This is a recurrent or feedforward analog neural network processor having a multi-level neuron array and a synaptic matrix for storing weighted analog values of synaptic connection strengths which is characterized by temporarily changing one connection strength at a time to determine its effect on system output relative to the desired target. That connection strength is then adjusted based on the effect, whereby the processor is taught the correct response to training examples connection by connection.", "Analog hardware for learning neural networks "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for implementing a compressed recurrent neural network (RNN). One of the systems includes a compressed RNN, the compressed RNN comprising a plurality of recurrent layers, wherein each of the recurrent layers has a respective recurrent weight matrix and a respective inter-layer weight matrix, and wherein at least one of recurrent layers is compressed such that a respective recurrent weight matrix of the compressed layer is defined by a first compressed weight matrix and a projection matrix and a respective inter-layer weight matrix of the compressed layer is defined by a second compressed weight matrix and the projection matrix.", "Compressed recurrent neural network models "]
["A teaching method for a recurrent neural network (10) having hidden (16), output (14) and input (12) neurons calculates weighting errors over a limited number of propagations of the network. This process permits the use of conventional teaching sets, such as are used with feedforward networks, to be used with recurrent networks. The teaching outputs are substituted for the computed activations (Z(3), Z(4)) of the output (14) neurons in the forward propagation and error correction stages. Back propagated error from the last propagation is assumed to be zero for the hidden (16) neurons. A method of reducing drift of the network with respect to a modeled process is also described and a forced cycling method to eliminate the time lag between network input and output.", "Teaching method for recurrent neural networks "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for using recurrent neural networks to analyze health events. One of the methods includes: processing each of a plurality of initial temporal sequences of health events to generate, for each of the initial temporal sequences, a respective network internal state of a recurrent neural network for each time step in the initial temporal sequence; storing, for each of the initial temporal sequences, one or more of the network internal states for the time steps in the temporal sequence in a repository; obtaining a first temporal sequence; processing the first temporal sequence using the recurrent neural network to generate a sequence internal state for the first temporal sequence; and selecting one or more initial temporal sequences that are likely to include health events that are predictive of future health events in the first temporal sequence.", "Analyzing health events using recurrent neural networks "]
["A process and an arrangement for conditioning input variables of a neural network are described by the invention. From the input variables of the network, time series are formed and these are subdivided into intervals whose length depends on how far the interval and the measured variables contained therein lie back in the past. In this case, the interval length is selected to be larger the further the interval lies back in the past. By means of convolution using a bell-shaped function, a representative input value for the neural network is obtained from all these measured variables contained in the interval. All the input variables which are obtained in this way are fed to the network simultaneously during training and during operation. A memory is thus realized in a simple way for a forwardly directed neural network. Potential applications include, in particular, chemical processes having very different time constants.", "Process and arrangement for conditioning an input variable of a neural network "]
["In some examples, a computing device includes at least one processor; and at least one module, operable by the at least one processor to: output, for display at an output device, a graphical keyboard; receive an indication of a gesture detected at a location of a presence-sensitive input device, wherein the location of the presence-sensitive input device corresponds to a location of the output device that outputs the graphical keyboard; determine, based on at least one spatial feature of the gesture that is processed by the computing device using a neural network, at least one character string, wherein the at least one spatial feature indicates at least one physical property of the gesture; and output, for display at the output device, based at least in part on the processing of the at least one spatial feature of the gesture using the neural network, the at least one character string.", "Neural network for keyboard input decoding "]
["A brain machine interface (BMI) to control a device is provided. The BMI has a neural decoder, which is a neural to kinematic mapping function with neural signals as input to the neural decoder and kinematics to control the device as output of the neural decoder. The neural decoder is based on a continuous-time multiplicative recurrent neural network, which has been trained as a neural to kinematic mapping function. An advantage of the invention is the robustness of the decoder to perturbations in the neural data; its performance degrades less\u2014or not at all in some circumstances\u2014in comparison to the current state decoders. These perturbations make the current use of BMI in a clinical setting extremely challenging. This invention helps to ameliorate this problem. The robustness of the neural decoder does not come at the cost of some performance, in fact an improvement in performance is observed.", "Multiplicative recurrent neural network for fast and robust intracortical brain machine interface decoders "]
["A system for mitigating network attacks is provided. The system includes a protected network including a plurality of devices. The system further includes one or more attack mitigation devices communicatively coupled to the protected network. The attack mitigation devices are configured and operable to employ a recurrent neural network (RNN) to obtain probability information related to a request stream. The request stream may include a plurality of at least one of: HTTP, RTSP and/or DNS messages. The attack mitigation devices are further configured to analyze the obtained probability information to detect one or more atypical requests in the request stream. The attack mitigation services are also configured and operable to perform, in response to detecting one or more atypical requests, mitigation actions on the one or more atypical requests in order to block an attack.", "Using recurrent neural networks to defeat dns denial of service attacks "]
["Provided are methods and system for recognizing characters such as mathematical expressions or chemical formulas. An example method comprises the steps of receiving and processing an image by a pre-processing module to obtain one or more candidate regions, extracting features of each of the candidate regions by a feature extracting module such as a convolutional neural network (CNN), encoding the features into a distributive representation for each of the candidate regions separately using an encoding module such as a first long short-term memory (LSTM) based neural network, decoding the distributive representation into output representations using a decoding module such as a second LSTM-based recurrent neural network, and combining the output representations into an output expression, which is outputted in a computer-readable format or a markup language.", "Neural network based recognition of mathematical expressions "]
[null, "Feedback blind equalization method of dynamic wavelet neural network based on fuzzy control "]
["Expandable actuators surround a central conduit. Each actuator comprises a bladder that, when fluid is introduced, expands laterally while contracting longitudinally. A restorative spring can be placed inside a bladder and between the two ends to restore the actuator to its original shape as fluid is withdrawn. Multiple actuators can be placed in series to successively inflate and deflate and generate a peristaltic motion. One or more Shape Memory Alloy (SMA) springs can be affixed to one or more restorative springs to cause bending motion.", "Peristaltically self-propelled endoscopic device "]
["Hardware accelerator templates and design frameworks for implementing recurrent neural networks (RNNs) and variants thereof are described. A design framework module obtains a flow graph for an RNN algorithm. The flow graph identifies operations to be performed to implement the RNN algorithm and further identifies data dependencies between ones of the operations. The operations include matrix operations and vector operations. The design framework module maps the operations of the flow graph to an accelerator hardware template, yielding an accelerator instance comprising register transfer language code that describes how one or more matrix processing units and one or more vector processing units are to be arranged to perform the RNN algorithm. At least one of the one or more MPUs, as part of implementing the RNN algorithm, is to directly provide or directly receive a value from one of the one or more VPUs.", "Hardware accelerator template and design framework for implementing recurrent neural networks "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for identifying an object from a video. One of the methods includes obtaining multiple frames from a video, where each frame of the multiple frames depicts an object to be recognized, and processing, using an object recognition model, the multiple frames to generate data that represents a classification of the object to be recognized.", "Object recognition from videos using recurrent neural networks "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media for speech recognition. One method includes obtaining an input acoustic sequence, the input acoustic sequence representing an utterance, and the input acoustic sequence comprising a respective acoustic feature representation at each of a first number of time steps; processing the input acoustic sequence using a first neural network to convert the input acoustic sequence into an alternative representation for the input acoustic sequence; processing the alternative representation for the input acoustic sequence using an attention-based Recurrent Neural Network (RNN) to generate, for each position in an output sequence order, a set of substring scores that includes a respective substring score for each substring in a set of substrings; and generating a sequence of substrings that represent a transcription of the utterance.", "Speech recognition with attention-based recurrent neural networks "]
["Systems and methods for a multi-core optimized Recurrent Neural Network (RNN) architecture are disclosed. The various architectures affect communication and synchronization operations according to the Multi-Bulk-Synchronous-Parallel (MBSP) model for a given processor. The resulting family of network architectures, referred to as MBSP-RNNs, perform similarly to a conventional RNNs having the same number of parameters, but are substantially more efficient when mapped onto a modern general purpose processor. Due to the large gain in computational efficiency, for a fixed computational budget, MBSP-RNNs outperform RNNs at applications such as end-to-end speech recognition.", "Systems and methods for a multi-core optimized recurrent neural network "]
["Systems and processes for language identification using recurrent neural networks are provided. An example method includes, at an electronic device, receiving a first typed character of a character sequence and determining a character context of the first typed character based on the first typed character and a second typed character of the character sequence. The method further includes determining a confidence level that the character sequence is associated with a language of a plurality of languages based on the character context of the first typed character, and determining whether the confidence level exceeds a threshold, in accordance with a determination that the confidence level exceeds the threshold, providing the language as a candidate language, and in accordance with a determination that the confidence level does not exceed the threshold, forgoing providing the language as a candidate language.", "Language identification using recurrent neural networks "]
["Embodiments of the present invention include methods and systems for predicting the likelihood of topics appearing in a set of data such as text. A number of latent variable methods are used to convert the data into a set of topics, topic values and topic profiles. A number of time-course methods are used to model how topic values change given previous topic profiles, or to find historical times with similar topic values and then projecting the topic profile forward from that historical time to predict the likelihood of the topics appearing. Embodiments include utilizing focus topics, such as valence topics, and data representing financial measures to predict the likelihood of topics. Methods and systems for modeling data and predicting the likelihood of topics over other dimensions are also contemplated.", "Method and system to predict a data value "]
["The present invention relate to a method and system to predict the likelihood of data topics that may occur from data sources. The likelihood of the data topics may be predicted over other dimensions of time or over other dimensions. In the present invention, a topic means a defining characteristic, usually represented as a data element, of a single feature, activity, subject, behavior, event or an aggregation of such defining characteristics.", "Method and system to predict the likelihood of topics "]
[null, "Method and device for speech recognition by use of LSTM recurrent neural network model "]
[null, "Method and device for learning recurrent type neural network "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for predicting likelihoods of conditions being satisfied using recurrent neural networks. One of the systems is configured to process a temporal sequence comprising a respective input at each of a plurality of time steps and comprises: one or more recurrent neural network layers; one or more logistic regression nodes, wherein each of the logistic regression nodes corresponds to a respective condition from a predetermined set of conditions, and wherein each of the logistic regression nodes is configured to, for each of the plurality of time steps: receive the network internal state for the time step; and process the network internal state for the time step in accordance with current values of a set of parameters of the logistic regression node to generate a future condition score for the corresponding condition for the time step.", "Predicting likelihoods of conditions being satisfied using recurrent neural networks "]
["A method of processing data within a convolutional attention recurrent neural network (RNN) includes generating a current multi-dimensional attention map. The current multi-dimensional attention map indicates areas of interest in a first frame from a sequence of spatio-temporal data. The method further includes receiving a multi-dimensional feature map. The method also includes convolving the current multi-dimensional attention map and the multi-dimensional feature map to obtain a multi-dimensional hidden state and a next multi-dimensional attention map. The method identifies a class of interest in the first frame based on the multi-dimensional hidden state and training data.", "Video analysis with convolutional attention recurrent neural networks "]
["Systems and processes for word encoding are provided. In accordance with one example, a method includes, at an electronic device with one or more processors and memory, receiving a user input, determining a first similarity between a representation of the user input and a first acoustic model of a plurality of acoustic models, and determining a second similarity between the representation of the user input and a second acoustic model of the plurality of acoustic models. The method further includes determining whether the first similarity is greater than the second similarity. In accordance with a determination that the first similarity is greater than the second similarity, the first acoustic model may be selected; and in accordance with a determination that the first similarity is not greater than the second similarity, the second acoustic model may be selected.", "Efficient word encoding for recurrent neural network language models "]
[null, "Chinese name recognition method based on recurrent neural network "]
[null, "Learning method and speech recognition using chaotic recurrent neural networks "]
["The invention provides novel closed-loop neuroprosthetic devices and systems for preventing seizures in which control of the delivery of therapeutic electrical stimulation to a neural structure being monitored is determined by the dynamical electrophysiological state of the neural structure. In certain embodiments, a controller which generates predetermined control input is activated based on an Automated Seizure Warning system. Other embodiments of the systems and methods encompass direct control systems wherein the controller design is based on chaos theory. Yet other versions embody model-based control systems in which controller design is based on a model that represents the relationship between the control input and the dynamical descriptor.", "Closed-loop state-dependent seizure prevention systems "]
["The present disclosure relates to applying techniques similar to those used in neural network language modeling systems to a content recommendation system. For example, by associating consumed media content to words of a language model, the system may provide content predictions based on an ordering. Thus, the systems and techniques described herein may produce enhanced prediction results for recommending content (e.g. word) in a given sequence of consumed content. In addition, the system may account for additional user actions by representing particular actions as punctuation in the language model.", "Content Recommendation System using a Neural Network Language Model "]
["The present invention relates to a method and system for quantitative and semi-quantitative modeling of biological and physiological systems using image data. More specifically, the system utilizes time-series image data to improve the accuracy of the predictions made by a simulation model capable of forecasting the spatiotemporal evolution of a given biological or physiological system. Furthermore, in accordance with another aspect of the invention, the quality of experimentally acquired images can be improved by using a simulation model to eliminate noise and measurement errors from the acquired image data. Finally, in accordance with another aspect of the invention, certain undamped random disturbances in a biological or physiological system can be detected and tracked by applying a fading-memory filter to acquired time-series data and predictions of the time series using a simulation model that takes into account underlying physiological, chemical or biological variables.", "Biological modeling utilizing image data "]
["A method and an apparatus for diagnosis of sensors and/or processes through use of Bayesian belief networks. More specifically, the method and apparatus of the present invention achieve sensor and/or process fault detection, isolation, and accommodation.", "Bayesian belief networks for industrial processes "]
["Techniques for general-purpose lossless data compression using a neural network including compressing an original content item to a baseline lossless compressed data format. The baseline lossless compressed data format is binarized to a binarized format. The binarized format is arithmetically coded based on probability estimates from a neural network probability estimator. The neural network probability estimator generates the probability estimates for current symbols of the binarized format to be arithmetically coded based on symbols of the binarized format that have already been arithmetically coded.", "Techniques for general-purpose lossless data compression using a recurrent neural network "]
["A method and a system for generating a target character sequence from a semantic representation including a sequence of characters are provided. The method includes adapting a target background model, built from a vocabulary of words, to form an adapted background model. The adapted background model accepts subsequences of an input semantic representation as well as words from the vocabulary. The input semantic representation is represented as a sequence of character embeddings, which are input to an encoder. The encoder encodes each of the character embeddings to generate a respective character representation. A decoder then generates a target sequence of characters, based on the set of character representations. At a plurality of time steps, a next character in the target sequence is selected as a function of a previously generated character(s) of the target sequence and the adapted background model.", "Natural language generation through character-based recurrent neural networks with finite-state prior knowledge "]
["A nonlinear neural predictive control system provide for carrying out control\nactions on an industrial process the behavior of which can not be approximated by\nmeans of linear equations, generally a chemical process in which the control of one\nor more variables is requiered to be exerted, to carry out a prediction on the\ndynamical behavior of such a process in accordance with a nonlinear model up to a\npreviously fixed prediction horizon in order to effect a calculation of the control\nsignal at each sampling time by using a partially recurrent neural network in the\ncontroller.", "Nonlinear neural predictive control system "]
["A memory cell unit and a recurrent neural network including memory cell units are provided. The memory cell unit includes a first time gate configured to control a cell state value of the memory cell unit, based on a phase signal of an oscillatory frequency, and a second time gate configured to control an output value of the memory cell unit, based on the phase signal.", "Memory cell unit and recurrent neural network including multiple memory cell units "]
[null, "Compressed recurrent neural network models "]
["An all-optical, continuous-time, recurrent neural network is disclosed which is capable of executing a broad class of energy-minimizing neural net algorithms. The network is a resonator which contains a saturable, two-beam amplifier; two volume holograms; and a linear, two-beam amplifier. The saturable amplifier permits, through the use of a spatially patterned signal beam, the realization of a two-dimensional optical neuron array; the two volume holograms provide adaptive, global network interconnectivity; and the linear amplifier supplies sufficient resonator gain to permit convergent operation of the network.", "Continuous-time optical neural network process "]
[null, "Method of estimating coagulant injection rate in water treatment using deep neural network "]
["The invention consists of a learning system (108) based on the Dynamical System Architecture (DSA). Every implementation of the DSA is composed of a dynamic system (112) and a static system (116). The dynamic system (112) generates an intermediate output (114), which together with an external input (100) is fed into the static system (116), which produces a generated output (106). Every time the dynamic system (112) is reset it produces the same trajectory: an intermediate output (114) that does not cross over itself during the temporal span of each of the events generated by a reference system (102) whose behavior has to be duplicated. The static system (116) can be anything that behaves like a trainable universal function approximator. Training uses the intermediate output (114), the external input (100), the observed output (104) produced by the reference system (102) whose behavior is going to be mimicked, and a self-organizing procedure that only modifies the parameters of the static system (116). Training of the dynamic system (112) is not necessary.", "Method and a system for solving dynamic problems using the dynamical system architecture "]
["In implementations of the subject matter described herein, an action detection scheme using a recurrent neural network (RNN) is proposed. Joint locations for a skeleton representation of an observed entity in a frame of a video and a predefined action label for the frame are obtained to train a learning network including RNN elements and a classification element. Specifically, first weights for mapping the joint locations to a first feature for the frame generated by a first RNN element in a learning network and second weights for mapping the joint locations to a second feature for the frame generated by a second RNN element in the learning network are determined based on the joint locations and the predefined action label. The first and second weights are determined by increasing a first correlation between the first feature and a first subset of the joint locations and a second correlation between the second feature and the first subset of the joint locations. Based on the joint locations and the predefined action label, a parameter for a classification element included in the learning network is also determined by increasing a probability of the frame being associated with the predefined action label. The probability is generated by the classification element at least based on the first and second features.", "Skeleton-based action detection using recurrent neural network "]
["A speech recognition apparatus in which the speech signal is digitalized and subjected to special analysis, word end detection is effected by energy analysis of the speech signal and the recognition system utilizes a Markov model in combination with a neural network learning by specific training steps.", "Speaker independent isolated word recognition system using neural networks "]
[null, "A New Class of Recurrent Neural Networks for Identification of Finite State Automata "]
["A method of adaptively selecting a configuration for a machine learning process includes determining current system resources and performance specifications of a current system. A new configuration for the machine learning process is determined based at least in part on the current system resources and the performance specifications. The method also includes dynamically selecting between a current configuration and the new configuration based at least in part on the current system resources and the performance specifications.", "Adaptive selection of artificial neural networks "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media for acoustic modeling of audio data. One method includes receiving audio data representing a portion of an utterance, providing the audio data to a trained recurrent neural network that has been trained to indicate the occurrence of a phone at any of multiple time frames within a maximum delay of receiving audio data corresponding to the phone, receiving, within the predetermined maximum delay of providing the audio data to the trained recurrent neural network, output of the trained neural network indicating a phone corresponding to the provided audio data using output of the trained neural network to determine a transcription for the utterance, and providing the transcription for the utterance.", "Latency constraints for acoustic modeling "]
[null, "Method of intellectual information processing in neuron network "]
["The present invention relates to a method and apparatus for providing maintenance of a rail vehicle (10) by time series prediction, wherein at least two recurrent neural network models (200-1 to 200-m) which correspond to predetermined parts of the rail vehicle (10), that are exposed to degradation, are constructed, wherein the models (200-1 to 200-m) are used for time series prediction of sensor output information which relates to a corresponding part of the rail vehicle (10) within a condition based maintenance system of the rail vehicle (10), and wherein a maintenance output (P1 to Pm) is generated for each of the predetermined parts based on the time series prediction.", "Railway vehicle maintenance system with modular recurrent neural networks for performing time series prediction "]
["There are disclosed an apparatus for calculating a posteriori probabilities of phoneme symbols and a speech recognition apparatus using the apparatus for calculating a posteriori probabilities of phoneme symbols. A feature extracting section extracts speech feature parameters from a speech signal of an uttered speech sentence composed of an inputted character series, and a calculating section calculates a a posteriori probability of a phoneme symbol of the speech signal, by using a bidirectional recurrent neural network. The bidirectional recurrent neural network includes (a) an input layer for receiving the speech feature parameters extracted by the feature extracting means and a plurality of hypothetical phoneme symbol series signals, (b) an intermediate layer of at least one layer having a plurality of units, and (c) an output layer for outputting a a posteriori probability of each phoneme symbol. The input layer includes (a) a first input neuron group having a plurality of units, for receiving a plurality of speech feature parameters and a plurality of phoneme symbol series signals, (b) a forward module, and (c) a backward module.", "Apparatus for calculating a posterior probability of phoneme symbol, and speech recognition apparatus "]
[null, "Signal amplification methods Recurrent Neural Networks "]
["A method generates bounding-boxes within frames of a sequence of frames. The bounding-boxes may be generated via a recurrent neural network (RNN) such as a long short-term memory (LSTM) network. The method includes receiving the sequence of frames and generating an attention feature map for each frame of the sequence of frames. Each attention feature map indicates at least one potential moving object. The method also includes up-sampling each attention feature map to determine an attention saliency for pixels in each frame of the sequence of frames. The method further includes generating a bounding-box within each frame based on the attention saliency and temporally smoothing multiple bounding-boxes along the sequence of frames to obtain a smooth sequence of bounding-boxes. The method still further includes localizing an action location within each frame based on the smooth sequence of bounding-boxes.", "Action localization in sequential data with attention proposals from a recurrent network "]
["The present disclosure provides systems and methods that include or otherwise leverage a machine-learned neural synthesizer model. Unlike a traditional synthesizer which generates audio from hand-designed components like oscillators and wavetables, the neural synthesizer model can use deep neural networks to generate sounds at the level of individual samples. Learning directly from data, the neural synthesizer model can provide intuitive control over timbre and dynamics and enable exploration of new sounds that would be difficult or impossible to produce with a hand-tuned synthesizer. As one example, the neural synthesizer model can be a neural synthesis autoencoder that includes an encoder model that learns embeddings descriptive of musical characteristics and an autoregressive decoder model that is conditioned on the embedding to autoregressively generate musical waveforms that have the musical characteristics one audio sample at a time.", "Generating music with deep neural networks "]
["An artificial multiped is constructed (either in simulation or embodied) in such a way that its natural body dynamics allow the lower part of each leg to swing naturally under the influence of gravity. The upper part of each leg is actively actuated in the sagittal plane. The necessary input to drive the above-mentioned actuators is derived from a neural network controller. The latter is arranged as two bi-directionally coupled chains of neural oscillators, the number of which equals twice that of the legs to be actuated. Parameter optimisation of the controllers is achieved by evolutionary computation in the form of a genetic algorithm.", "Artificial multiped and motion controller therefor "]
["A system identifying device precisely represents as a mathematical model the features of a system such as a robot manipulator, various industrial plants, etc., and can be operated as hardware by identifying an object system over a neural network, thereby successfully identifying a non-linear system as well as a linear system. Furthermore, an adaptive control device can perform an online learning by using the identifying device to obtain a teaching signal to be used in operating the identifying device in an adaptive control device.", "System identifying device and adaptive learning control device "]
["A method for the computer-assisted control and/or regulation of a technical system is provided. The method is used to efficiently reduce a high-dimensional state space describing the technical system to a smaller dimension. The reduction of the state space is performed using an artificial recurrent neuronal network. In addition, the reduction of the state space enables conventional learning methods, which are only designed for small dimensions of state spaces, to be applied to complex technical systems with an initially large state space, wherein the conventional learning methods are performed in the reduced state space. The method can be used with any technical system, especially gas turbines.", "Method for the computer-assisted control and/or regulation of a technical system "]
["The present invention extends to methods, systems, and computer program products for detecting, classifying, and tracking abnormal data in a data stream. Embodiments include an integrated set of algorithms that enable an analyst to detect, characterize, and track abnormalities in real-time data streams based upon historical data labeled as predominantly normal or abnormal. Embodiments of the invention can detect, identify relevant historical contextual similarity, and fuse unexpected and unknown abnormal signatures with other possibly related sensor and source information. The number, size, and connections of the neural networks all automatically adapted to the data. Further, adaption appropriately and automatically integrates unknown and known abnormal signature training within one neural network architecture solution automatically. Algorithms and neural networks architecture are data driven, resulting more affordable processing. Expert knowledge can be incorporated to enhance the process, but sufficient performance is achievable without any system domain or neural networks expertise.", "Detecting, classifying, and tracking abnormal data in a data stream "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for using recurrent neural networks to analyze health events. One of the methods includes obtaining a first temporal sequence of health events, wherein the first temporal sequence comprises respective health-related data associated with a particular patient at each of a plurality of time steps; processing the first temporal sequence of health events using a recurrent neural network to generate a neural network output for the first temporal sequence; and generating, from the neural network output for the first temporal sequence, health analysis data that characterizes future health events that may occur after a last time step in the temporal sequence.", "Analyzing health events using recurrent neural networks "]
["A method for transforms input signals, by first defining a model for transforming the input signals, wherein the model is specified by constraints and a set of model parameters. An iterative inference procedure is derived from the model and the set of model parameters and unfolded into a set of layers, wherein there is one layer for each iteration of the procedure, and wherein a same set of network parameters is used by all layers. A neural network is formed by untying the set of network parameters such that there is one set of network parameters for each layer and each set of network parameters is separately maintainable and separately applicable to the corresponding layer. The neural network is trined to obtain a trained neural network, and then input signals are transformed using the trained neural network to obtain output signals.", "Neural Networks for Transforming Signals "]
["A method for computer-supported control and/or regulation of a technical system is provided. In the method a reinforcing learning method and an artificial neuronal network are used. In a preferred embodiment, parallel feed-forward networks are connected together such that the global architecture meets an optimal criterion. The network thus approximates the observed benefits as predictor for the expected benefits. In this manner, actual observations are used in an optimal manner to determine a quality function. The quality function obtained intrinsically from the network provides the optimal action selection rule for the given control problem. The method may be applied to any technical system for regulation or control. A preferred field of application is the regulation or control of turbines, in particular a gas turbine.", "Method for computer-supported control and/or regulation of a technical system "]
["In one aspect, this specification describes a recurrent neural network system implemented by one or more computers that is configured to process input sets to generate neural network outputs for each input set. The input set can be a collection of multiple inputs for which the recurrent neural network should generate the same neural network output regardless of the order in which the inputs are arranged in the collection. The recurrent neural network system can include a read neural network, a process neural network, and a write neural network. In another aspect, this specification describes a system implemented as computer programs on one or more computers in one or more locations that is configured to train a recurrent neural network that receives a neural network input and sequentially emits outputs to generate an output sequence for the neural network input.", "Processing and generating sets using recurrent neural networks "]
["Deep neural networks can be visualized. For example, first values for a first layer of nodes in a neural network, second values for a second layer of nodes in the neural network, and/or third values for connections between the first layer of nodes and the second layer of nodes can be received. A quilt graph can be output that includes (i) a first set of symbols having visual characteristics representative of the first values and representing the first layer of nodes along a first axis; (ii) a second set of symbols having visual characteristics representative of the second values and representing the second layer of nodes along a second axis; and/or (iii) a matrix of blocks between the first axis and the second axis having visual characteristics representative of the third values and representing the connections between the first layer of nodes and the second layer of nodes.", "Visualizing deep neural networks "]
["Implementing a neural network includes determining whether to process a combination of a first region of an input feature map and a first region of a convolution kernel and, responsive to determining to process the combination, performing a convolution operation on the first region of the input feature map using the first region of the convolution kernel to generate at least a portion of an output feature map.", "Neural network suppression "]
["An apparatus for executing a recurrent neural network and LSTM, comprising a command storage unit (1), a controller unit (2), a data access unit (3), an interconnection module (4), a main computation module (5), and a plurality of slave computation modules (6). The slave computation modules (6) are used for multiplying input data to obtain a partial sum and saving same until all the data in the neural network is inputted, and returning the results to the main computation module (5); and the main computation module (5) is used for implementing interpolation activation of the sums returned by the slave computation modules (6) in a forward process, and interpolating same in a reverse process to obtain an activation derivative, and multiplying same by a gradient. The present apparatus can solve the problems of insufficient CPU and GPU computational performance and high front end decoding overheads, effectively improving the support for forward computation in multi-layer artificial neural networks.", "Apparatus and method for executing recurrent neural network and lstm computations "]
["According to embodiments, a recurrent neural network (RNN) is equipped with a set data structure whose operations are differentiable, which data structure can be used to store information for a long period of time. This differentiable set data structure can \u201cremember\u201d an event in the sequence of sequential data that may impact another event much later in the sequence, thereby allowing the RNN to classify the sequence based on many kinds of long dependencies. An RNN that is equipped with the differentiable set data structure can be properly trained with backpropagation and gradient descent optimizations. According to embodiments, a differentiable set data structure can be used to store and retrieve information with a simple set-like interface. According to further embodiments, the RNN can be extended to support several add operations, which can make the differentiable set data structure behave like a Bloom filter.", "Differentiable set to increase the memory capacity of recurrent neural networks "]
[null, "Traffic flow prediction method based on recurrent neural network memory length of time "]
["A method and system are provided. The method includes receiving by a computer having a processor and a memory, sequence data that includes labeled data and unlabeled data. The method further includes generating, by the computer having the processor and the memory, a recurrent neural network model of the sequence data, the recurrent neural network model having a recurrent layer and an aggregate layer. The recurrent neural network model feeds sequences generated from the recurrent layer into the aggregate layer for aggregation, stores temporal dependencies in the sequence data, and generates labels for at least some of the unlabeled data.", "Customer profile learning based on semi-supervised recurrent neural network using partially labeled sequence data "]
["Described herein are systems and methods for multimodal recurrent network processing. In an embodiment, a system for evaluating multimodal data comprising a multimodal data input and a multimodal processing module is described. The multimodal data input may comprise the multimodal data, the multimodal data may comprise a first modality and a second modality. The multimodal processing module may be configured to receive the multimodal data comprising the first modality and the second modality; evaluate the first modality using a first recursive neural network comprising a first transformation matrix; evaluate the second modality using a second recursive neural network comprising the first transformation matrix; and determine an output based, at least in part, on evaluating the first modality and the second modality.", "Rgb-d scene labeling with multimodal recurrent neural networks "]
["According to one embodiment, a system includes a sensor component and a detection component. The sensor component is configured to obtain a first stream of sensor data and a second stream of sensor data, wherein each of the first stream and second stream comprise a plurality of sensor frames. The detection component is configured to generate a concatenated feature map based on a sensor frame of a first type and a sensor frame of a second type. The detection component is configured to detect one or more objects based on the concatenated feature map. One or more of generating and detecting comprises generating or detecting using a neural network with a recurrent connection that feeds information about features or objects from previous frames.", "Object Detection Using Recurrent Neural Network And Concatenated Feature Map "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for implementing a convolutional gated recurrent neural network (CGRN). In one of the systems, the CGRN is configured to maintain a state that is a tensor having dimensions x by y by m, wherein x, y, and m are each greater than one, and for each of a plurality of time steps, update a currently maintained state by processing the currently maintained state through a plurality of convolutional gates.", "Convolutional gated recurrent neural networks "]
["Using a recurrent neural network (RNN) that has been trained to a satisfactory level of performance, highly discriminative features can be extracted by running a sample through the RNN, and then extracting a final hidden state hh where i is the number of instructions of the sample. This resulting feature vector may then be concatenated with the other hand-engineered features, and a larger classifier may then be trained on hand-engineered as well as automatically determined features. Related apparatus, systems, techniques and articles are also described.", "Recurrent Neural Networks for Malware Analysis "]
["Various systems, mediums, and methods may perform operations, such as collecting various types of data from one or more data sources. Further, the operations may include learning user behaviors based on iterations of the collected historical data with a recurrent neural network (RNN) with long short term memory (LSTM). Yet further, the operations may include determining one or more feature vectors that represents the learned user behaviors. In addition, the operations may include generating one or more models associated with the learned user behaviors based on the one or more determined vectors.", "User Data Learning Based on Recurrent Neural Networks with Long Short Term Memory "]
["A system-effected method for synthesizing speech, or recognizing speech including a sequence of expressive speech utterances. The method can be computer-implemented and can include system-generating a speech signal embodying the sequence of expressive speech utterances. Other possible steps include: system-marking the speech signal with a pitch marker indicating a pitch change at or near a first zero amplitude crossing point of the speech signal following a glottal closure point, at a minimum, at a maximum or at another location; system marking the speech signal with at least one further pitch marker; system-aligning a sequence of prosodically marked text with the pitch-marked speech signal according to the pitch markers; and system outputting the aligned text or the aligned speech signal, respectively. Computerized systems, and stored programs for implementing method embodiments of the invention are also disclosed.", "Methods for aligning expressive speech utterances with text and systems therefor "]
["[Object] An object is to provide a training method of improving training of a recurrent neural network (RNN) using time-sequential data.", "Recurrent neural network training method, computer program therefor and speech recognition device "]
[null, "Ultrasonic wave motor transient characteristic testing device and control system thereof "]
["Described herein are systems and methods for creating and using Convolutional Recurrent Neural Networks (CRNNs) for small-footprint keyword spotting (KWS) systems. Inspired by the large-scale state-of-the-art speech recognition systems, in embodiments, the strengths of convolutional layers to utilize the structure in the data in time and frequency domains are combined with recurrent layers to utilize context for the entire processed frame. The effect of architecture parameters were examined to determine preferred model embodiments given the performance versus model size tradeoff. Various training strategies are provided to improve performance. In embodiments, using only \u02dc230 k parameters and yielding acceptably low latency, a CRNN model embodiment demonstrated high accuracy and robust performance in a wide range of environments.", "Convolutional recurrent neural networks for small-footprint keyword spotting "]
["A method and system are provided. The method includes obtaining, by a hardware processor, candidate data representing a plurality of candidates. The method further includes calculating, by the hardware processor, for each of the candidates, a temporal next state of a Recurrent Neural Network (RNN) by inputting a corresponding one of the candidates to the RNN at a current state. The method also includes merging, by the hardware processor, the temporal next state for each of the candidates to obtain the temporal next state of the RNN.", "Recurrent neural network processing pooling operation "]
["Systems and devices including an imaging sensor to capture video sequences in an environment having safety concerns therein. The systems and devices further including a processor to generate driving series based on observations from the video sequences, and generate predictions of future events based on the observations using a dual-stage attention-based recurrent neural network (DA-RNN). The DA-RNN includes an input attention mechanism to extract relevant driving series, an encoder to encode the extracted relevant driving series into hidden states, a temporal attention mechanism to extract relevant hidden states, and a decoder to decode the relevant hidden states. The processor further generates a signal for initiating an action to machines to mitigate harm to items.", "Video system using dual stage attention based recurrent neural network for future event prediction "]
["Speaker diarization is performed on audio data including speech by a first speaker, speech by a second speaker, and silence. The speaker diarization includes segmenting the audio data using a long short-term memory (LSTM) recurrent neural network (RNN) to identify change points of the audio data that divide the audio data into segments. The speaker diarization includes assigning a label selected from a group of labels to each segment of the audio data using the LSTM RNN. The group of labels comprising includes labels corresponding to the first speaker, the second speaker, and the silence. Each change point is a transition from one of the first speaker, the second speaker, and the silence to a different one of the first speaker, the second speaker, and the silence. Speech recognition can be performed on the segments that each correspond to one of the first speaker and the second speaker.", "Using long short-term memory recurrent neural network for speaker diarization segmentation "]
["Systems and methods for assessing quality of input text using recurrent neural networks is disclosed. The system obtains input text from user and performs a comparison of each word from input text with words from dictionary (or trained data) to determine a closest recommended word for each word in the input text. The input text is further analyzed to determine context of each word based on at least a portion of input text, and based on determined context, at least one of correct sentences, incorrect sentences, and/or complex sentences are determined from the input text. Each word is converted to a vector based on concept(s) by comparing each word across sentences of input text to generate vectors set, and quality of the input text is assessed based on vectors set, the comparison, determined context and at least one of correct sentences, incorrect sentences, complex sentences, or combinations thereof.", "Systems and methods for assessing quality of input text using recurrent neural networks "]
["The technology disclosed provides a quasi-recurrent neural network (QRNN) encoder-decoder model that alternates convolutional layers, which apply in parallel across timesteps, and minimalist recurrent pooling layers that apply in parallel across feature dimensions.", "Quasi-recurrent neural network based encoder-decoder model "]
["Audio features, such as perceptual linear prediction (PLP) features and time derivatives thereof, are extracted from frames of training audio data including speech by multiple speakers, and silence, such as by using linear discriminant analysis (LDA). The frames are clustered into k-means clusters using distance measures, such as Mahalanobis distance measures, of means and variances of the extracted audio features of the frames. A recurrent neural network (RNN) is trained on the extracted audio features of the frames and cluster identifiers of the k-means clusters into which the frames have been clustered. The RNN is applied to audio data to segment audio data into segments that each correspond to one of the cluster identifiers. Each segment can be assigned a label corresponding to one of the cluster identifiers. Speech recognition can be performed on the segments.", "Using recurrent neural network for partitioning of audio data into segments that each correspond to a speech feature cluster identifier "]
["A neural network unit (NNU) includes N neural processing units (NPU). Each NPU has an arithmetic unit and an accumulator. First and second multiplexed registers of the N NPUs collectively selectively operate as respective first and second N-word rotaters. First and second memories respectively hold rows of N weight/data words and provide the N weight/data words of a row to corresponding ones of the N NPUs. The NPUs selectively perform: multiply-accumulate operations on rows of N weight words and on a row of N data words, using the second N-word rotater; convolution operations on rows of N weight words, using the first N-word rotater, and on rows of N data words, the rows of weight words being a data matrix, and the rows of data words being elements of a convolution kernel; and pooling operations on rows of N weight words, using the first N-word rotater.", "Multi-operation neural network unit "]
["A device can receive GPS data or values for a set of metrics at a set of GPS points that form a GPS track of a vehicle. The device can determine additional values for additional metrics using the GPS data or the values for the set of metrics. The device can determine a set of vectors for the set of GPS points using the GPS data, the values, or the additional values. The set of vectors can be used in a recurrent neural network (RNN) to classify the vehicle. The device can process the set of vectors via one or more sets of RNN layers of the RNN. The device can determine a classification of the vehicle using a result of processing the set of vectors. The result can be output by the output layer. The device can perform an action based on the classification of the vehicle.", "Vehicle classification using a recurrent neural network (rnn) "]
["The technology relates to performing letter-to-sound conversion utilizing recurrent neural networks (RNNs). The RNNs may be implemented as RNN modules for letter-to-sound conversion. The RNN modules receive text input and convert the text to corresponding phonemes. In determining the corresponding phonemes, the RNN modules may analyze the letters of the text and the letters surrounding the text being analyzed. The RNN modules may also analyze the letters of the text in reverse order. The RNN modules may also receive contextual information about the input text. The letter-to-sound conversion may then also be based on the contextual information that is received. The determined phonemes may be utilized to generate synthesized speech from the input text.", "Advanced recurrent neural network based letter-to-sound "]
["Provided are systems and techniques that provide an output phrase describing an image. An example method includes creating, with a convolutional neural network, feature maps describing image features in locations in the image. The method also includes providing a skeletal phrase for the image by processing the feature maps with a first long short-term memory (LSTM) neural network trained based on a first set of ground truth phrases which exclude attribute words. Then, attribute words are provided by processing the skeletal phrase and the feature maps with a second LSTM neural network trained based on a second set of ground truth phrases including words for attributes. Then, the method combines the skeletal phrase and the attribute words to form the output phrase.", "Recurrent neural network architectures which provide text describing images "]
["An audio system is described that corrects for linear and nonlinear distortions. The system can include a physical loudspeaker system responsive to an audio input signal, an adaptive circuit, e.g., with a recurrent neural network, to correct for non-linear distortions from the loudspeaker.", "Adaptive correction of loudspeaker using recurrent neural network "]
[null, "Multi-state based on a nonlinear time scale recurrent neural network decoupling Observation Method "]
["The technology disclosed proposes using a combination of computationally cheap, less-accurate bag of words (BoW) model and computationally expensive, more-accurate long short-term memory (LSTM) model to perform natural processing tasks such as sentiment analysis. The use of cheap, less-accurate BoW model is referred to herein as \u201cskimming\u201d. The use of expensive, more-accurate LSTM model is referred to herein as \u201creading\u201d. The technology disclosed presents a probability-based guider (PBG). PBG combines the use of BoW model and the LSTM model. PBG uses a probability thresholding strategy to determine, based on the results of the BoW model, whether to invoke the LSTM model for reliably classifying a sentence as positive or negative. The technology disclosed also presents a deep neural network-based decision network (DDN) that is trained to learn the relationship between the BoW model and the LSTM model and to invoke only one of the two models.", "Deep Neural Network-Based Decision Network "]
[null, "Neural network, neural network system and neural network processing program "]
["In one aspect, an example method includes a processor (1) applying a feature map network to an image to create a feature map comprising a grid of vectors characterizing at least one feature in the image and (2) applying a probability map network to the feature map to create a probability map assigning a probability to the at least one feature in the image, where the assigned probability corresponds to a likelihood that the at least one feature is an overlay. The method further includes the processor determining that the probability exceeds a threshold, and responsive to the processor determining that the probability exceeds the threshold, performing a processing action associated with the at least one feature.", "Recurrent Deep Neural Network System for Detecting Overlays in Images "]
["The present invention relates to artificial neural networks, for example, deep neural networks. In particular, the present invention relates to a multi-iteration compression method for deep neural networks and the device thereof. More specifically, the present invention relates to how to compress dense neural networks into sparse neural networks without degrading the accuracy of the neural", "Multi-iteration compression for deep neural networks "]
["According to one embodiment, a system includes a sensor component and a detection component. The sensor component is configured to obtain a plurality of sensor frames, wherein the plurality of sensor frames comprise a series of sensor frames captured over time. The detection component is configured to detect objects or features within a sensor frame using a neural network. The neural network comprises a recurrent connection that feeds forward an indication of an object detected in a first sensor frame into one or more layers of the neural network for a second, later sensor frame.", "Recurrent Deep Convolutional Neural Network For Object Detection "]
["The present invention relates to artificial neural networks, for example, deep neural networks. In particular, the present invention relates to a compression method considering load balance for deep neural networks and the device thereof. More specifically, the present invention relates to how to compress dense neural networks into sparse neural networks in an efficient way so as to improve utilization of resources of the hardware platform.", "Compression method for deep neural networks with load balance "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media for large vocabulary continuous speech recognition. One method includes receiving audio data representing an utterance of a speaker. Acoustic features of the audio data are provided to a recurrent neural network trained using connectionist temporal classification to estimate likelihoods of occurrence of whole words based on acoustic feature input. Output of the recurrent neural network generated in response to the acoustic features is received. The output indicates a likelihood of occurrence for each of multiple different words in a vocabulary. A transcription for the utterance is generated based on the output of the recurrent neural network. The transcription is provided as output of the automated speech recognition system.", "Acoustic-to-word neural network speech recognizer "]
["The present invention relates to artificial neural networks, for example, deep neural networks. In particular, the present invention relates to a compression method for deep neural networks with proper use of mask and the device thereof. More specifically, the present invention relates to how to compress dense neural networks into sparse neural networks while maintaining or even improving the accuracy of the neural networks after compression.", "Compression of deep neural networks with proper use of mask "]
["The technology disclosed relates to evolving deep neural network structures. A deep neural network structure includes a plurality of modules with submodules and interconnections among the modules and the submodules. In particular, the technology disclosed relates to storing candidate genomes that identify respective values for a plurality of hyperparameters of a candidate genome. The hyperparameters include global topology hyperparameters, global operational hyperparameters, local topology hyperparameters, and local operational hyperparameters. It further includes evolving the hyperparameters by training, evaluating, and procreating the candidate genomes and corresponding modules and submodules.", "Evolution of deep neural network structures "]
["Techniques related to compressing a pre-trained dense deep neural network to a sparsely connected deep neural network for efficient implementation are discussed. Such techniques may include iteratively pruning and splicing available connections between adjacent layers of the deep neural network and updating weights corresponding to both currently disconnected and currently connected connections between the adjacent layers.", "Dynamic neural network surgery "]
[null, "Realization method of diagonal recurrent neural network controller in multiple platforms "]
["A method, computer readable medium, and system are disclosed for detecting and classifying hand gestures. The method includes the steps of receiving an unsegmented stream of data associated with a hand gesture, extracting spatio-temporal features from the unsegmented stream by a three-dimensional convolutional neural network (3DCNN), and producing a class label for the hand gesture based on the spatio-temporal features.", "Online detection and classification of dynamic gestures with recurrent convolutional neural networks "]
[null, "Image character sequence recognition system based on recurrent neural network "]
["A method for the computer-aided regulation and/or control of a technical system is provided. In the method, first a simulation model of the technical system is created, to which subsequently a plurality of learning and/or optimization methods are applied. Based on the results of these methods, the method best suited for the technical system is selected. The selected learning and/or optimization method is then used to regulate the technical system. Based on the simulation model, the method can thus be used to train an initial controller, which can be used as an intelligent controller, and is not modified during further regulation of the technical system.", "Method for the computer-aided regulation and/or control of a technical system, especially a gas turbine "]
["A recurrent neural network including an input layer, a hidden layer, and an output layer, wherein the hidden layer includes hybrid memory cell units, each of the hybrid memory cell units including a first memory cells of a first type, the first memory cells being configured to remember a first cell state value fed back to each of gates to determine a degree to which each of the gates is open or closed, and configured to continue to update the first cell state value, and a second memory cells of a second type, each second memory cell of the second memory cells including a first time gate configured to control a second cell state value of the second memory cell based on phase signals of an oscillatory frequency, and a second time gate configured to control an output value of the second memory cell based on the phase signals, and each second memory cell of the second memory cells being configured to remember the second cell state value.", "Hybrid memory cell unit and recurrent neural network including hybrid memory cell units "]
[null, "Method and device for optimizing neural network structure "]
[null, "Based on the evaluation method kinds of networks and recurrent neural network of product design "]
["Operations of computing devices are managed using one or more deep neural networks (DNNs), which may receive, as DNN inputs, data from sensors, instructions executed by processors, and/or outputs of other DNNs. One or more DNNs, which may be generative, can be applied to the DNN inputs to generate DNN outputs based on relationships between DNN inputs. The DNNs may include DNN parameters learned using one or more computing workloads. The DNN outputs may be, for example, control signals for managing operations of computing devices, predictions for use in generating control signals, warnings indicating an acceptable state is predicted, and/or inputs to one or more neural networks. The signals enhance performance, efficiency, and/or security of one or more of the computing devices. DNNs can be dynamically trained to personalize operations by updating DNN weights or other parameters.", "Systems and methods for optimizing operations of computing devices using deep neural networks "]
[null, "Dialogue data interaction processing method and device based on recurrent neural network "]
[null, "Recurrent neural network-based social network message burst detection method and system "]
["A deep neural network may be trained on the data of one or more entities, also know as Alices. An outside computing entity, also known as a Bob, may assist in these computations, without receiving access to Alices' data. Data privacy may be preserved by employing a \u201csplit\u201d neural network. The network may comprise an Alice part and a Bob part. The Alice part may comprise at least three neural layers, and the Bob part may comprise at least two neural layers. When training on data of an Alice, that Alice may input her data into the Alice part, perform forward propagation though the Alice part, and then pass output activations for the final layer of the Alice part to Bob. Bob may then forward propagate through the Bob part. Similarly, backpropagation may proceed backwards through the Bob part, and then through the Alice part of the network.", "Secure Training of Multi-Party Deep Neural Network "]
[null, "Mobile phone app interaction method based on recurrent neural network "]
[null, "Recurrent neural network based input method and device "]
["In one implementation, a computer-implemented method includes receiving, at a computer system, a request to predict a next word in a dialog being uttered by a speaker; accessing, by the computer system, a neural network comprising i) an input layer, ii) one or more hidden layers, and iii) an output layer; identifying the local context for the dialog of the speaker; selecting, by the computer system and using a semantic model, at least one vector that represents the semantic context for the dialog; applying input to the input layer of the neural network, the input comprising i) the local context of the dialog and ii) the values for the at least one vector; generating probability values for at least a portion of the candidate words; and providing, by the computer system and based on the probability values, information that identifies one or more of the candidate words.", "Integration of semantic context information "]
[null, "Recurrent-neural-network-based maneuver load controller and controlling method thereof "]
["The technology disclosed provides a so-called \u201cjoint many-task neural network model\u201d to solve a variety of increasingly complex natural language processing (NLP) tasks using growing depth of layers in a single end-to-end model. The model is successively trained by considering linguistic hierarchies, directly connecting word representations to all model layers, explicitly using predictions in lower tasks, and applying a so-called \u201csuccessive regularization\u201d technique to prevent catastrophic forgetting. Three examples of lower level model layers are part-of-speech (POS) tagging layer, chunking layer, and dependency parsing layer. Two examples of higher level model layers are semantic relatedness layer and textual entailment layer. The model achieves the state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment.", "Deep Neural Network Model for Processing Data Through Mutliple Linguistic Task Hiearchies "]
[null, "New enterprise name finding method based on bidirectional recurrent neural network "]
[null, "Video face recognition method based on recurrent neural network and video face recognition system based on recurrent neural network "]
[null, "Kinds of Web-based recurrent neural network forecasting method User Access "]
[null, "Device and method used for executing recurrent neural network (RNN) and LSTM operations "]
[null, "Action recognition method based on attention mechanism of convolution recurrent neural network "]
[null, "Offshore oil-gas platform detection method based on structural recurrent neural network "]
[null, "One kind of Mongolian language model based on Recurrent Neural Networks "]
["A method for computer-aided control of any technical system is provided. The method includes two steps, the learning of the dynamic with historical data based on a recurrent neural network and a subsequent learning of an optimal regulation by coupling the recurrent neural network to a further neural network. The recurrent neural network has a hidden layer comprising a first and a second hidden state at a respective time point. The first hidden state is coupled to the second hidden state using a matrix to be learned. This allows a bottleneck structure to be created, in that the dimension of the first hidden state is smaller than the dimension of the second hidden state or vice versa. The autonomous dynamic is taken into account during the learning of the network, thereby improving the approximation capacity of the network. The technical system includes a gas turbine.", "Method for computer-aided control or regualtion of a technical system "]
[null, "Three-dimensional animation generation method based on deep recurrent neural network algorithm "]
["A differential recurrent neural network (RNN) is described that handles dependencies that go arbitrarily far in time by allowing the network system to store states using recurrent loops without adversely affecting training. The differential RNN includes a state component for storing states, and a trainable transition and differential non-linearity component which includes a neural network. The trainable transition and differential non-linearity component takes as input, an output of the previous stored states from the state component along with an input vector, and produces positive and negative contribution vectors which are employed to produce a state contribution vector. The state contribution vector is input into the state component to create a set of current states. In one implementation, the current states are simply output. In another implementation, the differential RNN includes a trainable OUT component which includes a neural network that performs post-processing on the current states before outputting them.", "Differential recurrent neural network "]
[null, "Recurrent-neural-network-model-based sliding input method and system "]
[null, "The kinds of methods used in recurrent neural network associated with the selected issues "]
[null, "Recurrent neural network model construction method and gearbox fault detection method and device "]
[null, "Gaussian background modeling and recurrent neural network combined vehicle type classification method "]
[null, "UAV Surveillance Target based on recurrent neural network forecasting method of evolution "]
[null, "Music score modeling and generating method based on recurrent neural network "]
["Embodiments of the invention can include methods and systems for controlling clearances in a turbine. In one embodiment, a method can include applying at least one operating parameter as an input to at least one neural network model, modeling via the neural network model a thermal expansion of at least one turbine component, and taking a control action based at least in part on the modeled thermal expansion of the one or more turbine components. An example system can include a controller operable to determine and apply the operating parameters as inputs to the neural network model, model thermal expansion via the neural network model, and generate a control action based at least in part on the modeled thermal expansion.", "Methods and Systems for Neural Network Modeling of Turbine Components "]
[null, "File fragmentation classification method and apparatus based on recurrent neural network "]
[null, "Crime prediction method based on intertwining time sequence local connection recurrent neural network "]
[null, "Reliability analysis method based on generalized recurrent neural network and response surface method "]
[null, "Method and device for learning recurrent neural network "]
[null, "Password dictionary generation method based on recurrent neural network "]
[null, "Attentional mechanisms embedded sentiment lexicon recurrent neural network text sentiment analysis "]
[null, "Identity verification method and device based on recurrent neural network "]
[null, "Information extraction method based on bi-directional recurrent neural network "]
[null, "Based on two-way kind of recurrent neural network approach to product design "]
["Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for environment simulation. In one aspect, a system comprises a recurrent neural network configured to, at each of a plurality of time steps, receive a preceding action for a preceding time step, update a preceding initial hidden state of the recurrent neural network from the preceding time step using the preceding action, update a preceding cell state of the recurrent neural network from the preceding time step using at least the initial hidden state for the time step, and determine a final hidden state for the time step using the cell state for the time step. The system further comprises a decoder neural network configured to receive the final hidden state for the time step and process the final hidden state to generate a predicted observation characterizing a predicted state of the environment at the time step.", "Recurrent neural networks "]
[null, "Multiple features fused bidirectional recurrent neural network fine granularity opinion mining method "]
[null, "Image description method for multi-stage connection recurrent neural network "]
[null, "Based on Recurrent Neural Networks kinds of stock price prediction methods "]
[null, "Recommendation algorithm based on recurrent neural network "]
[null, "Multimode recurrent neural network picture description method based on FCN feature extraction "]
[null, "Recurrent neural network-based community question answering expert recommendation method "]
["Estimating a response to a token instance of interest, including the steps of: receiving token instances to which a user was exposed, receiving a total response of the user to the token instances; receiving attention levels of the user in the token instances; selecting the token instance of interest from among the token instances based on the attention level; and estimating the response to the token instance of interest from the total response.", "Method and system for estimating response to token instance of interest "]
[null, "Recurrent neural network position prediction method based on time spatial context "]
[null, "Hardware architecture of model compression-based recurrent neural network accelerator "]
[null, "Dynamic model identification method and system based on recurrent neural network "]
[null, "Fundamental frequency extraction model based on LSTM recurrent neural network and training method thereof "]
["PURPOSE: To attain the practical use of a recurrent neural network by producing a reset signal to a context layer of the network if the monitored change of the time series data is coincident with a pattern chosen previously.", "Resetting method for recurrent neural network "]
[null, "Application layer multicasting tree constructing method based on two-layer recurrent neural network "]
[null, "Text classification algorithm based on multi-angle convolutional neural network and recurrent neural network "]
[null, "Gru kind of recurrent neural network-based multi-label learning "]
["An intelligent microsensor module (10, 100, 210, 300, 355, 410) is provided that can fuse data streams from a variety of sources and then locally determine the current state of the environment in which the intelligent microsensor is placed. The resultant state rather than raw data is communicated to the outside world when the microsensor is queried. The intelligent microsensor module (10, 100, 210, 300, 355, 410) of the present invention can locally determine and execute an action to be taken based on the determined state of the environment. The module (10, 100, 210, 300, 355, 410) can be readily reconfigured for multiple applications.", "Application specific intelligent microsensors "]
[null, "Recurrent neural network-based discrete emotion recognition method "]
[null, "Learning system for series connection type recurrent neural network "]
[null, "Voice identification method using long-short term memory model recurrent neural network "]
[null, "A method for computer-aided learning of a recurrent neural network for modeling a dynamic system "]
[null, "Water meter disc area detection method based on full convolution recurrent neural network "]
[null, "Recurrent neural network-based complex image character sequence recognition system "]
[null, "Secondary protein structureprediction method based on deep neural network "]
[null, "Method and program for early detection of sepsis with deep neural networks "]
[null, "Lexical item weight query learning method based on recurrent neural network "]
[null, "Multi-mechanism mixed recurrent neural network model compression method "]
[null, "Mechanical part health indicator construction method based on recurrent neural network fusion "]
[null, "Recurrent neural network method for improving time sequence data mining capability of recommendation model "]
[null, "Henhouse micro climatic environment intelligent control system based on wireless sensor network "]
[null, "DNase species removal method based on the depth of recurrent neural network in high-throughput DNA sequencing data base bias propensity "]
[null, "Text error correction method and device based on artificial intelligence recurrent neural network "]
[null, "Fiber optic gyroscope temperature drift modeling method by optimizing dynamic recurrent neural network through genetic algorithm "]
[null, "Bone conducted speech enhancement method based on deep bidirectional gate recurrent neural network "]
[null, "Recurrent neural network-based diet recommendation system "]
[null, "Complex optical text sequence identification system based on convolution and recurrent neural network "]
[null, "RNN (Recurrent Neural Network) code testing method and apparatus "]
[null, "Learning device for recurrent neural network "]
[null, "Chord composition method based on recurrent neural network "]
[null, "Recurrent neural network-based database query time prediction method "]
[null, "Deep-structure recurrent neural network-based PM2.5 prediction method "]
[null, "Image text description method based on knowledge transfer multi-modal recurrent neural network "]
[null, "Indoor positioning method based on path matching and coding decoding recurrent neural network "]
[null, "Bluetooth auxiliary wireless network positioning system based on recurrent neural network "]
[null, "Hyper-spectral image classification method based on recurrent neural network "]
[null, "Character personality analysis method based on recurrent neural network and character personality analysis device based on recurrent neural network and storage medium "]
[null, "Image target detection box selection method based on recurrent neural network "]
[null, "Method for detecting academic document plagiarism based on deep neural networks "]
[null, "Decision feedback recurrent neural network equalizer and learning method for the equalizer "]
[null, "Kinds of training methods based on recurrent neural network model of the Mongolian language "]
[null, "Series-wound long short-term memory recurrent neural network-based heating load prediction method "]
[null, "Standard recurrent neural network engine idle model identification method "]
[null, "Method for calculating the trajectory likelihood of recurrent neural network based on output state constraints "]
["A neural system is disclosed for processing an exogenous input process to produce a good outward output process with respect to a performance criterion, even if the range of one or both of these processes is necessarily large and/or keeps necessarily expanding during the operation of the neural system. The disclosed neural system comprises a recurrent neural network (RNN) and at least one range extender or reducer, each of which is a dynamic transformer. A range reducer transforms dynamically at least one component of the exogenous input process into inputs to at least one input neuron of said RNN. A range extender transforms dynamically outputs of at least one output neuron of said RNN into at least one component of the outward output process. There are many types of range extender and reducer, which have different degrees of effectiveness and computational costs. For a neural system under design, the types of range extenders and/or reducers are selected jointly with the architecture of the RNN in consideration of the neural system's processing performance with respect to the mentioned performance criterion, the RNN's size and the computational cost of selected range extenders and reducers so as to maximize the cost effectiveness of the neural system.", "Neural systems with range reducers and/or extenders "]
[null, "User session recommendation method based on deep neural network "]
[null, "Summary generation method based on recurrent neural network "]
[null, "Newborn painful expression recognition method based on deep neural network "]
[null, "Video denoising method based on deep recursive neural network "]
[null, "Online action detection using recurrent neural network "]
[null, "Neural network unit with output buffer feedback for performing recurrent neural network computations "]
[null, "Using a single-layer recurrent neural network method and apparatus for segmenting a foreground information and background information in order to perform optical character recognition labels "]
[null, "Implicit discourse relation analyzing method based on recurrent neural network "]
[null, "Unsupervised crowd abnormity monitoring and positioning method based on recurrent neural network modeling "]
[null, "Multi-target tracking method based on recurrent neural network "]
[null, "Road network state predicting method based on recurrent neural network "]
[null, "Learning system of recurrent neural network of series coupling type "]
[null, "Deep neural network compression method considering matrix correlation "]
[null, "Learning system of recurrent neural network of series coupling type "]
[null, "Quantum-behaved particle swarm optimization (QPSO) recurrent predictor neural network (RPNN) method for financial time series prediction "]
[null, "Calculation method and device for deep neural network, and readable storage medium "]
[null, "Method and device for supporting recurrent neural network structure construction "]
[null, "Learning system for serial connection type recurrent neural network "]
[null, "Password generation system based on recurrent neural network and probabilistic context free grammar "]
[null, "Deep neural network compression method of considering load balance "]
[null, "Leaning system for parallel connection type recurrent neural network "]
[null, "Recurrent neural network sparse connection method based on block tensor decomposition "]
[null, "Method for compressing deep neural network "]
[null, "Automatic kinds of training methods very deep layer neural network "]
[null, "Recommendation method and system based on recurrent neural network "]
[null, "Deep neural network-based text consistency analysis method "]
[null, "Wavefront reconstruction method based on dynamics recurrent neural network "]
[null, "Remote monitoring and online debugging method of diagonal recurrent neural network control system "]
[null, "Species 3D object recognition method recurrent neural network based on the curvature characteristics "]
[null, "Long and short-term memory type recurrent neural network-based pedestrian attribute prediction method "]
[null, "FIG species based object detector system and resolve network Recurrent Neural Network "]
[null, "Text summarization generation system and method based on coding-decoding deep neural networks "]
["A deep learning model is provided to efficiently detect disease from an image (e.g., an x-ray image) and annotate its contexts. In one example of the disclosed technology, a method of generating an annotation sequence describing an input image includes training a convolutional neural network (CNN) with a series of reference images and associated annotation sequences, training a recurrent neural network (RNN) by initializing the RNN with the trained CNN embedding of the reference image and a first word of an annotation sequence, sampling the CNN and RNN with a reference image, and producing a sequence of annotation describing the image, disease(s) in the image, one or more attributes or contexts. In one examples of the disclosed technology, mean pooling is applied to the state vectors of RNN to obtain a joint image/text context vector summarizing the contexts of image and text annotation.", "Recurrent neural feedback model for automated image annotation "]
[null, "Method for controlling a weights in the decision feedback recurrent neural network equalizer "]
[null, "Gesture recognition method based on recurrent 3D convolutional neural network "]
[null, "Method for integrating long short term memory recurrent neural network and gradient boosting decision tree "]
["The claimed identity of a person is verified from a sample of that person's speech. The speech sample is processed to extract a set of characteristic features. These features are then compared with stored features previously derived from a speech sample of the claimed identity, using a plurality of separate algorithms in parallel. The outputs of the individual algorithms are then fed to an artificial neural network which is used as a further decision algorithm to arrive at the final decision to accept or reject the speaker's identity claim. By using several different decision algorithms, and combining the outputs of those algorithms in a neural network, the speaker verification technique is more universally applicable, and hence more reliable.", "A speaker verification system "]
[null, "Webshell detection method based on deep neural network, and system thereof "]
["An apparatus for the determination of engine torque comprises a neural network receiving engine operational data, such as crankshaft rotation data, and providing an output corresponding to engine torque. The neural network may be, for example, a recurrent neural network (RNN) that is configured using training data obtained using a training process. By comparing a determined engine torque with an intended engine torque, for example determined from engine control input values such as throttle position, a useful engine diagnostic is obtained.", "Torque estimator for internal combustion engine "]
["A method for computer-aided closed and/or open-loop control of a technical system is provided. A first value of an output quantity is predicted on a data-based model at a current point in time. A second value of the output quantity is determined from an analytical model. The state of the technical system at the current point is assigned a confidence score in the correctness of prediction of the data-based model. A third value of the output quantity is determined from the first and second value as a function of the confidence score for controlling the technical system. A suitable value for the output quantity can be derived from the analytical model even for regions of the technical system in which the quality of prediction of the data-based model is low because of a small set of training data. The technical systems can be turbines, such as gas turbines.", "Method for computer-aided closed-loop and/or open-loop control of a technical system "]
["In one embodiment of the present invention, a loudspeaker modeling subsystem configures a neural lumped parameter loudspeaker (NeLP) model to represent the behavior of a loudspeaker. The NeLP model is implemented as a cascaded combination of a lumped parameter model (LPM) and a neural network (NN) model. To configure the model, the loudspeaker modeling subsystem first estimates values for the parameters used in the LPM. The loudspeaker modeling subsystem then \u201cfixes\u201d these parameters and trains the NN model to act on a predicted output pressure that is generated via the LPM. More specifically, the loudspeaker modeling subsystem configures the NN to modify the predicted output pressure to minimize the error between the predicted output pressure and a measured loudspeaker output pressure. Notably, by strategically fusing the LPM and the NN model, the NeLP model leverages the strengths and mitigates the weaknesses typically associated with conventional loudspeaker modeling techniques.", "Modeling loudspeakers based on cascading lumped parameter models with neural networks "]
["A system for training a neural network. A switch is linked to feature detectors in at least some of the layers of the neural network. For each training case, the switch randomly selectively disables each of the feature detectors in accordance with a preconfigured probability. The weights from each training case are then normalized for applying the neural network to test data.", "System and method for addressing overfitting in a neural network "]
["A neural network is trained to output a time dependent target vector defined over a predetermined time interval in response to a time dependent input vector defined over the same time interval by applying corresponding elements of the error vector, or difference between the target vector and the actual neuron output vector, to the inputs of corresponding output neurons of the network corrective feedback. This feedback decreases the error and quickens the learning process, so that a much smaller number of training cycles are required to complete the learning process. A conventional gradient descent algorithm is employed to update the neural network parameters at the end of the predetermined time interval. The foregoing process is repeated in repetitive cycles until the actual output vector corresponds to the target vector. In the preferred embodiment, as the overall error of the neutral network output decreases during successive training cycles, the portion of the error fed back to the output neurons is decreased accordingly, allowing the network to learn with greater freedom from teacher forcing as the network parameters converge to their optimum values. The invention may also be used to train a neural network with stationary training and target vectors.", "Fast temporal neural learning using teacher forcing "]
[null, "Neural-network-based control esp. for multi-degree of freedom robot three-arm manipulator - represents geometric characteristics of arms by model, and calculates full set of state variables e.g. arm angles and both end-point coordinates of manipulator to obtain best solution "]
["Provided is a compiler to map application program code to object code capable of being executed on an operating system platform. A first neural network module is trained to generate characteristic output based on input information describing attributes of the application program. A second neural network module is rained to receive as input the application program code and the characteristic output and, in response, generate object code. The first and second neural network modules are used to convert the application program code to object code.", "Method, system, and program for converting application program code to executable code using neural networks based on characteristics of the inputs "]
["The invention relates to automatically configuring of a lighting, particularly to creating a lighting, which follows a person, with a networked lighting system. A basic idea of the invention is to configure lighting in a network of lamps in that a lamp of the network adjusts its light emission depending on presence detection in its own direct environment and the presence detected in the environment of other lamps of the network. An embodiment of the invention relates to a system (10) for automatically configuring a lighting, wherein the system comprises - a network of lamps (12), in which every lamp is coupled to a presence detector (14) and can receive signals from other lamps in the network, wherein a received signal indicates an activity detected by the presence detector coupled to the lamp, which transmits the signal, and wherein - every lamp adjusts its light emission depending on the signal received from other lamps and the measurement of its presence detector. The invention allows automatically configuring a lighting with a network of lamps for a certain area in that lights go on before a person gets to a certain area.", "Automatically configuring of a lighting "]
["A dynamic bandwidth allocation method of an Ethernet passive optical network, comprises a predictor and a rule of QoS-promoted dynamic bandwidth allocation (PQ-DBA); the predictor predicts a client behavior and numbers of various kinds of packets by using a pipeline scheduling predictor consisted of a pipelined recurrent neural network (PRNN), and a learning rule of the extended recursive least squares (ERLS); the present invention establishes a better QoS traffic management for the OLT-allocated ONU bandwidth and client packets sent by priority.", "Dynamic Bandwidth Allocation Method of Ethernet Passive Optical Network "]
["A motor vehicle related operating signal, such as a torque signal produced by operation of an engine of the motor vehicle, is monitored to produce a frequency signature for the vehicle. The frequency signature is filtered by a fuzzy spectral filter to extract a frequency membership for the vehicle which is utilized to generate characteristic signals representative of the vehicle. Signals representative of vehicle inertia (J), vehicle horsepower (HP) and relative vehicle temperature (RVT) are extracted by a radial basis function (RBF) neural network. These characteristic signals are then used to control the vehicle directly via a powertrain control module (PCM) or via a robot driver for vehicle test purposes. For robot control, an anticipated throttle lag and an anticipated brake lag are generated and used to more accurately and repeatedly control the robot to simulate a human driver in following acceleration curves for vehicle testing purposes.", "System and method for distinguishing and characterizing motor vehicles for control of automatic drivers "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating representation of acoustic sequences. One of the methods includes: receiving an acoustic sequence, the acoustic sequence comprising a respective acoustic feature representation at each of a plurality of time steps; processing the acoustic feature representation at an initial time step using an acoustic modeling neural network; for each subsequent time step of the plurality of time steps: receiving an output generated by the acoustic modeling neural network for a preceding time step, generating a modified input from the output generated by the acoustic modeling neural network for the preceding time step and the acoustic representation for the time step, and processing the modified input using the acoustic modeling neural network to generate an output for the time step; and generating a phoneme representation for the utterance from the outputs for each of the time steps.", "Generating representations of acoustic sequences "]
[null, "Neural network-based low amplitude network flow anomaly detection method "]
["A learning apparatus includes a storage unit configured to store a network formed by a plurality of nodes each holding dynamics; a learning unit configured to learn the dynamics of the network in a self-organizing manner on the basis of observed time-series data; a winner-node determiner configured to determine a winner node, the winner node being a node having dynamics that best match the time-series data; and a weight determiner configured to determine learning weights for the dynamics held by the individual nodes according to distances of the individual nodes from the winner node. The learning unit is configured to learn the dynamics of the network in a self-organizing manner by degrees corresponding to the learning weights.", "Learning apparatus, learning method, and program for efficiently learning dynamics "]
["Presented herein are embodiments of state-of-the-art speech recognition systems developed using end-to-end deep learning. In embodiments, the model architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, embodiments of the system do not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learn a function that is robust to such effects. A phoneme dictionary, nor even the concept of a \u201cphoneme,\u201d is needed. Embodiments include a well-optimized recurrent neural network (RNN) training system that can use multiple GPUs, as well as a set of novel data synthesis techniques that allows for a large amount of varied data for training to be efficiently obtained. Embodiments of the system can also handle challenging noisy environments better than widely used, state-of-the-art commercial speech systems.", "Systems and methods for speech transcription "]
["The invention describes a method and apparatus for effectively communicating data along the acoustic channel of a subterranean well. The method comprises optimally driving an acoustic transmitter with an adaptive transmitter controller. A data signal is transmitted along the acoustic channel and detected as a distorted signal along the acoustic channel. The distorted signal is input to the adaptive transmitter controller which, based on the detected signal, modifies later transmissions to counteract the distorting effects of the transmitter and acoustic channel. The adaptive transmitter controller preferably comprises a neural network. Another receiver may be employed, at a point further from the transmitter, to receive the optimized signals.", "Adaptive acoustic transmitter controller apparatus and method "]
["Described herein are systems and methods for generating and using attention-based deep learning architectures for visual question answering task (VQA) to automatically generate answers for image-related (still or video images) questions. To generate the correct answers, it is important for a model's attention to focus on the relevant regions of an image according to the question because different questions may ask about the attributes of different image regions. In embodiments, such question-guided attention is learned with a configurable convolutional neural network (ABC-CNN). Embodiments of the ABC-CNN models determine the attention maps by convolving image feature map with the configurable convolutional kernels determined by the questions semantics. In embodiments, the question-guided attention maps focus on the question-related regions and filters out noise in the unrelated regions.", "Systems and methods for attention-based configurable convolutional neural networks (abc-cnn) for visual question answering "]
[null, "Translation method and translation device based on neural network model "]
[null, "Method and system for enhancing features by aid of bidirectional long-term and short-term memory recurrent neural networks "]
["A method for the computer-aided control of a technical system is provided. A recurrent neuronal network is used for modeling the dynamic behaviour of the technical system, the input layer of which contains states of the technical system and actions carried out on the technical system, which are supplied to a recurrent hidden layer. The output layer of the recurrent neuronal network is represented by an evaluation signal which reproduces the dynamics of technical system. The hidden states generated using the recurrent neural network are used to control the technical system on the basis of a learning and/or optimization method.", "Method for the computer-aided control of a technical system "]
["A method of reducing image resolution in a deep convolutional network (DCN) includes dynamically selecting a reduction factor to be applied to an input image. The reduction factor can be selected at each layer of the DCN. The method also includes adjusting the DCN based on the reduction factor selected for each layer.", "Reducing image resolution in deep convolutional networks "]
["Embodiments of the present invention provide a method implemented by a computer program for detecting and identifying valve failure in a reciprocating compressor and further for predicting valve failure in the compressor. Embodiments of the present invention detect and predict the valve failure using wavelet analysis, logistic regression, and neural networks. A pressure signal from the valve of the reciprocating compressor presents a non-stationary waveform from which features can be extracted using wavelet packet decomposition. The extracted features, along with temperature data for the valve, are used to train a logistic regression model to classify defective and normal operation of the valve. The wavelet features extracted from the pressure signal are also used to train a neural network model to predict to predict the future trend of the pressure signal of the system, which is used as an indicator for performance assessment and for root cause detection of the compressor valve failures.", "Computer program and method for detecting and predicting valve failure in a reciprocating compressor "]
["A method for approximation of optimal control for a nonlinear discrete time system in which the state variables are first obtained from a system model. Control sequences are then iteratively generated for the network to optimize control variables for the network and in which the value for each control variable is independent of the other control variables. Following optimization of the control variables, the control variables are then mapped onto a recurrent neural network utilizing conventional training methods.", "Method for approximation of optimal control for nonlinear discrete time systems "]
["Methods and systems for detecting misfire events in a multicylinder engine are disclosed. One method includes associating a neural network with a cylinder of a multicylinder engine. The method also includes inputting to the neural network a plurality of crankshaft parameters. The method further includes determining the existence of an engine misfire in the cylinder based on the output of the neural network.", "Neural network-based engine misfire detection systems and methods "]
["As part of an analysis of the likelihood that a given input (e.g. a file, etc.) includes malicious code, a convolutional neural network can be used to review a sequence of chunks into which an input is divided to assess how best to navigate through the input and to classify parts of the input in a most optimal manner. At least some of the sequence of chunks can be further examined using a recurrent neural network in series with the convolutional neural network to determine how to progress through the sequence of chunks. A state of the at least some of the chunks examined using the recurrent neural network summarized to form an output indicative of the likelihood that the input includes malicious code. Methods, systems, and articles of manufacture are also described.", "Neural attention mechanisms for malware analysis "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for recognizing keywords using a long short term memory neural network. One of the methods includes receiving, by a device for each of multiple variable length enrollment audio signals, a respective plurality of enrollment feature vectors that represent features of the respective variable length enrollment audio signal, processing each of the plurality of enrollment feature vectors using a long short term memory (LSTM) neural network to generate a respective enrollment LSTM output vector for each enrollment feature vector, and generating, for the respective variable length enrollment audio signal, a template fixed length representation for use in determining whether another audio signal encodes another spoken utterance of the enrollment phrase by combining at most a quantity k of the enrollment LSTM output vectors for the enrollment audio signal.", "User specified keyword spotting using long short term memory neural network feature extractor "]
[null, "Digital neural network processor for consumer goods, games, telecommunications or medical equipment or vehicle "]
[null, "Simulating signal from electronic sensor in motor vehicle using virtual sensor in vehicle control device, based on neural network model "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for augmenting neural networks with an external memory. One of the systems includes a memory interface subsystem that is configured to perform operations comprising determining a respective content-based weight for each of a plurality of locations in an external memory; determining a respective allocation weight for each of the plurality of locations in the external memory; determining a respective final writing weight for each of the plurality of locations in the external memory from the respective content-based weight for the location and the respective allocation weight for the location; and writing data defined by the write vector to the external memory in accordance with the final writing weights.", "Augmenting neural networks with external memory "]
["This document generally describes systems, methods, devices, and other techniques related to speaker verification, including (i) training a neural network for a speaker verification model, (ii) enrolling users at a client device, and (iii) verifying identities of users based on characteristics of the users' voices. Some implementations include a computer-implemented method. The method can include receiving, at a computing device, data that characterizes an utterance of a user of the computing device. A speaker representation can be generated, at the computing device, for the utterance using a neural network on the computing device. The neural network can be trained based on a plurality of training samples that each: (i) include data that characterizes a first utterance and data that characterizes one or more second utterances, and (ii) are labeled as a matching speakers sample or a non-matching speakers sample.", "Neural Networks For Speaker Verification "]
["Apparatuses and methods for image processing are provided. The image processing apparatus performs area classification and object detection in an image, and includes a feature map generator configured to generate the feature map of the input image using the neural network, and an image processor configured to classify the areas and to detect the objects in the image using the generated feature map.", "Image processing apparatus and method based on deep learning and neural network learning "]
["In an artificial neural network a method and neuron device that produce weight-adjustment factors, also called error values (116), for pre-synaptic neurons (302 a . . .  302 c) that are used to adjust the values of connection weights (106 . . . 106 n) in neurons (100) used in artificial neural networks (ANNs). The amount of influence a pre-synaptic neuron has had over a post-synaptic neuron is calculated during signal propagation in the post-synaptic neuron (422 a . . .  422 n) and accumulated for the pre-synaptic neuron (426) for each post-synaptic neuron to which the pre-synaptic neuron's output is connected (428). Influence values calculated for use by pre-synaptic neurons may further be modified by the post-synaptic neuron's output value (102) (option 424), and its error value (116) (option 1110).", "Feedback-tolerant method and device producing weight-adjustment factors for pre-synaptic neurons in artificial neural networks "]
[null, "Multi-target modeling method for complex industrial process "]
["Neural networks may be used in certain automatic speech recognition systems. To improve performance of these neural networks, they may be updated/retrained during run time by training the neural network based on the output of a speech recognition system or based on the output of the neural networks themselves. The outputs may include weighted outputs, lattices, weighted N-best lists, or the like. The neural networks may be acoustic model neural networks or language model neural networks. The neural networks may be retrained after each pass through the network, after each utterance, or in varying time scales.", "Adaptive neural network speech recognition models "]
["Described is a system for searching a continuous data stream for exact matches with a priori stored data sequences. The system includes a neural network with an input and an output layer. The input layer has one neuron for each possible character or number in the data stream, and the output layer has one neuron for each stored pattern. Importantly, the delays of the connections from input to output layer are engineered to match the temporal occurrence of an input character within a stored sequence. Thus, if an input sequence has the proper time gaps between characters, matching a stored pattern, then the delayed neural signals result in a simultaneous activation at the receiving neuron, which indicates a detected pattern. For storing a pattern, only one connection for each pair of input character and output neuron has to be specified resulting in sparse coding and quick storage.", "Neural network device with engineered delays for pattern storage and matching "]
["Electronic communications can be normalized using a neural network. For example, a noncanonical communication that includes multiple terms can be received. The noncanonical communication can be preprocessed by (I) generating a vector including multiple characters from a term of the multiple terms; and (II) repeating a substring of the term in the vector such that a last character of the substring is positioned in a last position in the vector. The vector can be transmitted to a neural network configured to receive the vector and generate multiple probabilities based on the vector. A normalized version of the noncanonical communication can be determined using one or more of the multiple probabilities generated by the neural network. Whether the normalized version of the noncanonical communication should be outputted can also be determined using at least one of the multiple probabilities generated by the neural network.", "Normalizing electronic communications using a neural network "]
["A system for detecting a network intrusion includes a first neural network for determining a first plurality of weight values corresponding to a plurality of vectors of an input data, a second neural network for updating the first plurality of weight values received from the first neural network to a second plurality of weight values based on the plurality of vectors of the input data, a third neural network for updating the second plurality of weight values received from the second neural network to a third plurality of weight values based on the plurality of vectors of the input data, and a classification module for classifying the plurality of vectors under at least one of a plurality of intrusions based on the third plurality of weight values received from the third neural network.", "Feature Based Three Stage Neural Network Intrusion Detection "]
["The present invention relates to a method and a program for bone age calculation using deep neural networks. The method for bone age calculation using deep neural networks, according to an embodiment of the present invention, comprises: a step (S200) of receiving an image to be analyzed (200) which is a specific medical image subject to bone age calculation; and a step (S400) of calculating, by means of at least one computer (100), bone age by analyzing the medical image using deep neural networks. The present invention calculates bone age by accumulating, as big data, medical images of a specific race (especially, Koreans) and analyzing the medical images, thereby being able to calculate accurate bone age which coincides with a race.", "Method and program for bone age calculation using deep neural networks "]
[null, "System and method for teaching movement to leg type robot "]
["A method and system detects actions of an object in a scene by first acquiring a video of the scene as a sequence of images, wherein each image includes pixels, wherein the video is partitioned into chunks. The object in the video is tracked. For each object and each chunk of the video, trajectories of the pixels within a bounding box located over the object are tracked, and cropped trajectories and cropped images for one or more images in the chunk are produced using the bounding box. Then, the cropped trajectories and cropped images are passed to a recurrent neural network (RNN) that outputs a relative score for each action of interest.", "Method and System for Detecting Actions in Videos "]
["A method for the computer-assisted control and/or regulation of a technical system is provided. The method includes two steps, namely learning the dynamics of a technical system using historical data based on a recurrent neuronal network, and the subsequent learning of an optimum regulation by coupling the recurrent neuronal network to another neuronal network. The method can be used with any technical system in order to control the system in an optimum computer-assisted manner. For example, the method can be used in the control of a gas turbine.", "Method for the computer-assisted control and/or regulation of a technical system "]
["Deep Neural Networks (DNN) are time shifted relative to one another and trained. The time-shifted networks may then be combined to improve recognition accuracy. The approach is based on an automatic speech recognition (ASR) system using DNN and using time shifted features. Initially, a regular ASR model is trained to produce a first trained DNN. Then a top layer (e.g., SoftMax layer) and the last hidden layer (e.g., Sigmoid) are fine-tuned with same data set but with a feature window left- and right-shifted to create respective second and third left-shifted and right-shifted DNNs. From these three DNN networks, four combination networks may be generated: left- and right-shifted, left-shifted and centered, centered and right-shifted, and left-shifted, centered, and right-shifted. The centered networks are used to perform the initial (first-pass) ASR. Then the other six networks are used to perform rescoring. The resulting are combined using ROVER (recognizer output voting error reduction) or another technique to improve recognition performance as compared to the centered DNN by itself", "Centered, left- and right-shifted deep neural networks and their combinations "]
["A method transforms a noisy audio signal to an enhanced audio signal, by first acquiring the noisy audio signal from an environment. The noisy audio signal is processed by an enhancement network having network parameters to jointly produce a magnitude mask and a phase estimate. Then, the magnitude mask and the phase estimate are used to obtain the enhanced audio signal.", "Method for Enhancing Audio Signal using Phase Information "]
[null, "Method for simulating system having solar generator/wind generator/cogenerator "]
["A method of biasing a deep neural network includes determining whether an element has an increased probability of being present in an input to the network. The method also includes adjusting a bias of activation functions of neurons in the network to increase sensitivity to the element. In one configuration, the bias is adjusted without adjusting weights of the network. The method further includes adjusting an output of the network based on the biasing.", "Incorporating top-down information in deep neural networks via the bias term "]
["A plausible neural network (PLANN) is an artificial neural network with weight connection given by mutual information, which has the capability of inference and learning, and yet retains many characteristics of a biological neural network. The learning algorithm is based on statistical estimation, which is faster than the gradient decent approach currently used. The network after training becomes a fuzzy/belief network; the inference and weight are exchangeable, and as a result, knowledge extraction becomes simple. PLANN performs associative memory, supervised, semi-supervised, unsupervised learning and function/relation approximation in a single network architecture. This network architecture can easily be implemented by analog VLSI circuit design.", "Plausible neural network with supervised and unsupervised cluster analysis "]
[null, "Emotion recognition device and method, emotion recognition method of robot device, learning method of robot device and robot device "]
["A system for object recognition in which a multi-dimensional scanner generates a temporal sequence of multi-dimensional output data of a scanned object. That data is then coupled as an input signal to a trainable dynamic system. The system exemplified by a general-purpose recurrent neural network is previously trained to generate an output signal representative of the class of the object in response to a temporal sequence of multi-dimensional data.", "Method and system for object recognition based on a trainable dynamic system "]
[null, "Road traffic information detection method "]
["An information processing apparatus includes a first recurrent neural network (RNN) for performing processing which corresponds to a time-series and a second RNN for processing another correlated time-series. The difference between a context set output by the first RNN and a context set output by the second RNN is computed by a subtractor, and the obtained difference is used as a prediction error. Backpropagation is performed based on the prediction error, thus determining a coefficient for each neuron of an output layer, an intermediate layer, and an input layer.", "Information processing apparatus and method, and recording medium "]
[null, "Fuzzy fusion identification method of rotating speed of sensorless motor "]
["Systems and methods for iris authentication are disclosed. In one aspect, a deep neural network (DNN) with a triplet network architecture can be trained to learn an embedding (e.g., another DNN) that maps from the higher dimensional eye image space to a lower dimensional embedding space. The DNN can be trained with segmented iris images or images of the periocular region of the eye (including the eye and portions around the eye such as eyelids, eyebrows, eyelashes, and skin surrounding the eye). With the triplet network architecture, an embedding space representation (ESR) of a person's eye image can be closer to the ESRs of the person's other eye images than it is to the ESR of another person's eye image. In another aspect, to authenticate a user as an authorized user, an ESR of the user's eye image can be sufficiently close to an ESR of the authorized user's eye image.", "Deep neural network for iris identification "]
["Fully automated methods and systems for processing complex data sets to identify abnormalities are described. In one embodiment, the system includes wavelet processing, recursive processing to determine prominent features, and then utilizing feed forward neural networks (FFNNs) to classify feature vectors generated in the wavelet and recursive processing. With respect to wavelet processing, multiresolution (five-level) and multidirection (two-dimensional) wavelet analysis with quadratic spline wavelets is performed to transform each image. The wavelets are a first-order derivative of a smoothing function and enhance the edges of image objects. Because two-dimensional wavelet transforms quantize an image in terms of space and spatial frequency and can be ordered linearly, the data is processed recursively to determine prominent features. A neural network approach derived from sequential recursive auto-associative memory is then used to parse the wavelet coefficients and hierarchy data. Since the wavelet coefficients are continuous, linear output instead of sigmoidal output is used. This variation is therefore referred to as linear output sequential recursive auto-associative memory, or LOSRAAM. The objective of training the LOSRAAM network is to have the output exactly match the input. Context units arising from serial evaluation of the wavelet coefficient triplets may be collected as vectors. These vectors are subjected to cluster analysis. This analysis yields a number of identifiable and discrete states. From these states, feature vectors are created. Each element in the feature vector represents the number of times the corresponding state from the above cluster analysis is found. Then, feed forward neural networks (FFNNs) are trained to classify feature vectors.", "Neural network based methods and systems for analyzing complex data "]
["The invention concerns a neural net system comprising a plurality of levels (20, 21, 22) of interconnected computing elements (10, 11), said levels comprising an input level (20) whereto a sequence of input speech vectors (2) can be applied at least at a first rate. At least one computing element (10) includes a supervisory classifier (102) to reduce the rate of the delivered signal. A parametering unit enables to deliver said speech vectors (2) from a digital speech signal. The invention enables speech recognition from a set of speech vectors corresponding to a large number of frames of the speech signal.", "Neural network and its use for speech recognition "]
[null, "Deep learning artificial neural network-based task provision platform "]
["A prediction model is generated by training an ensemble of multiple neural networks, and estimating the performance error of the ensemble. In a subsequent stage a subsequent ensemble is trained using an adapted training set so that the preceding bias component of performance error is modelled and compensated for in the the new ensemble. In each successive stage the error is compared with that of all of the preceding ensembles combined. No further stages take place when there is no improvement in error. Within each stage, the optimum number of iterative weight updates is determined, so that the variance component of performance error is minimised.", "Neural network training "]
["A recursive neurofilter comprising a recursive neural network (NN) is disclosed for processing an information process to estimate a signal process with respect to an estimation error criterion. The information process either consists of a measurement process, or if the signal and measurement processes are time-variant, consists of the measurement process as well as a time variance process, that describes the time-variant properties of the signal and measurement processes. The recursive neurofilter is synthesized from exemplary realizations of the signal and information processes. No assumptions such as Gaussian distribution, linear dynamics, additive noise, and Markov property are required. The synthesis is performed essentially through training recursive NNs. The training criterion is constructed to reflect the mentioned estimation error criterion with the exemplary realizations. If an estimation error statistics process of a primary recursive neurofilter is needed, an ancillary recursive neurofilter is used to produce an approximate of this estimation error statistics process. An ancillary recursive neurofilter inputs either said primary recursive neurofilter's input process or said primary recursive neurofilter's input and output processes.", "Recursive neural filters "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for compressing images using neural networks. One of the methods includes receiving an image; processing the image using an encoder neural network, wherein the encoder neural network is configured to receive the image and to process the image to generate an output defining values of a first number of latent variables that each represent a feature of the image; generating a compressed representation of the image using the output defining the values of the first number of latent variables; and providing the compressed representation of the image for use in generating a reconstruction of the image.", "Compressing images using neural networks "]
["A learning apparatus for learning time series data, includes a learning unit for updating, in a self-organizing manner based on an observed value of the time series data, a time series pattern storage network including a plurality of nodes, each node having a time series pattern model representing a time series pattern of the time series data.", "Method and apparatus for learning data, method and apparatus for recognizing data, method and apparatus for generating data, and computer program "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for training a neural network. One of the methods includes obtaining partitioned training data for the neural network, wherein the partitioned training data comprises a plurality of training items each of which is assigned to a respective one of a plurality of partitions, wherein each partition is associated with a respective difficulty level; and training the neural network on each of the partitions in a sequence from a partition associated with an easiest difficulty level to a partition associated with a hardest difficulty level, wherein, for each of the partitions, training the neural network comprises: training the neural network on a sequence of training items that includes training items selected from the training items in the partition interspersed with training items selected from the training items in all of the partitions.", "Training neural networks on partitioned training data "]
["Electronic communications can be normalized using neural networks. For example, an electronic representation of a noncanonical communication can be received. A normalized version of the noncanonical communication can be determined using a normalizer including a neural network. The neural network can receive a single vector at an input layer of the neural network and transform an output of a hidden layer of the neural network into multiple values that sum to a total value of one. Each value of the multiple values can be a number between zero and one and represent a probability of a particular character being in a particular position in the normalized version of the noncanonical communication. The neural network can determine the normalized version of the noncanonical communication based on the multiple values. Whether the normalized version should be output can be determined based on a result from a flagger including another neural network.", "Normalizing electronic communications using a neural-network normalizer and a neural-network flagger "]
["The present application relates to an environment control system. The environment control system is capable of detecting variations of natural environment and artificial environment, and control the use of electronic devices automatically or semi-automatically. Based on collected information stored in a built-in storage module, the environment control system may calculate and learn the user's habit of use with respect to the electronic devices using network connection.", "Environment control system "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating images using neural networks. One of the methods includes generating the output image pixel by pixel from a sequence of pixels taken from the output image, comprising, for each pixel in the output image, generating a respective score distribution over a discrete set of possible color values for each of the plurality of color channels.", "Generating images using neural networks "]
["A method, computer readable medium, and system are disclosed for classifying video image data. The method includes the steps of processing training video image data by at least a first layer of a convolutional neural network (CNN) to extract a first set of feature maps and generate classification output data for the training video image data. Spatial classification accuracy data is computed based on the classification output data and target classification output data and spatial discrimination factors for the first layer are computed based on the spatial classification accuracies and the first set of feature maps.", "Fusing multilayer and multimodal deep neural networks for video classification "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for augmenting neural networks to generate additional outputs. One of the systems includes a neural network and a sequence processing subsystem, wherein the sequence processing subsystem is configured to perform operations comprising, for each of the system inputs in a sequence of system inputs: receiving the system input; generating an initial neural network input from the system input; causing the neural network to process the initial neural network input to generate an initial neural network output for the system input; and determining, from a first portion of the initial neural network output for the system input, whether or not to cause the neural network to generate one or more additional neural network outputs for the system input.", "Augmenting neural networks to generate additional outputs "]
["Apparatus for generating a second one or more sequences of elements after receiving a first one or more sequence of elements. The apparatus comprises", "Apparatus for generating sequences of elements "]
["Systems, methods, devices, and other techniques are disclosed for using an augmented neural network system to generate a sequence of outputs from a sequence of inputs. An augmented neural network system can include a controller neural network, a hierarchical external memory, and a memory access subsystem. The controller neural network receives a neural network input at each of a series of time steps processes the neural network input to generate a memory key for the time step. The external memory includes a set of memory nodes arranged as a binary tree. To provide an interface between the controller neural network and the external memory, the system includes a memory access subsystem that is configured to, for each of the series of time steps, perform one or more operations to generate a respective output for the time step. The capacity of the neural network system to account for long-range dependencies in input sequences may be extended. Also, memory access efficiency may be increased by structuring the external memory as a binary tree.", "Augmenting Neural Networks With Hierarchical External Memory "]
["A method and system are provided for speech recognition. The speech recognition method includes the steps of preparing training data representing acoustic parameters of each of phonemes at each time frame; receiving an input signal representing a sound to be recognized and converting the input signal to input data; comparing the input data at each frame with the training data of each of the phonemes to derive a similarity measure of the input data with respect to each of the phonemes; and processing the similarity measures obtained in the comparing step using a neural net model governing development of activities of plural cells to conduct speech recognition of the input signal. In the processing step, each cell is associated with one respective phoneme and one frame, a development of the activity of each cell at each frame in the neural net model is suppressed by the activities of other cells on the same frame corresponding to different phonemes, and the development of the activity of each cell at each frame being enhanced by the activities of other cells corresponding to the same phoneme at different frames. In the process, the phoneme of a cell that has developed the highest activity is determined as a winner at the corresponding frame to produce a list of winners at respective frames. A phoneme is outputted as a recognition result for the input signal in accordance with the list of the winners at the respective frames that have been determined in the step of processing.", "Acoustic speech recognition method and system using stereo vision neural networks with competition and cooperation "]
["Features are disclosed for using a neural network to tag sequential input without using an internal representation of the neural network generated when scoring previous positions in the sequence. A predicted or determined label (e.g., the highest scoring or otherwise most probable label) for input at a given position in the sequence can be used when scoring input corresponding to the next position the sequence. Additional features are disclosed for training a neural network for use in tagging sequential input without using an internal representation of the neural network generated when scoring previous positions the sequence.", "Markov-based sequence tagging using neural networks "]
["Recurrent conditional random field (R-CRF) embodiments are described. In one embodiment, the R-CFR receives feature values corresponding to a sequence of words. Semantic labels for words in the sequence of words are then generated and each label is assigned to the appropriate one of the words in the sequence of words. The R-CRF used to accomplish these tasks includes a recurrent neural network (RNN) portion and a conditional random field (CRF) portion. The RNN portion receives feature values associated with a word in the sequence of words and outputs RNN activation layer activations data that is indicative of a semantic label. The CRF portion inputs the RNN activation layer activations data output from the RNN for one or more words in the sequence of words and outputs label data that is indicative of a separate semantic label that is to be assigned to each of the words.", "Recurrent conditional random fields "]
["In one embodiment, a method comprises receiving a selection of a neural network topology type; identifying a synapse memory mapping scheme for the selected neural network topology type from a plurality of synapse memory mapping schemes that are each associated with a respective neural network topology type; and mapping a plurality of synapse weights to locations in a memory based on the identified synapse memory mapping scheme.", "Neuromorphic computer with reconfigurable memory mapping for various neural network topologies "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for processing word sequences using neural networks. One of the methods includes receiving a first sequence of words arranged according to a first order; and for each word in the first sequence, beginning with a first word in the first order: determining a topic vector that is associated with the word; generating a combined input from the word and the topic vector, and processing the combined input through one or more sequence modeling layers to generate a sequence modeling output for the word; and processing one or more of the sequence modeling outputs through an output layer to generate a neural network output for the first sequence of words.", "Topic-based sequence modeling neural networks "]
["The present invention relates to an improved artificial neural network for predicting one or more next items in a sequence of items based on an input sequence item. The improved artificial neural network has greatly reduced memory requirements, making it suitable for use on electronic devices such as mobile phones and tablets. The invention includes an electronic device on which the improved artificial neural network operates, and methods of predicting the one or more next items in the sequence using the improved artificial neural network.", "Improved artificial neural network for language modelling and prediction "]
["Example implementations include a system and method of recognizing behavior of a user. In example implementations, a first post and at least one subsequent post indicative of a product and associated with a first social media account is obtained. A relevance probability is calculated for each of the obtained first post and the at least one subsequent post. The obtained first post and the at least one subsequent post are sequentially analyzed by a second neural network to determine output values relevant to probability of purchasing the product. A probability of purchasing the product is calculated based on the determined output values associated with each post and the calculated relevance probabilities. Product-related information is transmitted to the user associated with the obtained first post based on the determined probability of purchasing the product.", "Behavior prediction on social media using neural networks "]
[null, "Method for intelligent information processing in neural network "]
["A method of predicting action labels for a video stream includes receiving the video stream and calculating an optical flow of consecutive frames of the video stream. An attention map is generated from the current frame of the video stream and the calculated optical flow. An action label is predicted for the current frame based on the optical flow, a previous hidden state and the attention map.", "Recurrent networks with motion-based attention for video understanding "]
["Disclosed herein is an intelligent agent to analyze a media object. The agent comprises a trained model comprising a number of state layers for storing a history of actions taken by the agent in each of a number of previous iterations performed by the agent in analyzing a media object. The stored state may be used by the agent in a current iteration to determine whether or not to make, or abstain from making, a prediction from output generated by the model, identify another portion of the media object to analyze, end analysis. Output from the agent's model may comprise a semantic vector that can be mapped to a semantic vector space to identify a number of labels for a media object.", "Media content analysis system and method "]
[null, "Method for intelligent information processing in neural network "]
["A method and a system generate a time-series signal indicative of a variation of the environment in vicinity of the vehicle with respect to a motion of the vehicle and submit the time-series signal to the neural network to produce a reference trajectory as a function of time that satisfies time and spatial constraints on a position of the vehicle. The neural network is trained in to transform time-series signals to reference trajectories of the vehicle. The motion trajectory tracking the reference trajectory while satisfying constraints on the motion of the vehicle is determined and the motion of the vehicle is controlled to follow the motion trajectory.", "System and method for controlling vehicle using neural network "]
["A neural network model, such as a deep neural network (DNN), is trained using many speech examples to perform beam selection in a microphone array-based speech processing system. The DNN is trained using many different speech examples that are labeled with position or direction information relative to a training microphone array. The DNN may then be trained to recognize a direction of incoming speech so that at runtime the trained DNN may process input audio data from a microphone array and may output to a beam selector an indicator of the desired beam that may be selected for further processing. The DNN may be configured to output a beam index and/or coordinates (or other position data) corresponding to an estimated location of the detected speech. The DNN may also be configured to output acoustic unit data corresponding to speech units (for example corresponding to phonemes, senons, etc. such as those of a detected wakeword or other word).", "Neural network based beam selection "]
[null, "Method for intelligent information processing in neural network "]
["A neural network designing method forms a RNN (Recurrent Neural Network) circuit to include a plurality of oscillating RNN circuits configured to output natural oscillations, and an adding circuit configured to obtain a sum of outputs of the plurality of oscillating RNN circuits, and inputs discrete data to the plurality of oscillating RNN circuits in order to compute a fitting curve with respect to the discrete data output from the adding circuit.", "Neural network designing method and digital-to-analog fitting method "]
["A system and method are presented for neural network based feature extraction for acoustic model development. A neural network may be used to extract acoustic features from raw MFCCs or the spectrum, which are then used for training acoustic models for speech recognition systems. Feature extraction may be performed by optimizing a cost function used in linear discriminant analysis. General non-linear functions generated by the neural network are used for feature extraction. The transformation may be performed using a cost function from linear discriminant analysis methods which perform linear operations on the MFCCs and generate lower dimensional features for speech recognition. The extracted acoustic features may then be used for training acoustic models for speech recognition systems.", "System and method for neural network based feature extraction for acoustic model development "]
[null, "Method for intelligent information processing in neural network "]
[null, "Image recognition method using deep learning analysis modular systems "]
[null, "An apparatus for generating paragraph based on artificial neural network and method thereof "]
[null, "Device for predicting power generated after several hours on basis of predicted wind velocity in wind power generator "]
[null, "Neural network circuit device, a neural network, the neural network processing method and a neural network executing program "]
[null, "Neural network system and the robot camera controller and a neural network program using the same "]
["Methods and apparatus for extending a neural network, reducing its dimension and processing input data are provided. The method of extending a neural network involves selecting, with a processor, a node of a neural network, adding a new node in a layer that includes the selected node, and setting connection weights of the new node based on connection weights of the selected node.", "Method and apparatus for extending neural network "]
[null, "Method and recording medium for automatic composition using artificial neural network "]
["The present invention relates to a method for recognizing speech, which method\nleads to an improved recognition rate compared to prior art. Within the method a\nlanguage model is applied which is based on attribute information of a word,\nwhich is descriptive for syntactic and/or semantic information and/or the like of\nthe respective word. The method for recognizing speech according to the invention,\ncomprises the steps of receiving (S0) a speech input (SI), generating (S1) a\nset of ordered hypotheses (OH), wherein each hypothesis contains at least one\nhypothesis word, generating (S2) attribute information (AI) for at least one of said\nat least one hypothesis word, the attribute information being generated to be descriptive\nfor syntactic and/or semantic information and/or the like of the respective\nhypothesis word, using (S3) a language model (LM) which is based on said\nattribute information (AI) to calculate word probabilities for said at least one of\nsaid at least one hypothesis word, which word probabilities are descriptive for the\nposterior probabilities of the respective hypothesis word given a plurality of previous\nhypothesis words, using (S4) said word probabilities for generating a set of re-ordered\nhypotheses (ROH), choosing (S5) at least one best hypothesis (BH) from\nsaid set of re-ordered hypotheses (ROH) as a recognition result (RR),\nand outputting (S6) said recognition result.", "Method for Recognizing Speech with attributes "]
[null, "Neural Networks for selecting to be performed by a robot Agent Actions "]
["Object detection and across disparate fields of view are provided. A first image generated by a first recording device with a first field of view, and a second image generated by a second recording device with a second field of view can be obtained. An object detection component can detect a first object within the first field of view, and a second object within the second field of view. An object classification component can determine first and second level classification categories of the first object. Object components can correlate the first object with the second object based on the descriptor of the first object or a descriptor of the second object, and can determine a characteristic or the first object or the second object based on the correlation.", "Object detection and classification "]
["An apparatus and method are described for reducing the parameter density of a deep neural network (DNN). A layer-wise pruning module to prune a specified set of parameters from each layer of a reference dense neural network model to generate a second neural network model having a relatively higher sparsity rate than the reference neural network model; a retraining module to retrain the second neural network model in accordance with a set of training data to generate a retrained second neural network model; and the retraining module to output the retrained second neural network model as a final neural network model if a target sparsity rate has been reached or to provide the retrained second neural network model to the layer-wise pruning model for additional pruning if the target sparsity rate has not been reached.", "Method and apparatus for reducing parameter density of deep neural network (dnn) "]
[null, "Entity relationship extracting method based on deep neural network "]
["Aspects disclosed in the detailed description include memory compression in a deep neural network (DNN). To support a DNN application, a fully connected weight matrix associated with a hidden layer(s) of the DNN is divided into a plurality of weight blocks to generate a weight block matrix with a first number of rows and a second number of columns. A selected number of weight blocks are randomly designated as active weight blocks in each of the first number of rows and updated exclusively during DNN training. The weight block matrix is compressed to generate a sparsified weight block matrix including exclusively active weight blocks. The second number of columns is compressed to reduce memory footprint and computation power, while the first number of rows is retained to maintain accuracy of the DNN, thus providing the DNN in an efficient hardware implementation without sacrificing accuracy of the DNN application.", "Memory compression in a deep neural network "]
["Methods and systems are provided for detecting and cataloging qualities in music. While both the data volume and heterogeneity of the digital music content is huge, it has become increasingly important and convenient to build a recommendation or search system to facilitate surfacing these content to the user or consumer community. Embodiments use deep convolutional neural network to imitate how human brain processes hierarchical structures in the auditory signals, such as music, speech, etc., at various timescales. This approach can be used to discover the latent factor models of the music based upon acoustic hyper-images that are extracted from the raw audio waves of music. These latent embeddings can be used either as features to feed to subsequent models, such as collaborative filtering, or to build similarity metrics between songs, or to classify music based on the labels for training such as genre, mood, sentiment, etc.", "Modeling of the latent embedding of music using deep neural network "]
["Deep learning is used to identify specific, potential risks to an enterprise (of which litigation is the prime example) while such risks are still internal electronic communications. The system involves mining and using existing classifications of data (e.g., from a litigation database) to train one or more deep learning algorithms, and then examining the internal electronic communications with the trained algorithm, to generate a scored output that will enable enterprise personnel to be alerted to risks and take action in time to prevent the risks from resulting in harm to the enterprise or others.", "Using classified text and deep learning algorithms to identify risk and provide early warning "]
["Training neural networks by constructing a neural network model having neurons each associated with a quantized activation function adapted to output a quantized activation value. The neurons are arranged in layers and connected by connections associated quantized connection weight functions adapted to output quantized connection weight values. During a training process a plurality of weight gradients are calculated during backpropagation sub-processes by computing neuron gradients, each of an output of a respective the quantized activation function in one layer with respect to an input of the respective quantized activation function. Each neuron gradient is calculated such that when an absolute value of the input is smaller than a positive constant threshold value, the respective neuron gradient is set as a positive constant output value and when the absolute value of the input is smaller than the positive constant threshold value the neuron gradient is set to zero.", "Quantized neural network training and inference "]
["Disclosed is a neural network training method and apparatus, and recognition method and apparatus. The neural network training apparatus receives data and train a neural network based on remaining hidden nodes obtained by excluding a reference hidden node from hidden nodes included in the neural network, wherein the reference hidden node maintains a value in a previous time interval until a subsequent time interval.", "Neural network training method and apparatus, and recognition method and apparatus "]
["The present invention is a fully connected feed forward network that includes at least one hidden layer 16. The hidden layer 16 includes nodes 20 in which the output of the node is fed back to that node as an input with a unit delay produced by a delay device 24 occurring in the feedback path 22 (local feedback). Each node within each layer also receives a delayed output (crosstalk) produced by a delay unit 36 from all the other nodes within the same layer 16. The node performs a transfer function operation based on the inputs from the previous layer and the delayed outputs. The network can be implemented as analog or digital or within a general purpose processor. Two teaching methods can be used: (1) back propagation of weight calculation that includes the local feedback and the crosstalk or (2) more preferably a feed forward gradient decent which immediately follows the output computations and which also includes the local feedback and the crosstalk. Subsequent to the gradient propagation, the weights can be normalized, thereby preventing convergence to a local optimum. Education of the network can be incremental both on and off-line. An educated network is suitable for modeling and controlling dynamic nonlinear systems and time series systems and predicting the outputs as well as hidden states and parameters. The educated network can also be further educated during on-line processing.", "Neural node network and model, and method of teaching same "]
[null, "Media information classification method and device based on DNN (deep neural network) "]
["Methods and recordable media for monitoring structural health conditions. The present invention provides a method for interrogating a damage of a host structure using a diagnostic network patch (DNP) system having patches. An interrogation module partitions the plurality of patched in subgroups and measures the sensor signals generated and received by actuator and sensor patches, respectively. Then, a process module loads sensor signal data to identify Lamb wave modes, determine the time of arrival of the modes and generate a tomographic image. It also determines distribution of other structural condition indices to generate tomographic images of the host structure. A set of tomographic images can be stacked to generate a hyperspectral tomography cube. A classification module generates codebook based on K-mean/Learning Vector Quantization algorithm and uses a neural-fuzzy-inference system to determine the type of damages of the host structure.", "Methods for monitoring structural health conditions "]
[null, "Text classification method based on cyclic convolution network "]
["An approximation is determined for the future system behavior by a similarity analysis using a previously known behavior of the dynamic system, whereupon the future system behavior is determined by using the approximation for the future behavior of the dynamic system as well as a neuronal network structure, especially a causal retro-causal network (causality analysis).", "Method, computer program with program code means, and computer program product for determining a future behavior of a dynamic system "]
["Methods for detecting and validating an estimated situation using a situation-dependent predictor of a user response to token instances, including: receiving a temporal window of token instances and a putative situation for a user; predicting an expected response of the user to being exposed to the temporal window of token instances; receiving a value of a measurement channel of the user taken during exposure of the user to the temporal window of token instances; identifying that difference between the received value of the user measurement channel and the predicted expected response of the user is above a predefined threshold; and indicating that the putative situation is wrong.", "Discovering and classifying situations that influence affective response "]
["Apparatus for generating sequences of elements including at least one task unit, each of which has an upper and a lower neural network connected in a hierarchical relationship and is operable to output a sequence of elements. Each of the upper and lower neural networks is a class of temporal neural networks having an infinite number of internal states.", "Sequence generator "]
["A method and system for early detection of incipient faults in an electric motor are disclosed. First, current and voltage values for one or more phases of the electric motor are measured during motor operations. A set of current predictions is then determined via a neural network-based current predictor based on the measured voltage values and an estimate of motor speed values of the electric motor. Next, a set of residuals is generated by combining the set of current predictions with the measured current values. A set of fault indicators is subsequently computed from the set of residuals and the measured current values. Finally, a determination is made as to whether or not there is an incipient electrical, mechanical, and/or electromechanical fault occurring based on the comparison result of the set of fault indicators and a set of predetermined baseline values.", "Method and system for early detection of incipient faults in electric motors "]
[null, "Mechanical fault fast diagnosis method based on deep neural network "]
[null, "Deep machine learning for performing a touch motion prediction "]
["Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for generating acoustic models. In some implementations, a first neural network trained as an acoustic model using the connectionist temporal classification algorithm is obtained. Output distributions from the first neural network are obtained for an utterance. A second neural network is trained as an acoustic model using the output distributions produced by the first neural network as output targets for the second neural network. An automated speech recognizer configured to use the trained second neural network is provided.", "Generating acoustic models "]
["A method for the computer-supported generation of a data-driven model of a technical system, in particular of a gas turbine or wind turbine, based on training data is disclosed. The data-driven model is preferably learned in regions of training data having a low data density. According to the invention, it is thus ensured that the data-driven model is generated for information-relevant regions of the training data. The data-driven model generated is used in a particularly preferred embodiment for calculating a suitable control and/or regulation model or monitoring model for the technical system. By determining optimization criteria, such as low pollutant emissions or low combustion dynamics of a gas turbine, the service life of the technical system in operation can be extended. The data model generated by the method according to the invention can furthermore be determined quickly and using low computing resources, since not all training data is used for learning the data-driven model.", "Method for the computer-supported generation of a data-driven model of a technical system, in particular of a gas turbine or wind turbine "]
["A method of transfer learning includes receiving second data and generating, via a first network, second labels for the second data. In one configuration, the first network has been previously trained on first labels for first data. Additionally, the second labels are generated for training a second network.", "Transfer learning in neural networks "]
["A method of training a neural network model includes determining a specificity of multiple filters after a predetermined number of training iterations. The method also includes training each of the filters based on the specificity.", "Filter specificity as training criterion for neural networks "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for augmenting neural networks with an external memory. One of the systems includes a controller neural network that includes a Least Recently Used Access (LRUA) subsystem configured to: maintain a respective usage weight for each of a plurality of locations in the external memory, and for each of the plurality of time steps: generate a respective reading weight for each location using a read key, read data from the locations in accordance with the reading weights, generate a respective writing weight for each of the locations from a respective reading weight from a preceding time step and the respective usage weight for the location, write a write vector to the locations in accordance with the writing weights, and update the respective usage weight from the respective reading weight and the respective writing weight.", "Augmenting Neural Networks with External Memory "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for selecting answers to questions about documents. One of the methods includes receiving a document comprising a plurality of document tokens; receiving a question associated with the document, the question comprising a plurality of question tokens; processing the document tokens and the question tokens using a reader neural network to generate a joint numeric representation of the document and the question; and selecting, from the plurality of document tokens, an answer to the question using the joint numeric representation of the document and the question.", "Reading comprehension neural networks "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for processing sequences using convolutional neural networks. One of the methods includes, for each of the time steps: providing a current sequence of audio data as input to a convolutional subnetwork, wherein the current sequence comprises the respective audio sample at each time step that precedes the time step in the output sequence, and wherein the convolutional subnetwork is configured to process the current sequence of audio data to generate an alternative representation for the time step; and providing the alternative representation for the time step as input to an output layer, wherein the output layer is configured to: process the alternative representation to generate an output that defines a score distribution over a plurality of possible audio samples for the time step.", "Processing sequences using convolutional neural networks "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for identifying answers to questions using neural networks. One of the methods includes receiving an input text passage and an input question string; processing the input text passage using an encoder neural network to generate a respective encoded representation for each passage token in the input text passage; at each time step: processing a decoder input using a decoder neural network to update the internal state of the decoder neural network; and processing the respective encoded representations and a preceding output of the decoder neural network using a matching vector neural network to generate a matching vector for the time step; and generating an answer score that indicates how well the input text passage answers a question posed by the input question string.", "Answer to question neural networks "]
["Systems and Methods for training a neural network represented as a computational graph are disclosed. An example method begins with obtaining data representing a computational graph. The computational graph is then augmented to generate a training computational graph for training the neural network using a machine learning training algorithm that includes computing a gradient of an objective function with respect to each of the parameters of the neural network. Augmenting the computational graph includes inserting a plurality of gradient nodes and training edges into the computational graph to generate a backward path through the computational graph that represents operations for computing the gradients of the objective function with respect to the parameters of the neural network. The neural network is trained using the machine learning training algorithm by executing the training computational graph.", "Training neural networks represented as computational graphs "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for augmenting neural networks with an external memory. One of the methods includes providing an output derived from a first portion of a neural network output as a system output; determining one or more sets of writing weights for each of a plurality of locations in an external memory; writing data defined by a third portion of the neural network output to the external memory in accordance with the sets of writing weights; determining one or more sets of reading weights for each of the plurality of locations in the external memory from a fourth portion of the neural network output; reading data from the external memory in accordance with the sets of reading weights; and combining the data read from the external memory with a next system input to generate the next neural network input.", "Augmenting neural networks with external memory "]
["In a system comprising a plurality of resources for performing useful work, a resource allocation controller function, which is customized to the particular system's available resources and configuration, dynamically allocates resources and/or alters configuration to accommodate a changing workload. Preferably, the resource allocation controller is part of the computer's operating system which allocates resources of the computer system. The resource allocation controller uses a controller neural network for control, and a separate system model neural network for modelling the system and training the controller neural network. Performance data is collected by the system and used to train the system model neural network. A system administrator specifies computer system performance targets which indicate the desired performance of the system. Deviations in actual performance from desired performance are propagated back through the system model and ultimately to the controller neural network to create a closed loop system for resource allocation.", "Adaptive resource allocation using neural networks "]
["The present invention relates to a control algorithm constructing device that constructs a control algorithm controlling the motion of a robot, and a controller that controls the motion of the robot in accordance with the constructed control algorithm, with the purpose of reducing the cost and time taken to create the control algorithm as compared with the conventional method such as an MZP method to solve a mechanical equation, in which the control algorithm is constructed by a recurrent neural network (RNN) including a neuron generating an output with an analogue lag with respect to an input, the coefficients of the RNN are determined in succession from the term of lower degree to the term of higher degree.", "Robot control algorithm constructing apparatus, robot control algorithm constructing program storage medium, robot controller, robot control program storage medium, and robot "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for augmenting neural networks with an external memory. One of the methods includes providing an output derived from the neural network output for the time step as a system output for the time step; maintaining a current state of the external memory; determining, from the neural network output for the time step, memory state parameters for the time step; updating the current state of the external memory using the memory state parameters for the time step; reading data from the external memory in accordance with the updated state of the external memory; and combining the data read from the external memory with a system input for the next time step to generate the neural network input for the next time step.", "Augmented neural networks "]
["The invention is a system failure monitoring method and apparatus which learns the symptom-fault mapping directly from training data. The invention first estimates the state of the system at discrete intervals in time. A feature vector x of dimension k is estimated from sets of successive windows of sensor data. A pattern recognition component then models the instantaneous estimate of the posterior class probability given the features, p(wi |/x), 1\u2266i\u2266m. Finally, a hidden Markov model is used to take advantage of temporal context and estimate class probabilities conditioned on recent past history. In this hierarchical pattern of information flow, the time series data is transformed and mapped into a categorical representation (the fault classes) and integrated over time to enable robust decision-making.", "Hidden markov models for fault detection in dynamic systems "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating parse trees for input text segments. One of the methods includes obtaining an input text segment comprising a plurality of inputs arranged according to an input order; processing the inputs in the input text segment using an encoder long short term memory (LSTM) neural network to generate a respective encoder hidden state for each input in the input text segment; and processing the respective encoder hidden states for the inputs in the input text segment using an attention-based decoder LSTM neural network to generate a linearized representation of a parse tree for the input text segment.", "Generating parse trees of text segments using neural networks "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for augmenting neural networks with an external memory. One of the systems includes a sparse memory access subsystem that is configured to perform operations comprising generating a sparse set of reading weights that includes a respective reading weight for each of the plurality of locations in the external memory using the read key, reading data from the plurality of locations in the external memory in accordance with the sparse set of reading weights, generating a set of writing weights that includes a respective writing weight for each of the plurality of locations in the external memory, and writing the write vector to the plurality of locations in the external memory in accordance with the writing weights.", "Augmenting neural networks with sparsely-accessed external memory "]
["Methods for system failure prediction include clustering log files according to structural log patterns. Feature representations of the log files are determined based on the log clusters. A likelihood of a system failure is determined based on the feature representations using a neural network. An automatic system control action is performed if the likelihood of system failure exceeds a threshold.", "System failure prediction using long short-term memory neural networks "]
["The present invention relates to an improved artificial neural network for predicting one or more next items in a sequence of items based on an input sequence item. The artificial neural network is implemented on an electronic device comprising a processor, and at least one input interface configured to receive one or more input sequence items, wherein the processor is configured to implement the artificial neural network and generate one or more predicted next items in a sequence of items using the artificial neural network by providing an input sequence item received at the at least one input interface and a side input as inputs to the artificial neural network, wherein the side input is configured to maintain a record of input sequence items received at the input interface.", "Artificial neural network with side input for language modelling and prediction "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for augmenting neural networks with an external memory using reinforcement learning. One of the methods includes providing an output derived from the system output portion of the neural network output as a system output in the sequence of system outputs; selecting a memory access process from a predetermined set of memory access processes for accessing the external memory from the reinforcement learning portion of the neural network output; writing and reading data from locations in the external memory in accordance with the selected memory access process using the differentiable portion of the neural network output; and combining the data read from the external memory with a next system input in the sequence of system inputs to generate a next neural network input in the sequence of neural network inputs.", "Augmenting neural networks with external memory using reinforcement learning "]
["A method for detecting the occurrence of a specific event is provided. The method includes: capturing image data of a scene; detecting and tracking an object of interest in the image data; analyzing features of the object of interest; comparing the analyzed features with predetermined criteria indicative of the specific event; and determining whether the specific event has occurred based on the comparison. The method is particularly useful in the monitoring of the elderly where the specific event is selected from a list comprising a fall-down, stagger, and panic gesturing.", "Video based detection of fall-down and other events "]
["Embodiments of the present disclosure include a method that obtains a digital image. The method includes extracting a word block from the digital image. The method includes processing the word block by evaluating a value of the word block against a dictionary. The method includes outputting a prediction equal to a common word in the dictionary when a confidence factor is greater than a predetermined threshold. The method includes processing the word block and assigning a descriptor to the word block corresponding to a property of the word block. The method includes processing the word block using the descriptor to prioritize evaluation of the word block. The method includes concatenating a first output and a second output. The method includes predicting a value of the word block.", "System and method of character recognition using fully convolutional neural networks with attention "]
["Embodiments are described for minimizing a wait time for a rider after sending a ride request for a vehicle. An example computer-implemented method includes receiving a ride request, the request being for travel from a starting location to a zone in a geographic region during a specified timeslot. The method further includes predicting travel demand based on a number of ride requests in the zone during the specified timeslot. The method further includes requesting transport of one or more vehicles to the zone in response to the predicted number of ride requests when the travel demand is predicted to exceed a number of vehicles in the zone during the specified timeslot.", "Neural network computing systems for predicting vehicle requests "]
["A method for a human-computer dialogue, a neural network system and a user equipment. In the method for a human-computer dialogue, by means of vectorizing a natural language question and a knowledge base, a knowledge base-based intermediate result vector representing the similarity between the natural language question and a knowledge base answer is obtained by means of vector calculation; and then a fact-based correct natural language answer is obtained by means of calculation according to a question vector and the intermediate result vector. The method combines a dialogue and a knowledge base-based question and answer, can perform natural language interaction with a user, and gives a fact-based correct natural language answer according to the knowledge base.", "Method for human-computer dialogue, neural network system and user equipment "]
[null, "Based on an incomplete set of dynamic neural network support recursive Ship Course Intelligent Controller "]
["Robots and robotic systems and methods can employ artificial neural networks (ANNs) to significantly improve performance. The ANNs can operate alternatingly in forward and backward directions in interleaved fashion. The ANNs can employ visible units and hidden units. Various objective functions can be optimized. Robots and robotic systems and methods can execute applications including a plurality of agents in a distributed system, for instance with a number of hosts executing respective agents, at least some of the agents in communications with one another. The hosts can execute agents in response to occurrence of defined events or trigger expressions, and can operate with a maximum latency guarantee and/or data quality guarantee.", "Systems, devices, and methods for distributed artificial neural network computation "]
[null, "Automatic Recognition parallel structure-based natural language Novel Neural Network "]
["Embodiments are provided for analyzing and characterizing video data. According to certain aspects, an analysis machine may analyze video data and optional audio data corresponding thereto using one or more artificial neural networks (ANNs). The analysis machine may process an output of this analysis with a recurrent neural network and an additional ANN. The output of the additional ANN may include a prediction vector comprising a set of values representative of a set of characteristics associated with the video data.", "Neural network architecture for analyzing video data "]
["Systems and method for computing yield values through a neural network from a plurality of different data inputs are disclosed. In an embodiment, a server computer system receives a particular dataset relating to one or more agricultural fields wherein the particular data set comprises particular crop identification data, particular environmental data, and particular management practice data. Using a first neural network, the server computer system computes a crop identification effect on crop yield from the particular crop identification data. Using a second neural network, the server computer system computes an environmental effect on crop yield from the particular environmental data. Using a third neural network, the server computer system computes a management practice effect on crop yield from the management practice data. Using a master neural network, the server computer system computes one or more predicted yield values from the crop identification effect on crop yield, the environmental effect on crop yield, and the management practice effect on crop yield.", "Crop yield estimation using agronomic neural network "]
["A computer-implemented technique is described herein for identifying one or more content items that are relevant to an input linguistic item (e.g., an input query) using a deep-structured neural network, trained based on a corpus of click-through data. The input linguistic item has a collection of input tokens. The deep-structured neural network includes a first part that produces word embeddings associated with the respective input tokens, a second part that generates state vectors that capture context information associated with the input tokens, and a third part which distinguishes important parts of the input linguistic item from less important parts. The second part of the deep-structured neural network can be implemented as a recurrent neural network, such as a bi-directional neural network. The third part of the deep-structured neural network can generate a concept vector by forming a weighted sum of the state vectors.", "Identifying Relevant Content Items using a Deep-Structured Neural Network "]
["A land mark recognition apparatus presenting an enhanced learning capability with its neural net. Color image data of a land mark picked up by a television camera is quantized in the space defined by hue and chroma. Specifically, the color image data defined by hue and chroma falls within an area A1, the data is quantized as red data. Similarly, color image data falls within areas A3 and A3, the data is quantized as green and blue data, respectively. The color image data, after being quantized into a small number of data, is stored in the neural net.", "Land mark recognition method for mobile robot navigation "]
["Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for performing operations using data from a data source. In one aspect, a method includes a neural network system including a controller neural network configured to: receive a controller input for a time step and process the controller input and a representation of a system input to generate: an operation score distribution that assigns a respective operation score to an operation and a data score distribution that assigns a respective data score in the data source. The neural network system can also include an operation subsystem configured to: perform operations to generate operation outputs, wherein at least one of the operations is performed on data in the data source, and combine the operation outputs in accordance with the operation score distribution and the data score distribution to generate a time step output for the time step.", "Neural network programmer "]
["Systems, methods, and apparatus, including computer programs encoded on a computer storage medium, related to associative long short-term memory (LSTM) neural network layers configured to maintain N copies of an internal state for the associative LSTM layer, N being an integer greater than one. In one aspect, a system includes a recurrent neural network including an associative LSTM layer, wherein the associative LSTM layer is configured to, for each time step, receive a layer input, update each of the N copies of the internal state using the layer input for the time step and a layer output generated by the associative LSTM layer for a preceding time step, and generate a layer output for the time step using the N updated copies of the internal state.", "Associative long short-term memory neural network layers "]
["Systems and methods that include and/or leverage a neural network to approximate the steady-state performance of a turbine engine are provided. In one exemplary aspect, the neural network is trained to model a physics-based, steady-state cycle deck. When properly trained, novel input data can be input into the neural network, and as an output of the network, one or more performance indicators indicative of the steady-state performance of the turbine engine can be received. In another aspect, systems and methods for approximating the steady-state performance of a \u201cvirtual\u201d or target turbine engine based at least in part on a reference neural network configured to approximate the steady-state performance of a \u201cfielded\u201d or reference turbine engine are provided.", "Neural Network for Steady-State Performance Approximation "]
["Systems and methods in accordance with embodiments of the invention enable identifying informative features within input data using a neural network data structure. One embodiment includes a data structure describing a neural network that comprises a plurality of neurons; wherein the processor is configured by the feature application to: determine contributions of individual neurons to activation of a target neuron by comparing activations of a set of neurons to their reference values, where the contributions are computed by dynamically backpropagating an importance signal through the data structure describing the neural network; extracting aggregated features detected by the target neuron by: segmenting the determined contributions; clustering into clusters of similar segments; aggregating data to identify aggregated features of input data that contribute to the activation of the target neuron; and displaying aggregated features of input data to highlight important features.", "Systems and Methods for Holistic Extraction of Features from Neural Networks "]
["Trajectories are generated in response to an input label by using a reverse time delay neural network. The reverse time delay neural network comprises an input layer, a plurality of hidden layers, and an output layer, all arranged in succession so that the number of frames per layer increases as the network is traversed from the input layer to the output layer. Additionally, the number of features decreases as the network is traversed from the input layer to the output layer. Features of the trajectory are created from the input label so that a time series of frames can be output by the network. Frames generally relate to particular epochs of time or time units and a frame includes a plurality of features.", "Reverse time delay neural network for pattern generation "]
["The present disclosure provides methods and systems that can ascertain how genetic variants impact molecular phenotypes. Such methods and systems may use additional conservation information. In an aspect, the present disclosure provides a method for training a molecular phenotype neural network (MPNN), comprising: (a) providing a molecular phenotype neural network (MPNN) comprising one or more parameters; (b) providing a training data set comprising (i) a set of one or more inputs comprising biological sequences and (ii) for each input in the set of one or more inputs, a set of one or more molecular phenotypes corresponding to the input; (c) configuring the one or more parameters of the MPNN based on the training data set to minimize a total loss of the training data set, thereby training the MPNN; and (d) outputting the one or more parameters of the MPNN.", "Architectures for training neural networks using biological sequences, conservation, and molecular phenotypes "]
["Cooperative neural networks may be implemented by providing an input to a first neural network including a plurality of first parameters, and updating at least one first parameter based on an output from a recurrent neural network provided with the input, the recurrent neural network including a plurality of second parameters.", "Neural network cooperation "]
["Embodiments of the present disclosure include a method for extracting symbols from a digitized object. The method includes processing the word block against a dictionary. The method includes comparing the word block against a word in the dictionary, the comparison providing a confidence factor. The method includes outputting a prediction equal to the word when the confidence factor is greater than a predetermined threshold. The method includes evaluating properties of the word block when the confidence factor is less than the predetermined threshold. The method includes predicting a value of the word block based on the properties of the word block. The method further includes determining an error rate for the predicted value of the word block. The method includes outputting a value for the word block, the output equal to a calculated value corresponding to a value of the word block having the lowest error rate.", "System and method of character recognition using fully convolutional neural networks "]
["A system for training a neural network model, the neural network model comprising a plurality of layers including a first hidden layer associated with a first set of weights, the system comprising at least one computer hardware processor programmed to perform: obtaining training data; selecting a unitary rotational representation for representing a matrix of the first set weights, the selected unitary rotational representation comprising a plurality of parameters; training the neural network model using the training data using an iterative neural network training algorithm to obtain a trained neural network model, each iteration of the iterative neural network training algorithm comprising: updating values of the plurality of parameters in the selected unitary rotational representation for representing the matrix of the set of weights for the at least one hidden layer, and saving the trained neural network model.", "Systems and methods for training neural networks "]
[null, "Human behavior automatic recognition device using a neural network "]
["A method for implementing spiking neural network computations, the method including defining a dynamic node response function that exhibits spikes, where spikes are temporal nonlinearities for representing state over time; defining a static representation of said node response function; and using the static representation of the node response function to train a neural network. A system for implementing the method is also disclosed.", "Methods and systems for implementing deep spiking neural networks "]
[null, "Neural network learning device "]
["A method for implementing spiking neural network computations, the method including defining a dynamic node response function that exhibits spikes, where spikes are temporal nonlinearities for representing state over time; defining a static representation of said node response function; and using the static representation of the node response function to train a neural network. A system for implementing the method is also disclosed.", "Methods and systems for implementing deep spiking neural networks "]
["A method is described for designing systems that provide efficient implementations of feed-forward, recurrent, and deep networks that process dynamic signals using temporal filters and static or time-varying nonlinearities. A system design methodology is described that provides an engineered architecture. This architecture defines a core set of network components and operations for efficient computation of dynamic signals using temporal filters and static or time-varying nonlinearities. These methods apply to a wide variety of connected nonlinearities that include temporal filters in the connections. Here we apply the methods to synaptic models coupled with spiking and/or non-spiking neurons whose connection parameters are determined using a variety of methods of optimization.", "Methods and systems for implementing dynamic neural networks "]
["Systems, apparatus and methods are described including operations for a flexible neural network accelerator.", "Flexible neural network accelerator and methods therefor "]
["Described herein are systems and methods that receive as input a DNA or RNA sequence, extract features, and apply layers of processing units to compute one ore more condition-specific cell variables, corresponding to cellular quantities measured under different conditions. The system may be applied to a sequence containing a genetic variant, and also to a corresponding reference sequence to determine how much the condition-specific cell variables change because of the variant. The change in the condition-specific cell variables are used to compute a score for how deleterious a variant is, to classify a variant's level of deleteriousness, to prioritize variants for subsequent processing, and to compare a test variant to variants of known deleteriousness. By modifying the variant or the extracted features so as to incorporate the effects of DNA editing, oligonucleotide therapy, DNA- or RNA-binding protein therapy or other therapies, the system may be used to determine if the deleterious effects of the original variant can be reduced.", "Systems and methods for classifying, prioritizing and interpreting genetic variants and therapies using a deep neural network "]
["Systems, methods, and apparatus are provided using signals from a set of tactile sensors mounted on a surface to determine the three-dimensional morphology (e.g., size, shape, orientation, and/or position) and texture of objects of arbitrary shape. Analytical, numerical, and/or neural network approaches can be used to interpret the sensory data.", "Systems, methods, and apparatus for reconstruction of 3-d object morphology, position, orientation and texture using an array of tactile sensors "]
["A context-aware attention-based neural network is provided for answering an input question given a set of purportedly supporting statements for the input question. The neural network includes a processing element. The processing element is configured to calculate a question representation for the input question, based on word annotations and word-level attentions calculated for the input question. The processing element is further configured to calculate a sentence representation for each of the purportedly supporting statements, based on word annotations and word-level attentions calculated for each of the purportedly supporting statements. The processing element is also configured to calculate a context representation for the set of purportedly supporting statements with respect to the sentence representation for each of the purportedly supporting statements. The processing element is additionally configured to generate an answer to the input question based on the question representation and the context representation.", "Context-aware attention-based neural network for interactive question answering "]
[null, "Data merging method based linear constrainted cut minimum binary multiply "]
["The technology disclosed proposes a novel asynchronous evaluation strategy (AES) that increases throughput of evolutionary algorithms by continuously maintaining a queue of K individuals ready to be sent to the worker nodes for evaluation and evolving the next generation once a fraction Mi of the K individuals have been evaluated by the worker nodes, where Mi<<K. A suitable value for Mi is determined experimentally, balancing diversity and efficiency. The technology disclosed is extended to coevolution of deep neural network supermodules and blueprints in the form of AES for cooperative evolution of deep neural networks (CoDeepNEAT-AES). Applied to image captioning domain, a threefold speedup is observed on 200 graphics processing unit (GPU) worker nodes, demonstrating that the disclosed AES and CoDeepNEAT-AES are promising techniques for evolving complex systems with long and variable evaluation times.", "Asynchronous Evaluation Strategy For Evolution Of Deep Neural Networks "]
["A mechanism for compiling a generative description of an inference task into a neural network. First, an arbitrary generative probabilistic model from the exponential family is specified (or received). The model characterizes a conditional probability distribution for measurement data given a set of latent variables. A factor graph is generated for the generative probabilistic model. Each factor node of the factor graph is expanded into a corresponding sequence of arithmetic operations, based on a specified inference task and a kind of message passing algorithm. The factor graph and the sequences of arithmetic operations specify the structure of a neural network for performance of the inference task. A learning algorithm is executed, to determine values of parameters of the neural network. The neural network is then ready for performing inference on operational measurements.", "Automated Compilation of Probabilistic Task Description into Executable Neural Network Specification "]
[null, "Ultrasonic Motor robust recursive neural network sliding mode control system and method "]
[null, "Short text classification method based on convolution neutral network "]
["A machine learning paradigm called Graph Transformer Networks extends the applicability of gradient-based learning algorithms to systems composed of modules that take graphs as inputs and produce graphs as output. Training is performed by computing gradients of a global objective function with respect to all the parameters in the system using a kind of back-propagation procedure. A complete check reading system based on these concept is described. The system uses convolutional neural network character recognizers, combined with global training techniques to provides record accuracy on business and personal checks.", "Module for constructing trainable modular network in which each module inputs and outputs data structured as a graph "]
[null, "Method and apparatus for intelligent information processing in neural network "]
["A method of computation in a deep neural network includes discretizing input signals and computing a temporal difference of the discrete input signals to produce a discretized temporal difference. The method also includes applying weights of a first layer of the deep neural network to the discretized temporal difference to create an output of a weight matrix. The output of the weight matrix is temporally summed with a previous output of the weight matrix. An activation function is applied to the temporally summed output to create a next input signal to a next layer of the deep neural network.", "Temporal difference estimation in an artificial neural network "]
["An input to a complex multi-input process, such as injection molding, is optimized to produce a target output from that process through the use of a neural network trained to that process. A trial input is forward-propagated through the neural network and the output of the network compared to the target output. The difference is back-propagated through the network to determine an input error value in the network. This error value is used to correct the trial input. This correction process is repeated until the trial input produces the target output to within a predetermined degree of accuracy.", "Process optimization using a neural network "]
["The present disclosure provides systems and methods to reduce computational costs associated with convolutional neural networks. In addition, the present disclosure provides a class of efficient models termed \u201cMobileNets\u201d for mobile and embedded vision applications. MobileNets are based on a straight-forward architecture that uses depthwise separable convolutions to build light weight deep neural networks. The present disclosure further provides two global hyper-parameters that efficiently trade-off between latency and accuracy. These hyper-parameters allow the entity building the model to select the appropriately sized model for the particular application based on the constraints of the problem. MobileNets and associated computational cost reduction techniques are effective across a wide range of applications and use cases.", "Efficient Convolutional Neural Networks and Techniques to Reduce Associated Computational Costs "]
["We describe a method of reinforcement learning for a subject system having multiple states and actions to move from one state to the next. Training data is generated by operating on the system with a succession of actions and used to train a second neural network. Target values for training the second neural network are derived from a first neural network which is generated by copying weights of the second neural network at intervals.", "Methods and apparatus for reinforcement learning "]
["A method for selecting bit widths for a fixed point machine learning model includes evaluating a sensitivity of model accuracy to bit widths at each computational stage of the model. The method also includes selecting a bit width for parameters, and/or intermediate calculations in the computational stages of the mode. The bit width for the parameters and the bit width for the intermediate calculations may be different. The selected bit width may be determined based on the sensitivity evaluation.", "Bit width selection for fixed point neural networks "]
["Embodiments of the present disclosure relate to a method, system, apparatus, receiver module, and computer program product which may provide a power-efficient reception. In an example embodiment, at least one spectrum available for a desired communication may be scanned, and available communication channels may be categorized based on at least one determined channel property. A communication channel may be selected from the available communication channels based on the result of the categorization, and complexity of a receiver processing may be set based on the at least one channel property of the selected communication channel. Complexity of the receiver processing may be reduced in light of the property of the selected channel, so that power consumption may be kept low.", "Power-Efficient Intelligent Reception "]
["A method for operating an artificial neuron and an apparatus for performing the method are provided. The artificial neuron may calculate a change amount of an activation based on an input signal received via an input synapse, determine whether an event occurs in response to the calculated change amount of the activation, and transmit, to an output synapse, an output signal that corresponds to the event in response to an occurrence of the event.", "Method for neural network and apparatus performing same method "]
["A method distinguishes components of an acoustic signal by processing the signal to estimate a set of analysis features, wherein each analysis feature defines an element of the signal and has feature values that represent parts of the signal, processing the signal to estimate input features of the signal, and processing the input features using a deep neural network to assign an associative descriptor to each element of the signal, wherein a degree of similarity between the associative descriptors of different elements is related to a degree to which the parts of the signal represented by the elements belong to a single component of the signal. The the similarities between associative descriptors are processed to estimate correspondences between the elements of the signal and the components in the signal. Then, the signal is processed using the correspondences to distinguish component parts of the signal.", "Method for distinguishing components of an acoustic signal "]
["A method for generating synthetic behavior samples with a behavior generator includes drawing, at the behavior generator, a vector from a probability distribution obtained from behavior data of a plurality of users. The method also includes generating, with an artificial neural network decoder of the behavior generator, a synthetic behavior sample based on the vector. The method further includes tuning a model, which identifies a device user, using the generated synthetic behavior sample.", "Deep convolution neural network behavior generator "]
["A recurrent distribution network provides an embedded, cost-based decision mechanism for a real-time, closed-loop process control system that outputs to multiple controllable output devices while observing numerous input constraints. A global incremental distribution request to increase or decrease a controlled process variable is accepted by a recurrent neural network. The network iteratively solves and applies a distribution of a global incremental request to multiple controllable output devices based on individual incremental unit costs, output ranges, output scaling and process input constraint limits.", "Recurrent distribution network with input boundary limiters "]
["A method and apparatus for model-free, real-time, system-wide signal timing for a complex road network is provided. It provides timings in response to instantaneous flow conditions while accounting for the inherent stochastic variations in traffic flow through the use of a simultaneous perturbation stochastic approximation (SPSA) algorithm. This is achieved by setting up several (M) parallel neural networks, each of which produces optimal controls (signal timings) for any time instant (within one of the M time periods) based on observed traffic conditions. The SPSA optimization technique is critical to the feasibility of the approach since it provides the values of weight parameters in each of the neural networks without the need for a model of the traffic flow dynamics.", "Method and apparatus for model-free optimal signal timing for system-wide traffic control "]
["Hyper-parameters are selected for training a deep convolutional network by selecting a number of network architectures as part of a database. Each of the network architectures includes one or more local logistic regression layer and is trained to generate a corresponding validation error that is stored in the database. A threshold error for identifying a good set of network architectures and a bad set of network architectures may be estimated based on validation errors in the database. The method also includes choosing a next potential hyper-parameter, corresponding to a next network architecture, based on a metric that is a function of the good set of network architectures. The method further includes selecting a network architecture, from among next network architectures, with a lowest validation error.", "Hyper-parameter selection for deep convolutional networks "]
["Irregularities in crankshaft velocity introduced when measuring crankshaft rotation at a section of a crankshaft in an internal combustion engine that is less damped to torsional oscillations than is another more accessible crankshaft section are corrected by performing a nonlinear transformation via a neural network to predict rotation measurements that would have been obtained at the inaccessible section from data actually collected at the accessible crankshaft section. Thus, the effects of torsional oscillations in the crankshaft are substantially filtered away, resulting in crankshaft acceleration values that form the basis of a misfire detector having nearly maximum signal-to-noise performance.", "Nonlinear dynamic transform for correction of crankshaft acceleration having torsional oscillations "]
[null, "Structured neural networks for complex system control "]
["In one embodiment, when a computing system is in a first state, a first set of inputs from one or more first sensors is detected. A first sensor value array is generated, and the first value array is fed as input to a first function generated by a first neural network. One or more first output values are calculated based on the first function, and a determination is made based on these first output values if a first action has occurred. If a first action has occurred, a second sensor value array is generated from a second set of inputs from one or more second sensors. The second sensor value array is fed as input to a second function generated by a second neural network. One or more second output values are calculated based on the second function, and the first state is exited based on these second output values.", "Adjusting Mobile Device State Based on User Intentions and/or Identity "]
["An ATM traffic control apparatus and method for adaptively controlling ATM traffics. The ATM traffic control apparatus includes an output buffer for storing traffics generated by a plurality of traffic sources; traffic predictors of neural networks for generating predicted traffic values designating the number of cells which is expected to arrive during a future time slot, by adaptively learning the number of cells received during a setting time slot; a decision gate for deciding whether call congestion will happen by using predicted traffic values, an available buffer size of the output buffer and the number of cells to be transmitted during a setting time slot, and for generating the number of cells which can not be processed during a future time slot when it is decided that the congestion will happen; and a traffic controller of an expert system for calculating an optimal flow rate of each traffic source to control the congestion of the output buffer, by using service rates of the traffic sources, traffic types, peak bit rates, predicted traffic values, and the number of cells which can not be processed in the output buffer.", "Asynchronous transfer mode (ATM) traffic control apparatus and control method for handling a wide variety of ATM services "]
["Discriminative pretraining technique embodiments are presented that pretrain the hidden layers of a Deep Neural Network (DNN). In general, a one-hidden-layer neural network is trained first using labels discriminatively with error back-propagation (BP). Then, after discarding an output layer in the previous one-hidden-layer neural network, another randomly initialized hidden layer is added on top of the previously trained hidden layer along with a new output layer that represents the targets for classification or recognition. The resulting multiple-hidden-layer DNN is then discriminatively trained using the same strategy, and so on until the desired number of hidden layers is reached. This produces a pretrained DNN. The discriminative pretraining technique embodiments have the advantage of bringing the DNN layer weights close to a good local optimum, while still leaving them in a range with a high gradient so that they can be fine-tuned effectively.", "Discriminative pretraining of deep neural networks "]
["The present invention provides methods for diagnosing inflammatory bowel disease (IBD) or for differentiating between Crohn's disease (CD), ulcerative colitis (UC), and indeterminate colitis (IC) in an individual by using a combination of learning statistical classifiers based upon the presence or level of one or more IBD markers in a sample from the individual. The present invention also provides methods for diagnosing the presence or severity of IBD and for stratifying IBD in an individual by determining the level of one or more IBD markers in a sample from the individual and calculating an index value using an algorithm based upon the level of the IBD markers. Methods for monitoring the efficacy of IBD therapy, monitoring the progression or regression of IBD, and optimizing therapy in an individual having IBD are also provided.", "Methods of diagnosing inflammatory bowel disease "]
["A portable computing device or microprocessor/storage system including temperature sensors used to collect temperature readings of a breast tissue of a subject. The device would collect data from the sensors at regular time intervals over a period of time. All of the generated temperature data is stored in the portable computing or storage device. The sensors are placed on the greatest areas of interest on the breast, based on where most cancers develop, by using a sensor placeholder. The sensor placeholder would be lobate shaped, with the sensor placeholder aligning with the glandular regions of the breast where cancers are most likely to develop. The temperature data is then analyzed by one or more classifier systems and classified as either suspect or non-suspect tissue.", "Methods for collecting and analyzing thermal data based on breast surface temperature to determine suspect conditions "]
["A behavior control system of a robot for learning a phoneme sequence includes a sound inputting device inputting a phoneme sequence, a sound signal learning unit operable to convert the phoneme sequence into a sound synthesis parameter and to learn or evaluate a relationship between a sound synthesis parameter of a phoneme sequence that is generated by the robot and a sound synthesis parameter used for sound imitation, and a sound synthesizer operable to generate a phoneme sequence based on the sound synthesis parameter obtained by the sound signal learning unit.", "Robot behavior control system and method, and robot apparatus "]
["A portable computing device or microprocessor/storage system including temperature sensors used to collect temperature readings of a breast tissue of a subject. The device would collect data from the sensors at regular time intervals over a period of time. All of the generated temperature data is stored in the portable computing or storage device. The sensors are placed on the greatest areas of interest on the breast, based on where most cancers develop, by using a sensor placeholder. The sensor placeholder would be lobate shaped, with the sensor placeholder aligning with the glandular regions of the breast where cancers are most likely to develop. The temperature data is then analyzed by one or more classifier systems and classified as either suspect or non-suspect tissue.", "Placeholder for collecting and analyzing thermal data based on breast surface temperature to determine suspect conditions "]
["The use of a pipelined algorithm that performs parallelized computations to train deep neural networks (DNNs) for performing data analysis may reduce training time. The DNNs may be one of context-independent DNNs or context-dependent DNNs. The training may include partitioning training data into sample batches of a specific batch size. The partitioning may be performed based on rates of data transfers between processors that execute the pipelined algorithm, considerations of accuracy and convergence, and the execution speed of each processor. Other techniques for training may include grouping layers of the DNNs for processing on a single processor, distributing a layer of the DNNs to multiple processors for processing, or modifying an execution order of steps in the pipelined algorithm.", "Deep neural networks training for speech and pattern recognition "]
["Motions of a robot are defined by a plurality of frames P0 to P11 at a plurality of different time points. The frames include a plurality of reference frames. In the reference frames, the robot stands alone without falling. The data of the other frames, i.e., frames other than the reference frames, are set only roughly before the robot begins walking. When the robot starts walking, the roughly set data of the other frames is corrected based on control information calculated from the roughly set data and the data of the reference frames.", "Robot controller "]
["A method of quantizing a floating point machine learning network to obtain a fixed point machine learning network using a quantizer may include selecting at least one moment of an input distribution of the floating point machine learning network. The method may also include determining quantizer parameters for quantizing values of the floating point machine learning network based at least in part on the at least one selected moment of the input distribution of the floating point machine learning network to obtain corresponding values of the fixed point machine learning network.", "Fixed point neural network based on floating point neural network quantization "]
["A portable computing device or microprocessor/storage system including temperature sensors used to collect temperature readings of a breast tissue of a subject. The device would collect data from the sensors at regular time intervals over a period of time. All of the generated temperature data is stored in the portable computing or storage device. The sensors are placed on the greatest areas of interest on the breast, based on where most cancers develop, by using a sensor placeholder. The sensor placeholder would be lobate shaped, with the sensor placeholder aligning with the glandular regions of the breast where cancers are most likely to develop. The temperature data is then analyzed by one or more classifier systems and classified as either suspect or non-suspect tissue.", "System for analyzing thermal data based on breast surface temperature to determine suspect conditions "]
["An information processing system having neuron-like signal processors that are interconnected by synapse-like processing junctions that simulates and extends capabilities of biological neural networks. The information processing systems uses integrate-and-fire neurons and Temporally Asymmetric Hebbian learning (spike timing-dependent learning) to adapt the synaptic strengths. The synaptic strengths of each neuron are guaranteed to become optimal during the course of learning either for estimating the parameters of a dynamic system (system identification) or for computing the first principal component. This neural network is well-suited for hardware implementations, since the learning rule for the synaptic strengths only requires computing either spike-time differences or correlations. Such hardware implementation may be used for predicting and recognizing audiovisual information or for improving cortical processing by a prosthetic device.", "Applications of an algorithm that mimics cortical processing "]
["The invention is directed to means, utilizing a neural network, for estimating helicopter airspeed at speeds below about 50 knots using only fixed system parameters as inputs to the neural network. The system includes: means for entering at least one initial parameter; means for measuring, in a nonrotating reference frame associated with the helicopter, a plurality of variable state parameters generated during flight of the helicopter; means for determining a plurality of input parameters based on the at least one initial parameter and the plurality of variable state parameters and for generating successive signals representing the input parameters; at least one equation representing a nonlinear input-output relationship between the input parameters and airspeed; memory means for storing the at least one equation and for successively receiving and storing signals from the determining means; and processing means responsive to signals received from the memory means for generating airspeed information based on the input parameters and the at least one equation.", "Neural network based helicopter low airspeed indicator "]
["Systems and methods for determining ICP based on parameters that can be measured using non-invasive or minimally invasive techniques are provided, wherein a non-linear relationship is used to determine ICP based on one or more variable inputs. The first variable input relates to one or more properties of a cranial blood vessel and/or blood flow, such as acoustic backscatter from an acoustic transducer having a focus trained on a cranial blood vessel, flow velocity in a cranial blood vessel, and the like. Additional variables, such as arterial blood pressure (ABP), may be used in combination with a first variable input relating to one or more properties of a cranial blood vessel, such as flow velocity of the middle cerebral artery (MCA) to derive ICP using a non-linear relationship. Methods and systems for locating target areas based on their acoustic properties and for acoustic scanning of an area, identification of a target area of interest based on acoustic properties, and automated focusing of an acoustic source and/or detector on a desired target area are also provided. Acoustic transducer assemblies are described.", "Systems and methods for determining intracranial pressure non-invasively and acoustic transducer assemblies for use in such systems "]
["Technical solutions are described for improving efficiency of training a resistive processing unit (RPU) array using a neural network training methodology. An example method includes reducing asymmetric RPUs from the RPU array by determining an asymmetric value of an RPU from the RPU array, and burning the RPU in response to the asymmetry value being above a predetermined threshold. The RPU can be burned by causing an electric voltage across the RPU to be above a predetermined limit. The method further includes initiating the training methodology for the RPU array after the asymmetric RPUs from the RPU array are reduced.", "Killing asymmetric resistive processing units for neural network training "]
["A method of developing a controller for general (nonlinear) discrete-time systems, where the equations governing the system are unknown and where a controller is estimated without building or assuming a model for the system. The controller is constructed through the use of a function approximator (FA) such as a neural network or polynomial. This involves the estimation of the unknown parameters within the FA through the use of a stochastic approximation that is based on a simultaneous perturbation gradient approximation, which requires only system measurements (not a system model).", "Method for model-free control of general discrete-time systems "]
["A system to manage utilization of a plurality of types of assets by a plurality of independent entities is provided. The system comprises a controller including a processor operable to execute a plurality of program instructions generally representative of the steps of receiving a plurality of parameters from each of the plurality of independent entities; segmenting the plurality the independent entities into more than one cluster dependent on identified similarities in at least one of the plurality of parameters of the one or more of the plurality of entities; and generating a display of a comparison of the plurality of parameters of each of the independent entities relative to another in each of the plurality of clusters.", "System and method to manage assets of healthcare facility "]
["A method for monitoring a person of interest in a scene, the method comprising: capturing image data of the scene; detecting and tracking the person of interest in the image data; analyzing features of the person of interest; and detecting at least one of an event and behavior associated with the detected person of interest based on the features; and informing a third party of the at least one detected events and behavior.", "Computer vision based elderly care monitoring system "]
["A system and method related to a new approach to speech recognition that reacts to concepts conveyed through speech. In its fullest implementation, the system and method shifts the balance of power in speech recognition from straight sound recognition and statistical models to a more powerful and complete approach determining and addressing conveyed concepts. This is done by using a probabilistically unbiased multi-phoneme recognition process, followed by a phoneme stream analysis process that builds the list of candidate words derived from recognized phonemes, followed by a permutation analysis process that produces sequences of candidate words with high potential of being syntactically valid, and finally, by processing targeted syntactic sequences in a conceptual analysis process to generate the utterance's conceptual representation that can be used to produce an adequate response. The invention can be employed for a myriad of applications, such as improving accuracy or automatically generating punctuation for transcription and dictation, word or concept spotting in audio streams, concept spotting in electronic text, customer support, call routing and other command/response scenarios.", "Multi-phoneme streamer and knowledge representation speech recognition system and method "]
["A vehicle navigator system for monitoring the state characteristics of a vehicle traveling on a vehicle route is disclosed. In particular, the vehicle navigator system may utilize a measuring device such as an inertial measurement unit and a navigator on board the vehicle to calculate state characteristics of the vehicle. The vehicle navigator system may utilize a plurality of physical tags and a Global Positioning System to assist with the collapsing of accumulated error within the vehicle navigator system. The accuracy of the vehicle navigational system is increased with the number of physical tags positioned along the vehicle route. The vehicle navigator system in accordance with the present invention additionally includes a pattern recognition analyzer such as a neural network to calculate a plurality of virtual tags adjacent the vehicle route for increasing the accuracy of the system without increasing the number of physical tags utilized in the system.", "Vehicle navigator system "]
["Certain aspects of the present disclosure support a technique for neural temporal coding, learning and recognition. A method of neural coding of large or long spatial-temporal patterns is also proposed. Further, generalized neural coding and learning with temporal and rate coding is disclosed in the present disclosure.", "Method and apparatus for neural temporal coding, learning and recognition "]
["A learning-type movement control apparatus that learns the movement of an operation control device, predicts the movement thereof, and drives it so as to automatically move. The apparatus comprises an operation control device having a predetermined portion that is displaced according to a force exerted in an arbitrary direction, outputs the amount of the displacement at least as one-dimensional position-representing information, receives a feedback signal carrying information generated by adding displacement information to the position-representing information, and drives the predetermined portion according to a direction and a displacement that are based on the feedback signal. The apparatus also includes a learning section that receives the position-representing information and performs learning of the movement of the operation control device. The apparatus further includes a predicting section that performs prediction of a displacement of the operation control device according to the position-representing information the learning result of the learning section, and generates the feedback signal by adding the displacement information to the position-representing information.", "Learning- type movement control apparatus, method therefor, and distribution medium therefor "]
["A method for facilitating the avoidance of a vehicle collision with an object includes the following steps: a) providing a neural network, b) evolving a good driver, c) evolving a crash predictor, and d) outputting a graded warning signal.", "Crash prediction network with graded warning for vehicle "]
["Methods and systems for identifying data patterns from data comprising at least one data object, said data having at least one existing pattern class associated therewith, said data object being represented by a base feature vector, at least one of said base feature vectors having a structure of higher-level and lower-level feature vectors such that at least one element of a higher-level feature vector is itself a lower-level feature vector.", "Identifying data patterns "]
["Embodiments of the present invention are directed to methods and systems for training a neural network having weighted connections for classification of data, as well as embodiments corresponding to the use of such a neural network for the classification of data, including, for example, prediction of an event (e.g., disease). The method may include inputting input training data into the neural network, processing, by the neural network, the input training data to produce an output, determining an error between the output and a desired output corresponding to the input training data, rating the performance neural network using an objective function, wherein the objective function comprises a function C substantially in accordance with an approximation of the concordance index and adapting the weighted connections of the neural network based upon results of the objective function.", "Methods and systems for predicting occurrence of an event "]
["Adaptive neural networks can be used to effectively enhance signal detection in the inherently noisy environment of an oil well. The neural network can be either non-recurrent or recurrent in nature. The system is implemented with a computer that accepts input from at least one sensor mounted to the wellhead or near the wellhead. The detected contaminated signal is a combination of the event signal and the noise from the environment. The event signal can be, for example, the detonation of a perforation gun. The noise can be either random or periodic. The use of adaptive filtering allows the noise to be more precisely predicted and then subtracted from the contaminated signal to produce a cleaner representation of the event signal. Once the predicted noise is subtracted, the remaining event signal can be analyzed using voice or sound recognition systems to produce an output describing what event occurred.", "Method and apparatus for adaptively filtering noise to detect downhole events "]
["A system and method to calculate a mode of procurement of at least one asset is provided. The system comprises a tracking element operable to generate a signal representative of a location of the at least one asset, and a controller in communication with the at least one tracking element. The controller includes a processor operable to execute program instructions representative of the acts of measuring a utilization of the at least one asset having a unique identifier over at least one time interval, calculating a projected need of the at least one asset over a predetermined future time interval dependent on the utilization of the at least one asset, calculating a mode of procurement of the projected need of the at least one asset dependent on the projected need of the at least one asset, and displaying the mode of procurement to the user.", "System and method to calculate procurement of assets "]
["Means for estimating the charged state of a battery, capable of accurately estimating an SOC even though the battery repeats charging/discharging in short cycles. The charging/discharging current Ib measured by current sensing means (10) is integrated, and pseudo-SOC estimating means estimates a pseudo-SOC. Electromotive force estimating means (16) estimates the open voltage Voc from the pseudo-SOC. Voltage change estimating means (18) estimates the voltage change Vr due to the internal resistance from the charging/discharging current Ib. Dynamic voltage change estimating means (20) estimates the voltage change Vdyn due to the change of the charging/discharging current. An adder (22) adds the Voc, Vr, and Vdyn to calculate an estimated voltage Vest of the battery. SOC correction calculating means (26) calculates the correction of the SOC for equalizing the estimated voltage Vest to the measured voltage Vmes measured by voltage sensing means (12), and an adder (28) adds the correction to the pseudo-SOC to estimate the SOC of the battery.", "Means for estimating charged state of battery and method for estimating degraded state of battery "]
["Image content classification methods, systems and computer programs repeatedly scan an image having an array of image pixels, with at least one random neural network. Each scan corresponds to one of multiple texture patterns. A corresponding texture pattern is compared to each of multiple image portions for each of the multiple scans. A value is assigned to each image portion, corresponding to the texture pattern having the highest coincidence. An array of pixels corresponding to the assigned values for the image portions may then be displayed. Highly accurate results may be obtained, at high speed, without the need for lengthy expert analysis.", "Image content classification methods, systems and computer programs using texture patterns "]
["A non-linear dynamic predictive device (60) is disclosed which operates either in a configuration mode or in one of three runtime modes: prediction mode, horizon mode, or reverse horizon mode. An external device controller (50) sets the mode and determines the data source and the frequency of data. In prediction mode, the input data are such as might be received from a distributed control system (DCS) (10) as found in a manufacturing process; the device controller ensures that a contiguous stream of data from the DCS is provided to the predictive device at a synchronous discrete base sample time. In prediction mode, the device controller operates the predictive device once per base sample time and receives the output from the predictive device through path (14). In horizon mode and reverse horizon mode, the device controller operates the predictive device additionally many times during base sample time interval. In horizon mode, additional data is provided through path (52). In reverse horizon mode data is passed in a reverse direction through the device, utilizing information stored during horizon mode, and returned to the device controller through path (66). In the forward modes, the data are passed to a series of preprocessing units (20) which convert each input variable (18) from engineering units to normalized units. Each preprocessing unit feeds a delay unit (22) that time-aligns the input to take into account dead time effects such as pipeline transport delay. The output of each delay unit is passed to a dynamic filter unit (24). Each dynamic filter unit internally utilizes one or more feedback paths that are essential for representing the dynamic information in the process. The filter units themselves are configured into loosely coupled subfilters which are automatically set up during the configuration mode and allow the capability of practical operator override of the automatic configuration settings. The outputs (28) of the dynamic filter units are passed to a non-linear analyzer (26) which outputs a value in normalized units. The output of the analyzer is passed to a post-processing unit (32) that converts the output to engineering units. This output represents a prediction of the output of the modeled process. In reverse horizon mode, a value of 1 is presented at the output of the predictive device and data is passed through the device in a reverse flow to produce a set of outputs (64) at the input of the predictive device. These are returned to the device controller through path (66). The purpose of the reverse horizon mode is to provide essential information for process control and optimization. The precise operation of the predictive device is configured by a set of parameters. that are determined during the configuration mode and stored in a storage device (30). The configuration mode makes use of one or more files of training data (48) collected from the DCS during standard operation of the process, or through structured plant testing. The predictive device is trained in four phases (40, 42, 44, and 46) corresponding respectively to components 20/32, 22, 24 and 26. The importance of this predictive device is that it is shown to approximate a large class of non-linear dynamic processes. The structure of the predictive device allows it to be incorporated into a practical multivariable non-linear Model Predictive Control scheme, or used to estimate process properties.", "Non-linear dynamic predictive device "]
["A method of reducing computational complexity for a fixed point neural network operating in a system having a limited bit width in a multiplier-accumulator (MAC) includes reducing a number of bit shift operations when computing activations in the fixed point neural network. The method also includes balancing an amount of quantization error and an overflow error when computing activations in the fixed point neural network.", "Reduced computational complexity for fixed point neural network "]
["Methods and systems for backlash compensation. Restrictive assumptions on the backlash nonlinearity (e.g. the same slopes of the lines, etc.) are not required. The compensator scheme has dynamic inversion structure, with a neural network in the feedforward path that approximates the backlash inversion error plus filter dynamics needed for backstepping design. The neural network controller does not require preliminary off-line training. Neural network tuning is based on a modified Hebbian tuning law, which requires less computation than backpropagation. The backstepping controller uses a practical filtered derivative, unlike the usual differentiation required by earlier backstepping routines. Rigorous stability proofs are given using Lyapunov theory. Simulation results show that the proposed compensation scheme is an efficient way of improving the tracking performance of a vast array of nonlinear systems with backlash.", "Backlash compensation using neural network "]
["The present disclosure relates generally to improving acoustic source tracking and selection and, more particularly, to techniques for acoustic source tracking and selection using motion or position information. Embodiments of the present disclosure include systems designed to select and track acoustic sources. In one embodiment, the system may be realized as an integrated circuit including a microphone array, motion sensing circuitry, position sensing circuitry, analog-to-digital converter (ADC) circuitry configured to convert analog audio signals from the microphone array into digital audio signals for further processing, and a digital signal processor (DSP) or other circuitry for processing the digital audio signals based on motion data and other sensor data. Sensor data may be correlated to the analog or digital audio signals to improve source separation or other audio processing.", "Acoustic source tracking and selection "]
["Context-based priors are utilized in machine learning networks (e.g., neural networks) for detecting objects in images. The likely locations of objects are estimated based on context labels. A machine learning network identifies a context label of an entire image. Based on the, the network selects a set of likely regions for detecting objects of interest in the image.", "Context-based priors for object detection in images "]
["The invention provides a micro-control neuroprosthetic device and methods for predicting and controlling epileptic neuronal activity. The device includes a detection system that detects and collects electrophysiological information comprising action potentials from single neurons and ensembles of neurons in a neural structure such as an epileptogenic region of the brain in a subject. An analysis system included in the neuroprosthetic device evaluates the electrophysiological information and performs a real-time extraction of neuron firing features from which the system determines when stimulus intervention is required. The neuroprosthetic device further comprises a stimulation intervention system that provides stimulus output signals having a desired stimulation frequency and stimulation intensity directly to the neural structure in which abnormal neuronal activity is detected. The analysis system further analyzes collected electrophysiological information during or following stimulus intervention to assess the effects of the stimulation intervention and to provide outputs to maintain or modify the stimulation intervention.", "Closed-loop micro-control system for predicting and preventing epilectic seizures "]
["A robot apparatus (1) includes leg blocks (3A to 3D), head block (4), etc. as a moving part (16), a motion controller (102), learning unit (103), prediction unit (104) and a drive unit (105). When the moving part (106), any of the blocks, is operated from outside, the learning unit (103) learns a time-series signal generated due to the external operation. The motion controller (102) and drive unit (105) control together the moving part (106) based on a signal generated at the moving part (106) due to an external force applied to the robot apparatus (1) and a signal having already been learned by the learning unit (103) to make an action taught by the user. The prediction unit (105) predicts whether the moving part (106) makes the taught action according to the initial signal generated at the moving part (106) due to the applied external force. Thus, the robot apparatus (1) can learn an action taught by the user and determine an external force-caused signal to make the taught action.", "Operational control method, program, and recording media for robot device, and robot device "]
["A method and system for labeling a selected word of a sentence using a deep neural network includes, in one exemplary embodiment, determining an index term corresponding to each feature of the word, transforming the index term or terms of the word into a vector, and predicting a label for the word using the vector. The method and system, in another exemplary embodiment, includes determining, for each word in the sentence, an index term corresponding to each feature of the word, transforming the index term or terms of each word in the sentence into a vector, applying a convolution operation to the vector of the selected word and at least one of the vectors of the other words in the sentence, to transform the vectors into a matrix of vectors, each of the vectors in the matrix including a plurality of row values, constructing a single vector from the vectors in the matrix, and predicting a label for the selected word using the single vector.", "Deep Neural Networks and Methods for Using Same "]
["Methods for processing speech data are described herein. In one aspect of the invention, an exemplary method includes receiving a text sentence comprising a plurality of words, each of the plurality of words having a part of speech (POS) tag, generating a POS sequence based on the POS tag of each of the plurality of words, detecting a prosodic phrase break through a recurrent neural network (RNN), based on the POS sequence, and generating a prosodic phrases boundary based on the prosodic phrase break. Other methods and apparatuses are also described.", "Method and apparatus for detecting prosodic phrase break in a text to speech (TTS) system "]
["Deep Neural Network (DNN) training technique embodiments are presented that train a DNN while exploiting the sparseness of non-zero hidden layer interconnection weight values. Generally, a fully connected DNN is initially trained by sweeping through a full training set a number of times. Then, for the most part, only the interconnections whose weight magnitudes exceed a minimum weight threshold are considered in further training. This minimum weight threshold can be established as a value that results in only a prescribed maximum number of interconnections being considered when setting interconnection weight values via an error back-propagation procedure during the training. It is noted that the continued DNN training tends to converge much faster than the initial training.", "Exploiting sparseness in training deep neural networks "]
["A method and system for anatomical object detection using marginal space deep neural networks is disclosed. The pose parameter space for an anatomical object is divided into a series of marginal search spaces with increasing dimensionality. A respective deep neural network is trained for each of the marginal search spaces, resulting in a series of trained deep neural networks. Each of the trained deep neural networks can evaluate hypotheses in a current parameter space using discriminative classification or a regression function. An anatomical object is detected in a medical image by sequentially applying the series of trained deep neural networks to the medical image.", "Method and system for anatomical object detection using marginal space deep neural networks "]
["A semi-automatic, interactive robotic system for performing and/or simulating a multi-step task includes a user interface system, a recognition system adapted to communicate with the user interface system, a control system adapted to communicate with the recognition system, and a sensor-actuator system adapted to communicate with the control system. The recognition system is configured to recognize actions taken by a user while the user operates the user interface system and to selectively instruct the control system to cause the sensor-actuator system to perform, and/or simulate, one of an automatic step, a semi-automatic step or direct step of the multi-step task based on the recognized actions and a task model of the multi-step task.", "Human-machine collaborative robotic systems "]
["Systems and methods for sequence transcription with neural networks are provided. More particularly, a neural network can be implemented to map a plurality of training images received by the neural network into a probabilistic model of sequences comprising P(S|X) by maximizing log P(S|X) on the plurality of training images. X represents an input image and S represents an output sequence of characters for the input image. The trained neural network can process a received image containing characters associated with building numbers. The trained neural network can generate a predicted sequence of characters by processing the received image.", "Sequence transcription with deep neural networks "]
["A method of prognosing a mechanical system to predict when a failure may occur is disclosed. Measurement data corresponding to the mechanical system is used to extract one or more features by decomposing the measurement data into a feature space. A prediction model is then selected from a plurality of prediction models for the one or more features based at least on part on a degradation status of the mechanical system and a reinforcement learning model. A predicted feature space is generated by applying the selective prediction model to the feature space as well as a confidence value by comparing the predicted feature space with a normal baseline distribution, a faulty baseline distribution, or a combination thereof. A status of mechanical system based at least in part on the confidence value is then provided.", "Methods for prognosing mechanical systems "]
["An engine misfire detection system (10) for detecting engine misfire. System (10) includes a conventional controller (12) having a memory unit (14) and a plurality of sensors (16). Controller (12) includes a plurality of neural networks, which are trained by system (10), and which determine whether a firing event is a misfire based upon events occurring before the firing event and events occurring after the firing event. The neural networks are adaptively trained to compensate for the effects of engine variability and aging.", "Method for adaptive detection of engine misfire "]
[null, "Device and method for voice restoration "]
[null, "Nonlinear self-adaption control method of near-space hypersonic vehicle "]
["A method and system for controlling a dynamic nonlinear plant. An input signal controls the plant and an output signal represents a state of the plant in response to the received input signal. A memory stores input and output signals corresponding to m consecutive past states of the plant. A computer neural network predicts a set of future output states representative of the output signal corresponding to the next n consecutive future states of the plant in response to a set of trial control inputs. The trial control inputs represent the input signal corresponding to the next n consecutive future states of the plant. The neural network predicts the future output states based on the past input and output signals and the future trial control inputs. A processor generates the trial control inputs and determines a performance index, indicative of plant performance over time in response to the trial control inputs, as a function of the future output states. The processor generates the input signal for controlling the plant and modifies it as a function of the trial control inputs so that the performance index reaches a desired value.", "Neural network predictive control method and system "]
["A method and a system for quantifying anaesthesia and/or a state of vigilance (e.g. monitoring sedation or sleep) from a plurality of parameters acquired from a subject allows to determine an indicator that reliably quantifies the hypnotic component of anaesthesia and/or the state of vigilance and/or the analgesic component of anaesthesia, even if the number of parameters varies while monitoring the subject. Preferably, a first sub-indicator based on a first subset of parameters adapted to characterise a boundary region between consciousness and unconsciousness is determined, and a second sub-indicator based on a second subset of parameters adapted to characterise a level of hypnosis of said subject is determined, and said first sub-indicator and said second sub-indicator are then combined to compute a global indicator value.", "Method and system for quantifying anaesthesia or a state of vigilance "]
["An interactive system adapted for use in a shopping venue, comprises: an interactive and intelligent source of information, for example supplemental information related to articles available for selection by shoppers in a shopping venue, and not otherwise available to the shoppers during shopping; and, a plurality of interactive, mobile apparatus which shoppers can move throughout the shopping venue and use for transmitting queries to the interactive source of information and use for receiving information transmitted from the interactive source of information, whereby shoppers can receive information useful for evaluating the articles when making article selection decisions, and at least some of the received information can be formulated to influence the article selection decisions. At least some of the information transmitted to the shoppers can be responsive to the queries. An artificial intelligence unit can evaluate the queries and select information for inclusion in responses to the queries.", "Interactive shopping system with mobile apparatus "]
["A cardiac catheter continuously monitors cardiac output within an artery. One temperature sensor measures native blood temperature within the artery, while another temperature sensor measures the temperature of a thermal coil which is in thermal contact with the blood stream. The temperature signals are provided as inputs to a monitoring system which includes isolators, filters, and data processing circuits. A temperature difference signal over time is generated between the native blood temperature and the thermal coil temperature. First and second derivatives are taken of the temperature difference signal and selected features are extracted from the three waveforms. The extracted features are used as to calculate cardiac output. In the present case, a neural network processor is utilized to provide accurate cardiac output measurements based upon the extracted features.", "Continuous cardiac output monitoring system "]
["A robotic system for manipulating a catheter with a plurality of steering wires longitudinally situated within a length of the catheter includes a user interface configured to display a view of an anatomical model and to receive one or more user inputs; a catheter manipulator assembly configured to linearly actuate one or more control members of a catheter; and a robotic controller configured to provide a view of an anatomical model to the user interface; accept one or more user inputs from the user interface; register the one or more user inputs to a coordinate system associated with the anatomical model; compute one or more actuator commands from the one or more registered inputs; and cause the catheter manipulator assembly to linearly actuate one or more control members of a catheter in accordance with the computed actuator commands.", "Robotic catheter system "]
["A method for training a neural network includes receiving labeled training data at a master node, generating, by the master node, partitioned training data from the labeled training data and a held-out set of the labeled training data, determining a plurality of gradients for the partitioned training data, wherein the determination of the gradients is distributed across a plurality of worker nodes, determining a plurality of curvature matrix-vector products over the plurality of samples of the partitioned training data, wherein the determination of the plurality of curvature matrix-vector products is distributed across the plurality of worker nodes, and determining, by the master node, a second-order optimization of the plurality of gradients and the plurality of curvature matrix-vector products, producing a trained neural network configured to perform a structured classification task using a sequence-discriminative criterion.", "Training Deep Neural Network Acoustic Models Using Distributed Hessian-Free Optimization "]
["The present disclosure relates to a control system for user-guided robotic control of a medical device and includes an electronic control unit, a computer-readable memory coupled to the ECU, and a visualization system configured to provide a view of an anatomical model. The memory contains user interface logic configured to be executed by the ECU, and configured to obtain input from a touch screen display with respect to the view of an anatomical model. Control logic stored in the memory is also configured to be executed by said ECU and is configured to produce an actuation control signal responsive to the input to control actuation of a manipulator assembly so as to move the medical device.", "Intuitive user interface control for remote catheter navigation and 3D mapping and visualization systems "]
[null, "Real-time face recognition method based on deep neural network "]
["A method, system, device and computer program product for moving pictures experts group (MPEG) variable bit rate (VBR) video traffic classification using a nearest neighbor classifier, including determining I, P and B frame sizes for an input MPEG VBR video sequence; computing mean values of the I, P and B frame sizes; and classifying the input video sequence into one of a plurality of categories based on the computed mean values using a nearest neighbor classifier.", "Method, system, device and computer program product for MPEG variable bit rate (VBR) video traffic classification using a nearest neighbor classifier "]
["A computer-implemented method of training a neural network includes training a first neural network of a self organizing map type with a first set of first text documents each containing one or more keywords in a semantic context to map each document to a point in the self organizing map y semantic clustering; determining, for each keyword in the first set, all points in the self organizing map to which first documents containing said keyword are mapped, as a pattern and storing said pattern for said keyword in a pattern dictionary; forming at least one sequence of keywords from a second set of second text documents each containing one or more keywords in a semantic context; translating said at least one sequence of keywords into at least one sequence of patterns using the pattern dictionary; and training a second neural network with the at least one sequence of patterns.", "Methods, Apparatus and Products for Semantic Processing of Text "]
["A physical neural network is disclosed, which includes a connection network comprising a plurality of molecular conducting connections suspended within a connection gap formed between one or more input electrodes and one or more output electrodes. One or more molecular connections of the molecular conducting connections can be strengthened or weakened according to an application of an electric field across said connection gap. Thus, a plurality of physical neurons can be formed from said molecular conducting connections of said connection network. Additionally, a gate can be located adjacent said connection gap and which comes into contact with said connection network. The gate can be connected to logic circuitry which can activate or deactivate individual physical neurons among said plurality of physical neurons.", "Nanotechnology neural network methods and systems "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for detecting objects in images. One of the methods includes receiving an input image. A full object mask is generated by providing the input image to a first deep neural network object detector that produces a full object mask for an object of a particular object type depicted in the input image. A partial object mask is generated by providing the input image to a second deep neural network object detector that produces a partial object mask for a portion of the object of the particular object type depicted in the input image. A bounding box is determined for the object in the image using the full object mask and the partial object mask.", "Object detection using deep neural networks "]
[null, "Method and device for data identification based on multitask deep neural network "]
["A processor architecture for a learning machine is presented which uses a massive array of processing elements having local, recurrent connections to form global associations between functions defined on manifolds. Associations between these functions provide the basis for learning cause-and-effect relationships involving vision, audition, tactile sensation and kinetic motion. Two arbitrary images hold each other in place in a manifold association processor and form the basis of short-term memory.", "System and method for modeling the neocortex and uses therefor "]
["A disclosed apparatus comprises an adaptive observer that has an adaptive element to augment a linear observer to enhance its ability to control a nonlinear system. The adaptive element comprises a first, and optionally a second, nonlinearly parameterized neural network unit, the inputs and output layer weights of which can be adapted on line. The adaptive observer generates the neural network units' teaching signal by an additional linear error observer of the nominal system's error dynamics. The adaptive observer has the ability to track an observed system in the presence of unmodeled dynamics and disturbances. The adaptive observer comprises a delay element incorporated in the adaptive element in order to provide delayed values of an actual output signal and a control signal to the neural network units.", "Adaptive observer and related method "]
["The invention is directed to a method, utilizing a neural network, for estimating helicopter airspeed in the low airspeed flight range of below about 50 knots using only fixed system parameters as inputs to the neural network. The method includes the steps of: (a) defining input parameters derivable from variable state parameters generated during flight of the helicopter and measurable in a nonrotating reference frame associated with the helicopter; (b) determining the input parameters and a corresponding helicopter airspeed at a plurality of flight conditions representing a predetermined low airspeed flight domain of the helicopter; (c) establishing a learned relationship between the determined input parameters and the corresponding helicopter airspeed wherein the relationship is represented by at least one nonlinear equation; (d) storing the at least one nonlinear equation in a memory onboard the helicopter; (e) measuring real time values of the variable state parameters during low airspeed flight of the helicopter; (f) calculating real time values of the input parameters; (g) storing the real time values of the input parameters in the memory; (h) processing the real time values of the input parameters in accordance with the at least one nonlinear equation to determine real time airspeed; and (i) displaying the real time airspeed.", "Neural network based method for estimating helicopter low airspeed "]
["A method and system for approximating a deep neural network for anatomical object detection is discloses. A deep neural network is trained to detect an anatomical object in medical images. An approximation of the trained deep neural network is calculated that reduces the computational complexity of the trained deep neural network. The anatomical object is detected in an input medical image of a patient using the approximation of the trained deep neural network.", "Method and System for Approximating Deep Neural Networks for Anatomical Object Detection "]
["The adaptation and personalization of a deep neural network (DNN) model for automatic speech recognition is provided. An utterance which includes speech features for one or more speakers may be received in ASR tasks such as voice search or short message dictation. A decomposition approach may then be applied to an original matrix in the DNN model. In response to applying the decomposition approach, the original matrix may be converted into multiple new matrices which are smaller than the original matrix. A square matrix may then be added to the new matrices. Speaker-specific parameters may then be stored in the square matrix. The DNN model may then be adapted by updating the square matrix. This process may be applied to all of a number of original matrices in the DNN model. The adapted DNN model may include a reduced number of parameters than those received in the original DNN model.", "Low-footprint adaptation and personalization for a deep neural network "]
["A system and method for detecting behavior of a computing platform that includes obtaining platform data; for each data motif identifiers in a set data motif identifiers, performing data motif detection on data in an associated timescale, wherein a first data motif identifier operates on data in a first timescale, wherein a second data motif identifier operates on data in a second timescale, wherein the first timescale and second timescale are different; in a neural network model, synthesizing platform data anomaly detection with at least a set of features inputs from data motif detection of the set of motif identifiers; and signaling if a platform data anomaly is detected through the neural network model.", "System and method for detecting platform anomalies through neural networks "]
["Methods and systems for automatically establishing an enhanced electronic health record (EHR) for a patient include an automatic data collection facility that collects data of a medically related event in proximity to a patient upon occurrence of the event. The collected data may include medication administration data such as medication, time of administration, administration of a dosage of medication, reaction data, and the like. The collected data is communicated to a real-time data integration facility that automatically integrates the data with a patient's electronic health record to establish an enhanced electronic health record.", "Pharmacy management and administration with bedside real-time medical event data collection "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for training a deep neural network. One of the methods includes training a deep neural network with a first training set by adjusting values for each of a plurality of weights included in the neural network, and training the deep neural network to determine a probability that data received by the deep neural network has features similar to key features of one or more keywords or key phrases, the training comprising providing the deep neural network with a second training set and adjusting the values for a first subset of the plurality of weights, wherein the second training set includes data representing the key features of the one or more keywords or key phrases.", "Transfer learning for deep neural network based hotword detection "]
[null, "GBM multimodal magnetic resonance image segmentation method based on deep neural network "]
["Various technologies described herein pertain to conservatively adapting a deep neural network (DNN) in a recognition system for a particular user or context. A DNN is employed to output a probability distribution over models of context-dependent units responsive to receipt of captured user input. The DNN is adapted for a particular user based upon the captured user input, wherein the adaption is undertaken conservatively such that a deviation between outputs of the adapted DNN and the unadapted DNN is constrained.", "Conservatively adapting a deep neural network in a recognition system "]
["Computation elements are connected to one another with a first subsystem having a first input computation element, to which time series values, which each describe one state of a system in a first state space at a time, can be supplied. The first input computation element is connected to a first intermediate computation element, by which a state of the system can be described in a second state space at a time. In a second subsystem a second intermediate computation element, by which a state of the system can be described in the second state space at a time, is connected to a first output computation element, on which a first output signal can be tapped off. In a third subsystem a third intermediate computation element, by which a state of the system can be described in the second state space at a time, is connected to a second output computation element, on which a second output signal can be tapped off. The first subsystem, the second subsystem and the third subsystem are each connected to one another by a coupling between the intermediate computation elements. Weights, which are each associated with one connection between two intermediate computation elements are equal to one another, and weights which are each associated with a connection between an intermediate computation element and an output computation element are equal to one another.", "System and method for training and using interconnected computation elements to determine a dynamic response on which a dynamic process is based "]
["A data processing device for processing time-sequence data includes a data extracting unit extracting time-sequence data for a predetermined time unit from time-sequence data; and a processing unit obtaining scores for nodes of an SOM configured from multiple nodes provided with a spatial array configuration, the scores showing applicability to time-sequence data for a predetermined time unit thereof. The node with the best score is determined to be the winning node which is the node most applicable. The processing unit obtains scores as to the time-sequence data for one predetermined time unit, regarding a distance-restricted node wherein distance from the winning node as to the time-sequence for a predetermined time unit immediately preceding the time-sequence data of one predetermined time unit is within a predetermined distance. The distance-restricted node with the best the score is determined to be the winning node.", "Identifying temporal sequences using a recurrent self organizing map "]
["A method and system for anatomical object detection using marginal space deep neural networks is disclosed. The pose parameter space for an anatomical object is divided into a series of marginal search spaces with increasing dimensionality. A respective sparse deep neural network is trained for each of the marginal search spaces, resulting in a series of trained sparse deep neural networks. Each of the trained sparse deep neural networks is trained by injecting sparsity into a deep neural network by removing filter weights of the deep neural network.", "Method and System for Anatomical Object Detection Using Marginal Space Deep Neural Networks "]
["Methods and systems are disclosed for the detecting of whether a subject has a lung disorder such as asthma, tuberculosis or lung cancer. Monitoring the subject's health and prognosis is also disclosed.", "Breath analysis systems and methods for asthma, tuberculosis and lung cancer diagnostics and disease management "]
["Device and method of channel effect compensation for a telephone speech recognition system is disclosed. The telephone speech recognition system comprises a compensatory neutral network and a recognize. The compensatory neural network receives an input signal and compensates the input signal with a bias to generate an output signal. The compensatory neural network provides a plurality of first parameters to determine the bias. The recognizer is coupled to the compensatory neural network for classifying the output signal according to a plurality of second parameters in acoustic models to generate a recognition result and determine a recognition loss. The first parameters and second parameters are adjusted according to the recognition loss and an adjustment means during a training process.", "Device and method of channel effect compensation for telephone speech recognition "]
["Technologies pertaining to training a deep neural network (DNN) for use in a recognition system are described herein. The DNN is trained using heterogeneous data, the heterogeneous data including narrowband signals and wideband signals. The DNN, subsequent to being trained, receives an input signal that can be either a wideband signal or narrowband signal. The DNN estimates the class posterior probability of the input signal regardless of whether the input signal is the wideband signal or the narrowband signal.", "Exploiting heterogeneous data in deep neural network-based speech recognition systems "]
["A system and method for processing patient polysomnograph data are provided. An abstractor obtains raw patient polysomnograph data and generates a subset of the data to include selected factors, including data clusters. The subset of the patient polysomnograph data is transferred to two or more neural networks that process the data and generate sleep classification data. An integrator obtains the sleep classification data from the two or more neural networks by integrating the sleep classification data from each neural network. A cumulative sleep stage score is generated including confidence values and accuracy estimations for review.", "System and method for processing patient polysomnograph data utilizing multiple neural network processing "]
["Robotic devices may be trained using saliency maps derived from gaze of a trainer. In navigation applications, the saliency map may correspond to portions of the environment being observed by a driving instructor during training using a gaze detector. During an operation, a driver assist robot may utilize the saliency map in order to assess attention of the driver, detect potential hazards, and issue alerts. Responsive to a detection of a mismatch between the driver current attention and the target attention derived from the saliency map, the robot may issue a warning, and/or prompt the driver of an upcoming hazard. A data processing apparatus may employ gaze based saliency maps in order to analyze, e.g., surveillance camera feeds for intruders, open doors, hazards, policy violations (e.g., open doors).", "Apparatus and methods for training robots utilizing gaze-based saliency maps "]
["A method for analyzing data is provided. The method includes generating, using a processing device, a graph from raw data, the graph including a plurality of nodes and edges, deriving, using the processing device, at least one label for each node using a deep belief network, and identifying, using the processing device, a predetermined pattern in the graph based at least in part on the labeled nodes.", "Systems and methods for analyzing data using deep belief networks (DBN) and identifying a pattern in a graph "]
["Described is a system for representing, storing, and reconstructing an input signal. The system constructs an index of unique polychronous groups (PCGs) from a spiking neuron network. Thereafter, a basis set of spike codes is generated from the unique PCGs. An input signal can then be received, with the input signal being spike encoded using the basis set of spike codes from the unique PCGs. The input signal can then be reconstructed by looking up in a reconstruction table, for each unique PCG in the basis set in temporal order according to firing times, anchor neurons. Using a neuron assignment table, an output location can be looked up for each anchor neuron to place a value based on the firing times of each unique PCG. Finally, the output locations of the anchor neurons can be compiled to reconstruct the input signal.", "System for representing, storing, and reconstructing an input signal "]
[null, "Polarization SAR image classification method based on deep neural network "]
["A method and system for actively controlling transport and delivery of content across best-effort networks (FIG. 3A) includes computing expected content bit rate values (315) associated with a distributed application, and expected quality of service (QoS) metrics (325) for the application. Along with the actual measurements that are indicative of the content bit rate and metrics, the expected bit rates (315) and metrics (325) are used to control the bit rate being generated by the application. The expected bit rates (315) and metrics (325) may be based on measured values of previous bit rates, and metrics, or their various transformations, as well as previously forecasted bit rates and metrics (360), or their various transformations. The forecasting of bit rates and metrics may be facilitated by the use of an appropriate predictive algorithm. In this manner, active control of content being distributed over best-effort networks is achieved without substantially modifying the core network infrastructure.", "System for actively controlling distributed applications "]
[null, "Intelligent predicting self adaptive controller "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for image processing using deep neural networks. One of the methods includes receiving data characterizing an input image; processing the data characterizing the input image using a deep neural network to generate an alternative representation of the input image, wherein the deep neural network comprises a plurality of subnetworks, wherein the subnetworks are arranged in a sequence from lowest to highest, and wherein processing the data characterizing the input image using the deep neural network comprises processing the data through each of the subnetworks in the sequence; and processing the alternative representation of the input image through an output layer to generate an output from the input image.", "Processing images using deep neural networks "]
["A system that is capable of controlling multiple entertainment systems and/or speakers using voice commands. The system receives voice commands and may determine audio sources and speakers indicated by the voice commands. The system may generate audio data from the audio sources and may send the audio data to the speakers using multiple interfaces. For example, the system may send the audio data directly to the speakers using a network address, may send the audio data to the speakers via a voice-enabled device or may send the audio data to the speakers via a speaker controller. The system may generate output zones including multiple speakers and may associate input devices with speakers within the output zones. For example, the system may receive a voice command from an input device in an output zone and may reduce output audio generated by speakers in the output zone.", "Processing spoken commands to control distributed audio outputs "]
["A method and system for frame-level merging of HMM state predictions determined by different techniques is disclosed. An audio input signal may be transformed into a first and second sequence of feature vector, the sequences corresponding to each other and to a temporal sequence of frames of the audio input signal on a frame-by-frame basis. The first sequence may be processed by a neural network (NN) to determine NN-based state predictions, and the second sequence may be processed by a Gaussian mixture model (GMM) to determine GMM-based state predictions. The NN-based and GMM-based state predictions may be merged as weighted sums for each of a plurality of HMM state on a frame-by-frame basis to determine merged state predictions. The merged state predictions may then be applied to the HMMs to speech content of the audio input signal.", "Frame-level combination of deep neural network and gaussian mixture models "]
["A deep tensor neural network (DTNN) is described herein, wherein the DTNN is suitable for employment in a computer-implemented recognition/classification system. Hidden layers in the DTNN comprise at least one projection layer, which includes a first subspace of hidden units and a second subspace of hidden units. The first subspace of hidden units receives a first nonlinear projection of input data to a projection layer and generates the first set of output data based at least in part thereon, and the second subspace of hidden units receives a second nonlinear projection of the input data to the projection layer and generates the second set of output data based at least in part thereon. A tensor layer, which can converted into a conventional layer of a DNN, generates the third set of output data based upon the first set of output data and the second set of output data.", "Computer-implemented deep tensor neural network "]
["The present invention is a method of diagnosing a cardiopulmonary condition in an individual by comparing data from a progressive multi-stage test for the individual to a non-linear multi-variate model, preferably a recurrent artificial neural network having sensor fusion. The present invention relies on a cardiovascular model developed from physiological measurements of an individual. Any differences between the modeled parameters and the parameters of an individual at a given time are used for diagnosis.", "Artificial neural network cardiopulmonary modeling and diagnosis "]
["Compressing a machine learning network, such as a neural network, includes replacing one layer in the neural network with compressed layers to produce the compressed network. The compressed network may be fine-tuned by updating weight values in the compressed layer(s).", "Model compression and fine-tuning "]
["A Deep Neural Network (DNN) model used in an Automatic Speech Recognition (ASR) system is restructured. A restructured DNN model may include fewer parameters compared to the original DNN model. The restructured DNN model may include a monophone state output layer in addition to the senone output layer of the original DNN model. Singular value decomposition (SVD) can be applied to one or more weight matrices of the DNN model to reduce the size of the DNN Model. The output layer of the DNN model may be restructured to include monophone states in addition to the senones (tied triphone states) which are included in the original DNN model. When the monophone states are included in the restructured DNN model, the posteriors of monophone states are used to select a small part of senones to be evaluated.", "Restructuring deep neural network acoustic models "]
["Biometric assessment is performed by use of electromyography (EMG) signals detected from muscles at several locations on the hand/or other part of the body subject to fine motor control. The EMG signals are sensed and registered and the sensed signals are processed for use in performing the biometric assessment.", "EMG measured during controlled hand movement for biometric analysis, medical diagnosis and related analysis "]
["A physical neural network is disclosed, which comprises a liquid state machine. The physical neural network is configured from molecular connections located within a dielectric solvent between pre-synaptic and post-synaptic electrodes thereof, such that the molecular connections are strengthened or weakened according to an application of an electric field or a frequency thereof to provide physical neural network connections thereof. A supervised learning mechanism is associated with the liquid state machine, whereby connections strengths of the molecular connections are determined by pre-synaptic and post-synaptic activity respectively associated with the pre-synaptic and post-synaptic electrodes, wherein the liquid state machine comprises a dynamic fading memory mechanism.", "Physical neural network liquid state machine utilizing nanotechnology "]
["Methods and systems for predicting an end of life of a wind turbine component including receiving environmental conditions indicative of natural surroundings of wind turbines within a wind turbine farm, receiving component performance metrics indicative of an operation of wind turbines within a wind turbine farm, and distributing the wind turbines into peer-clusters such that the wind turbines within each of the peer-clusters have similar environmental conditions. The methods and systems further include identifying a low performing wind turbine and a remaining portion of wind turbines within one of the peer-clusters based upon a predicted performance model, processing the component performance metrics of the low performing wind turbine, identifying a critical component of the low performing wind turbine and predicting the end of life of the critical component of the low performing wind turbine.", "Turbine-To-Turbine Prognostics Technique For Wind Farms "]
["A method for training a deep neural network, comprises receiving and formatting speech data for the training, preconditioning a system of equations to be used for analyzing the speech data in connection with the training by using a non-fixed point quasi-Newton preconditioning scheme, and employing flexible Krylov subspace solvers in response to variations in the preconditioning scheme for different iterations of the training.", "Systems and methods for accelerating hessian-free optimization for deep neural networks by implicit preconditioning and sampling "]
["In a predictive action determination apparatus (10), a state observation section (12) observes a state with respect to an environment (11) and obtains state data s(t). An environment prediction section (13) predicts, based on the state data s(t), a future state change in the environment. A target state determination section (15) determines, as a target state, a future state suitable for action determination with reference to a state value storage section (14). A prediction-based action determination section (16) determines an action based on a determined target state.", "Predictive action decision device and action decision method "]
["Methods and systems for processing multilingual DNN acoustic models are described. An example method may include receiving training data that includes a respective training data set for each of two or more or languages. A multilingual deep neural network (DNN) acoustic model may be processed based on the training data. The multilingual DNN acoustic model may include a feedforward neural network having multiple layers of one or more nodes. Each node of a given layer may connect with a respective weight to each node of a subsequent layer, and the multiple layers of one or more nodes may include one or more shared hidden layers of nodes and a language-specific output layer of nodes corresponding to each of the two or more languages. Additionally, weights associated with the multiple layers of one or more nodes of the processed multilingual DNN acoustic model may be stored in a database.", "Multilingual, acoustic deep neural networks "]
["For tracking multiple, simultaneous voices, predicted tracking is used to follow individual voices through time, even when the voices are very similar in fundamental frequency. An acoustic waveform comprised of a group of voices is submitted to a frequency estimator, which may employ an average magnitude difference function (AMDF) calculation to determine the voice fundamental frequencies that are present for each voice. These frequency estimates are then used as input values to a recurrent neural network that tracks each of the frequencies by predicting the current fundamental frequency value for each voice present based on past fundamental frequency values in order to disambiguate any fundamental frequency trajectories that may be converging in frequency.", "Multiple voice tracking system and method "]
["Deep convolutional neural networks receive local and global representations of images as inputs and learn the best representation for a particular feature through multiple convolutional and fully connected layers. A double-column neural network structure receives each of the local and global representations as two heterogeneous parallel inputs to the two columns. After some layers of transformations, the two columns are merged to form the final classifier. Additionally, features may be learned in one of the fully connected layers. The features of the images may be leveraged to boost classification accuracy of other features by learning a regularized double-column neural network.", "Image assessment using deep convolutional neural networks "]
[null, "Vehicle license plate recognition method based on deep neural network "]
["Described herein are various technologies pertaining to a multilingual deep neural network (MDNN). The MDNN includes a plurality of hidden layers, wherein values for weight parameters of the plurality of hidden layers are learned during a training phase based upon training data in terms of acoustic raw features for multiple languages. The MDNN further includes softmax layers that are trained for each target language separately, making use of the hidden layer values trained jointly with multiple source languages. The MDNN is adaptable, such that a new softmax layer may be added on top of the existing hidden layers, where the new softmax layer corresponds to a new target language.", "Multilingual deep neural network "]
[null, "Deep-neural-network-based acoustic model training method, hosts and system "]
["A neural network comprises an input port connected to an output port by one or more paths, each of which comprises an alternating series of weights and neurons. The weights amplify passing signals by a strength factor. The network can be trained by finding a set of strength factor values for the weights such that the network produces the correct output pattern from a given input pattern. During training, a strength factor perturbating and refresh means applies perturbations to the strength factors of weights in the network, and updates the values of the strength factors depending on the difference between signals appearing at the output port, for a given pair of input and training patterns, when the weight is perturbed, and when it is not.", "Neural network with training by perturbation "]
[null, "Traffic sign classification method based on deep neural network "]
["A system may comprise one or more processors and memory storing instructions that, when executed by one or more processors, configure one or more processors to perform a number of operations or tasks, such as receiving a query or a document, and mapping the query or the document into a lower dimensional representation by performing at least one operational layer that shares at least two disparate tasks.", "Representation Learning Using Multi-Task Deep Neural Networks "]
["Provided is a DNN learning method that can reduce DNN learning time using data belonging to a plurality of categories. The method includes the steps of training a language-independent sub-network 120 and language-dependent sub-networks 122 and 124 with training data of Japanese and English. This step includes: a first step of training a DNN obtained by connecting neurons in an output layer of the sub-network 120 with neurons in an input layer of sub-network 122 with Japanese training data; a step of forming a DNN by connecting the sub-network 124 in place of the sub-network 122 to the sub-network 120 and training it with English data; repeating these steps alternately until all training data ends; and after completion, separating the first sub-network 120 from other sub-networks and storing it as a category-independent sub-network in a storage medium.", "Deep neural network learning method and apparatus, and category-independent sub-network learning apparatus "]
[null, "Calculation apparatus and method for accelerator chip accelerating deep neural network algorithm "]
["A neural network system and method for analyzing data sets, especially microarray gene expression data. The neural network is trained to generate time-dependent outcome predictions based on input features and outcome functions for a number of subjects. The features may be highly dimensional relative to the number of subjects, and feature selection is applied to the input feature data for training the neural network. A trained neural network processes input features from a subject to generate an outcome function that reflects the probability of the occurrence of an event at a given time point for that subject.", "Time-dependent outcome prediction using neural networks "]
["The present invention provides methods, systems, and code for accurately classifying whether a sample from an individual is associated with irritable bowel syndrome (IBS). In particular, the present invention is useful for classifying a sample from an individual as an IBS sample using a statistical algorithm and/or empirical data. The present invention is also useful for ruling out one or more diseases or disorders that present with IBS-like symptoms and ruling in IBS using a combination of statistical algorithms and/or empirical data. Thus, the present invention provides an accurate diagnostic prediction of IBS and prognostic information useful for guiding treatment decisions.", "Methods for diagnosing irritable bowel syndrome "]
[null, "Image inspection apparatus for manufactured articles using deep neural network training method and image inspection method of manufactured articles thereby "]
["Methods and systems for online incremental adaptation of neural networks using Gaussian mixture models in speech recognition are described. In an example, a computing device may be configured to receive an audio signal and a subsequent audio signal, both signals having speech content. The computing device may be configured to apply a speaker-specific feature transform to the audio signal to obtain a transformed audio signal. The speaker-specific feature transform may be configured to include speaker-specific speech characteristics of a speaker-profile relating to the speech content. Further, the computing device may be configured to process the transformed audio signal using a neural network trained to estimate a respective speech content of the audio signal. Based on outputs of the neural network, the computing device may be configured to modify the speaker-specific feature transform, and apply the modified speaker-specific feature transform to a subsequent audio signal.", "Online incremental adaptation of deep neural networks using auxiliary Gaussian mixture models in speech recognition "]
["A system and method for emotion detection and more specifically to an image-capture based system and method for detecting invisible and genuine emotions felt by an individual. The system provides a remote and non-invasive approach by which to detect invisible emotion with a high confidence. The system enables monitoring of hemoglobin concentration changes by optical imaging and related detection systems.", "System and method for detecting invisible human emotion "]
["Biometric assessment is performed by use of electromyography (EMG) signals detected from muscles at several locations on the hand/or other part of the body subject to fine motor control. In addition, electroencephalography (EEG), signals detect other biomarkers. The EMG and EEG signals are sensed, synchronized and registered. The signals are converted into digital data and are stored and processed for use in performing the biometric assessment.", "Method and apparatus for biometric analysis using eeg and emg signals "]
["Systems and techniques are provided for a ranking approach to train deep neural nets for multilabel image annotation. Label scores may be received for labels determined by a neural network for training examples. Each label may be a positive label or a negative label for the training example. An error of the neural network may be determined based on a comparison, for each of the training examples, of the label scores for positive labels and negative labels for the training example and a semantic distance between each positive label and each negative label for the training example. Updated weights may be determined for the neural network based on a gradient of the determined error of the neural network. The updated weights may be applied to the neural network to train the neural network.", "Ranking approach to train deep neural nets for multilabel image annotation "]
[null, "Method and system for optimizing deep neural network "]
[null, "Short-term wind speed forecasting method based on deep neural network transfer model "]
["Deep Neural Networks (DNNs) with many hidden layers and many units per layer are very flexible models with a very large number of parameters. As such, DNNs are challenging to optimize. To achieve real-time computation, embodiments disclosed herein enable fast DNN feature transformation via optimized memory bandwidth utilization. To optimize memory bandwidth utilization, a rate of accessing memory may be reduced based on a batch setting. A memory, corresponding to a selected given output neuron of a current layer of the DNN, may be updated with an incremental output value computed for the selected given output neuron as a function of input values of a selected few non-zero input neurons of a previous layer of the DNN in combination with weights between the selected few non-zero input neurons and the selected given output neuron, wherein a number of the selected few corresponds to the batch setting.", "Fast deep neural network feature transformation via optimized memory bandwidth utilization "]
["A method and system for separating noise from speech in real time is provided to improve intelligibility of speech for a variety of communications devices and hearing aids. From a speech signal, a plurality of frame-level features are extracted and form the input to the classifier. The classifier is a deep neural network comprising multiple hidden layers and an output layer with multiple output units. The classifier classifies the speech into a plurality of time-frequency units simultaneously. The classifier output constitutes an estimated ideal binary mask from which a fast gammatone filter bank is used to resynthesize the separated speech into an enhanced speech waveform.", "Real-time method for implementing deep neural network based speech separation "]
[null, "Cross-media sorting method based on deep neural network "]
[null, "Asymmetrical voice conversion method based on deep neural network feature mapping "]
[null, "Speech recognition method using feature compensation based on deep neural network "]
["A multi-layer electrically trainable analog neural network employing multiplexed output neurons having inputs organized into two groups, external and recurrent (i.e., feedback). Each layer of the network comprises a matrix of synapse cells which implement a matrix multiplication between an input vector and a weight matrix. In normal operation, an external input vector coupled to the first synaptic array generates a Sigmoid response at the output of a set of neurons. This output is then fed back to the next and subsequent layers of the network as a recurrent input vector. The output of second layer processing is generated by the same neurons used in first layer processing. Thus, the neural network of the present invention can handle N-layer operation by using recurrent connections and a single set of multiplexed output neurons.", "Multi-layer neural network employing multiplexed output neurons "]
["Novel multiplexed volume holographic optical elements for the development of highly multiplexed photonic interconnection and holographic memory systems with maximum optical throughput efficiency and minimum crosstalk, based on parallel incoherent/coherent double angularly multiplexed volume holographic recording and readout principles, are disclosed. These principles further provide for arbitrarily weighted and independent interconnections, which are of potential importance in the development of densely interconnected photonic implementations of neural networks, photonic interconnection networks for telecommunications switching and digital computing applications, optical information processors, and optical memories. Utilization of the principles that are key features of this holographic element allows for the single step transfer of all or part of the information stored in a three-dimensional holographic storage device to a second such device in a single exposure step. Variants of the multiplexed volume holographic optical element include bulk holographic recording media as well as stratified volume holographic optical elements, comprising in either case both holographic (optical) modulation patterns and computer-generated holograms. Further variants of the multiplexed volume holographic optical element incorporate a subhologram structure within the volume holographic optical element that allows for high efficiency recording in certain real time photorefractive media.", "Incoherent/coherent double angularly multiplexed volume holographic optical elements "]
[null, "Training system of back propagation neural network DNN (Deep Neural Network) "]
["A system and method are disclosed for automatically generating music on the basis of an initial sequence of input notes, and in particular to such a system and method utilizing a recursive artificial neural network (RANN) architecture. The aforementioned system includes a score interpreter (2) interpreting an initial input sequence, a rhythm production RANN (4) for generating a subsequent note duration, a note generation RANN (6) for generating a subsequent note, and feedback means for feeding the pitch and duration of the subsequent note back to the rhythm generation (4) and note generation (6) RANNs, the subsequent note thereby becoming the current note for a following iteration.", "System and method for automatic music generation using a neural network architecture "]
["A learning algorithm for the N-dimensional Coulomb network is disclosed which is applicable to multi-layer networks. The central concept is to define a potential energy of a collection of memory sites. Then each memory site is an attractor of other memory sites. With the proper definition of attractive and repulsive potentials between various memory sites, it is possible to minimize the energy of the collection of memories. By this method, internal representations may be \"built-up\" one layer at a time. Following the method of Bachmann et al. a system is considered in which memories of events have already been recorded in a layer of cells. A method is found for the consolidation of the number of memories required to correctly represent the pattern environment. This method is shown to be applicable to a supervised or unsupervised learning paradigm in which pairs of input and output patterns are presented sequentially to the network. The resulting learning procedure develops internal representations in an incremental or cumulative fashion, from the layer closest to the input, to the output layer.", "N-dimensional coulomb neural network which provides for cumulative learning of internal representations "]
["A method for training a neural network to perform assessments of image quality is provided. The method includes: inputting into the neural network at least one set of images, each set including an image and at least one degraded version of the image; performing comparative ranking of each image in the at least one set of images; and training the neural network with the ranking information. A neural network and image signal processing tuning system are disclosed.", "Label-free non-reference image quality assessment via deep neural network "]
["A method (2000), device (2200) and article of manufacture (2300) provide, in response to orthographic information, efficient generation of a phonetic representation. The method provides for, in response to orthographic information, efficient generation of a phonetic representation, using the steps of: inputting an orthography of a word and a predetermined set of input letter features; utilizing a neural network that has been trained using automatic letter phone alignment and predetermined letter features to provide a neural network hypothesis of a word pronunciation.", "Method, device and article of manufacture for neural-network based orthography-phonetics transformation "]
["A neuron device network is provided with a speech input layer, a context layer, a hidden layer, a speech output layer and a hypothesis layer. A phoneme to be learned is spectral-analyzed by an FFT unit and a vector row at a time point t is input to a speech input layer. Also, a vector state of the hidden layer at a time t-1 is input to the context layer, the vector row at a time t+1 is input to the speech output layer as an instructor signal, and a code row for hypothesizing the phoneme, or the code row, is input to the hypothesis layer. The time series relation of the vector rows and the phoneme are hypothetically learned. Alternatively, a spectrum, a cepstrum or a speech vector row based on outputs from the hidden layer of an auto-associative neural network is input to the speech input layer, and the code row is output from the hypothesis layer, taking into account the time series relation. The speech is recognized when a CPU reads the stored output values of the hidden layer and the connection weights of the hidden layer and the hypothesis layer from a memory of the neuron device network and calculates output values of the respective neuron devices of the hypothesis layer based on the output values and the connection weights. The corresponding phoneme is determined by collating the output values of the respective neuron devices of the hypothesis layer with the code rows in an instructor signal table.", "Neural network, a method of learning of a neural network and phoneme recognition apparatus utilizing a neural network "]
["Speech signal parameters are extracted from time-series data corresponding to different sound units containing the same vowel. The extracted parameters are used to train a statistical model, such as a Hidden Markov-based Model, that has a data structure for separately modeling the nuclear trajectory region of the vowel and its surrounding transition elements. The model is trained as through embedded re-estimation to automatically determine optimally aligned models that identify the nuclear trajectory region. The boundaries of the nuclear trajectory region serve to delimit the overlap region for subsequent sound unit concatenation.", "Identification of unit overlap regions for concatenative speech synthesis system "]
["Disclosed in a pattern recognition apparatus including a neural net including a plurality of mutually coupled excitatory element-inhibitory element paris. The elements in the neural net are coupled through pair coupling coefficients and an excitatory element coupling coefficient. A teacher signal generator outputs a successive teacher signal Q (t) and its infinitesimal variation \u0394Q (t) in accordance with a learning pattern, to apply the output teacher signal and the output small variation to a coupling coefficient controller. The coupling coefficient controller carries out a learning processing in accordance with the teacher signal, so that the coupling coefficients in the neural net are updated. An operation time necessary for a pattern recognition is shortened. In addition, since the learning processing is carried out by only a forward processing on time base, a learning time is also decrease.", "Pattern recognition apparatus and pattern learning apparatus employing neural net including excitatory element-inhibitory element pair couplings "]
["Isolated polypeptides comprising or consisting essentially of specific structural motifs (e.g., three \u03b2-sheets and two \u03b1-helices) are provided, wherein the polypeptides exhibit at least one cell signaling and/or other non-canonical activity of biological relevance. Also provided are polynucleotides encoding such polypeptides, binding agents that bind such polypeptides, analogs, variants and fragments of such polypeptides, etc., as well as compositions and methods of identifying and using any of the foregoing.", "Polypeptide structural motifs associated with cell signaling activity "]
["A neural network is trained using a training neural network having the same topology as the original network but having a differential network output and accepting also differential network inputs. This new training method enables deeper neural networks to be successfully trained by avoiding a problem occuring in conventional training methods in which errors vanish as they are propagated in the reverse direction through deep networks. An acceleration in convergence rate is achieved by adjusting the error used in training to compensate for the linkage between multiple training data points.", "Training a neural network using differential input "]
["Systems and methods for speech recognition incorporating environmental variables are provided. The systems and methods capture speech to be recognized. The speech is then recognized utilizing a variable component deep neural network (DNN). The variable component DNN processes the captured speech by incorporating an environment variable. The environment variable may be any variable that is dependent on environmental conditions or the relation of the user, the client device, and the environment. For example, the environment variable may be based on noise of the environment and represented as a signal-to-noise ratio. The variable component DNN may incorporate the environment variable in different ways. For instance, the environment variable may be incorporated into weighting matrices and biases of the DNN, the outputs of the hidden layers of the DNN, or the activation functions of the nodes of the DNN.", "Variable-component deep neural network for robust speech recognition "]
["In a speech recognition system, deep neural networks (DNNs) are employed in phoneme recognition. While DNNs typically provide better phoneme recognition performance than other techniques, such as Gaussian mixture models (GMM), adapting a DNN to a particular speaker is a real challenge. According to at least one example embodiment, speech data and corresponding speaker data are both applied as input to a DNN. In response, the DNN generates a prediction of a phoneme based on the input speech data and the corresponding speaker data. The speaker data may be generated from the corresponding speech data.", "Method and Apparatus for Speech Recognition Using Neural Networks with Speaker Adaptation "]
["Methods and systems for modifying at least one synapse of a physical neural network. A physical neural network implemented as an adaptive neural network can be provided, which includes one or more neurons and one or more synapses thereof, wherein the neurons and synapses are formed from a plurality of nanoparticles disposed within a dielectric solution in association with one or more pre-synaptic electrodes and one or more post-synaptic electrodes and an applied electric field. At least one pulse can be generated from one or more of the neurons to one or more of the pre-synaptic electrodes of a succeeding neuron and one or more post-synaptic electrodes of one or more of the neurons of the physical neural network, thereby strengthening at least one nanoparticle of a plurality of nanoparticles disposed within the dielectric solution and at least one synapse thereof.", "Adaptive neural network utilizing nanotechnology-based components "]
[null, "In-vehicle sound quality self-adapting active control system and method "]
["A method for training a visual prosthesis includes presenting a non-visual reference stimulus corresponding to a reference image to a visual prosthesis patient. The visual prosthesis including a plurality of electrodes. Training data sets are generated by presenting a series of stimulation patterns to the patient through the visual prosthesis. Each stimulation pattern in the series, after the first, is determined at least in part on a previous subjective patient selection of a preferred stimulation pattern among stimulation patterns previously presented in the series and a fitness function optimization algorithm. The presented stimulation patterns and the selections of the patient are stored and presented to a neural network off-line to determine a vision solution.", "Method and system for training a visual prosthesis "]
["Text may be converted to audible signals, such as speech, by first training a neural network 106 using recorded audio messages 204. To begin the training, the recorded audio messages are converted into a series of audio frames 205 having a fixed duration 213. Then, each audio frame is assigned a phonetic representation 203 and a target acoustic representation 208, where the phonetic representation 203 is a binary word that represents the phone and articulation characteristics of the audio frame, while the target acoustic representation 208 is a vector of audio information such as pitch and energy. After training, the neural network 106 is used in conversion of text into speech. First, text that is to be convened is translated to a series of phonetic frames 401 of the same form as the phonetic representations 208 and having the fixed duration 213. Then the neural network produces acoustic representations in response to context descriptions 207 that include some of the phonetic frames 401. The acoustic representations are then converted into a speech wave form by a synthesizer 107.", "Method and apparatus for converting text into audible signals using a neural network "]
["An autonomous citation indexing system which can be used as an assistant agent automates and enhances the task of finding publications in electronic form, including publications located on the world wide web. The system parses citations from papers and identifies citations to the same paper that may differ in syntax. The system also extracts and provides the context of citations to a given paper, allowing a researcher to determine what is published in other papers about a given paper. Common citations and word or string vector distance similarity are used to find related articles in a search.", "Autonomous citation indexing and literature browsing using citation context "]
["In an automatic speech recognition (ASR) processing system, ASR processing may be configured to process speech based on multiple channels of audio received from a beamformer. The ASR processing system may include a microphone array and the beamformer to output multiple channels of audio such that each channel isolates audio in a particular direction. The multichannel audio signals may include spoken utterances/speech from one or more speakers as well as undesired audio, such as noise from a household appliance. The ASR device may simultaneously perform speech recognition on the multi-channel audio to provide more accurate speech recognition results.", "Speech recognizer with multi-directional decoding "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for training a deep neural network. One of the methods includes generating a plurality of feature vectors that each model a different portion of an audio waveform, generating a first posterior probability vector for a first feature vector using a first neural network, determining whether one of the scores in the first posterior probability vector satisfies a first threshold value, generating a second posterior probability vector for each subsequent feature vector using a second neural network, wherein the second neural network is trained to identify the same key words and key phrases and includes more inner layer nodes than the first neural network, and determining whether one of the scores in the second posterior probability vector satisfies a second threshold value.", "Training multiple neural networks with different accuracy "]
["Technology is disclosed for inferring human attributes from images of people. The attributes can include, for example, gender, age, hair, and/or clothing. The technology uses part-based models, e.g., Poselets, to locate multiple normalized part patches from an image. The normalized part patches are provided into trained convolutional neural networks to generate feature data. Each convolution neural network applies multiple stages of convolution operations to one part patch to generate a set of fully connected feature data. The feature data for all part patches are concatenated and then provided into multiple trained classifiers (e.g., linear support vector machines) to predict attributes of the image.", "Pose-aligned networks for deep attribute modeling "]
["In a device and method for predicting a future muzzle velocity of an indirect fire weapon 3, 7 means 9, 11 responsive to a measurement of muzzle velocity are adapted to implement an adaptive empirical prediction method to predict the future muzzle velocity. The invention also relates to an aiming system and method for an indirect-fire weapon 3, 7. The system comprises a muzzle velocity measuring device 5, and predictor means 9, 11 responsive to an output of the muzzle velocity measuring device 5 for determining a new elevation setting from the weapon. Preferably, the predictor means utilizes an adaptive empirical prediction method such as a Kalman Filter or neural network.", "Weapons systems future muzzle velocity neural network "]
["A tensor deep stacked neural (T-DSN) network for obtaining predictions for discriminative modeling problems. The T-DSN network and method use bilinear modeling with a tensor representation to map a hidden layer to the predication layer. The T-DSN network is constructed by stacking blocks of a single hidden layer tensor neural network (SHLTNN) on top of each other. The single hidden layer for each block then is separated or divided into a plurality of two or more sections. In some embodiments, the hidden layer is separated into a first hidden layer section and a second hidden layer section. These multiple sections of the hidden layer are combined using a product operator to obtain an implicit hidden layer having a single section. In some embodiments the product operator is a Khatri-Rao product. A prediction is made using the implicit hidden layer and weights, and the output prediction layer is consequently obtained.", "Tensor deep stacked neural network "]
["The present invention provides methods for analyzing a combination of biomarkers to individualize tyrosine kinase inhibitor therapy in patients who have been diagnosed with cancer. In particular, the assay methods of the present invention are useful for predicting, identifying, or monitoring the response of a tumor, tumor cell, or patient to treatment with a tyrosine kinase inhibitor using an algorithm based upon biomarker profiling. The assay methods of the present invention are also useful for predicting whether a patient has a risk of developing toxicity or resistance to treatment with a tyrosine kinase inhibitor. In addition, the assay methods of the present invention are useful for monitoring tyrosine kinase inhibitor therapy in a patient receiving the drug to evaluate whether the patient will develop resistance to the drug. Furthermore, the assay methods of the present invention are useful for optimizing the dose of a tyrosine kinase inhibitor in a patient receiving the drug to achieve therapeutic efficacy and/or reduce toxic side-effects.", "Methods of predicting and monitoring tyrosine kinase inhibitor therapy "]
[null, "Power transmission line disaster monitoring and risk assessment platform based on satellite and weather information "]
["A robot is proposed which has a speech recognition unit to detect information supplied simultaneously with or just before or after detection of a touch by a touch sensor, an associative memory/recall memory to store action made correspondingly to the touch and input information (speech signal) detected by the speech recognition unit in association with each other, and an action generator to control the robot to make action recalled by the associative memory/recall memory based on a newly acquired input information (speech signal). The robot has also a sensor data processor to allow the robot to act correspondingly to the touch detection by the touch sensor. Thus, the robot can learn action in association with an input signal such as speech signal.", "Robot device, robot device action control method, external force detecting device and external force detecting method "]
["Provided herein is a system for creating, modifying, deploying and running intelligent systems by combining and customizing the function and operation of reusable component modules arranged into neural processing graphs which direct the flow of signals among the modules, analogous in part to biological brain structure and operation as compositions of variations on functional components and subassemblies.", "Methods and systems for neural and cognitive processing "]
["Methods and systems for modifying at least one synapse of a physicallelectromechanical neural network. A physical/electromechanical neural network implemented as an adaptive neural network can be provided, which includes one or more neurons and one or more synapses thereof, wherein the neurons and synapses are formed from a plurality of nanoparticles disposed within a dielectric solution in association with one or more pre-synaptic electrodes and one or more post-synaptic electrodes and an applied electric field. At least one pulse can be generated from one or more of the neurons to one or more of the pre-synaptic electrodes of a succeeding neuron and one or more post-synaptic electrodes of one or more of the neurons of the physical/electromechanical neural network, thereby strengthening at least one nanoparticle of a plurality of nanoparticles disposed within the dielectric solution and at least one synapse thereof.", "Adaptive neural network utilizing nanotechnology-based components "]
["A system and method for processing an acoustic input signal and providing at least one output acoustic signal to a user of a hearing-aid system. The hearing-aid system includes first and second channels with one of the channels having an adaptive delay. The first channel includes a directional unit for receiving the acoustic input signal and providing a directional signal; a correlative unit for receiving the directional signal and providing a noise reduced signal by utilizing correlative measures for identifying a speech signal of interest in the directional signal; and, a compensator for receiving the noise reduced signal and providing a compensated signal for compensating for a hearing loss of the user.", "Binaural adaptive hearing aid "]
["Methods and systems are disclosed herein in which a physical neural network can be configured utilizing nanotechnology. Such a physical neural network can comprise a plurality of molecular conductors (e.g., nanoconductors) which form neural connections between pre-synaptic and post-synaptic components of the physical neural network. Additionally, a learning mechanism can be applied for implementing Hebbian learning via the physical neural network. Such a learning mechanism can utilize a voltage gradient or voltage gradient dependencies to implement Hebbian and/or anti-Hebbian plasticity within the physical neural network. The learning mechanism can also utilize pre-synaptic and post-synaptic frequencies to provide Hebbian and/or anti-Hebbian learning within the physical neural network.", "Application of hebbian and anti-hebbian learning to nanotechnology-based physical neural networks "]
["A multifunctional neural network system for prediction which includes memory components to store previous values of data within a network. The memory components provide the system with the ability to learn relationships/patterns existent in the data over time.", "Multifunctional Neural Network System and Uses Thereof "]
["A neural network structure includes input units for receiving input data, and a plurality of neural networks connected in parallel and connected to the input units. The plurality of neural networks learn in turn correspondence between the input data and teacher data so that the difference between the input data and the teacher becomes small. The neural network structure further includes output units connected to the plurality of neural networks, for outputting a result of learning on the basis of the results of learning in the plurality of neural networks.", "Plural neural network system having a successive approximation learning method "]
["An analog-digital crosspoint-network includes a plurality of rows and columns, a plurality of synaptic nodes, each synaptic node of the plurality of synaptic nodes disposed at an intersection of a row and column of the plurality of rows and columns, wherein each synaptic node of the plurality of synaptic nodes includes a weight associated therewith, a column controller associated with each column of the plurality of columns, wherein each column controller is disposed to enable a weight change at a synaptic node in communication with said column controller, and a row controller associated with each row of the plurality of rows, wherein each row controller is disposed to control a weight change at a synaptic node in communication with said row controller.", "Hardware analog-digital neural networks "]
["A neural network, which can be implemented either in hardware or software, is constructed of neurons or neuron circuits each having only one significant processing element in the form of a multiplier. The number of training examples is compared to the number of neurons in the neural network to effectuate training. The neural network utilizes a training algorithm which does not require repetitive training and which yields a global minimum to each given set of input vectors.", "Neural network and method of using same "]
["A system and method of image processing using neural networks to control image processing elements. Neural network parameters are defined by genotypes consisting of network vectors. Genotypes may be selectively mutated and cross-bred to provide a mechanism for modifying the behavior of the neural networks, or phenotypes. Genetic modeling processes are used to perform such mutation and cross-over. User feedback concerning output images, is used to select particular genotypes for further mutation and exploration. Preconditioning is employed to extract structural information from source images prior to network processing. Genetic morphing and subnet fusion are also available, to provide additional variations on image processing operations.", "Image processing using genetic mutation of neural network parameters "]
["Neural network algorithms have impressively demonstrated the capability of modelling spatial information. On the other hand, the application of parallel distributed models to processing of temporal data has been severely restricted. The invention introduces a novel technique which adds the dimension of time to the well known back-propagatio", "Neural network for processing both spatial and temporal data with time based back-propagation "]
["A semantic attractor memory uses an evolving neural network architecture and learning rules derived from the study of human language acquisition and change to store, process and retrieve information. The architecture is based on multiple layer channels, with random connections from one layer to the next. One or more layers are devoted to processing input information. At least one processing layer is provided. One or more layers are devoted to processing outputs and feedback is provided from the outputs back to the processing layer or layers. Inputs from parallel channels are also provided to the one or more processing layers. With the exception of the feedback loop and central processing layers, the network is feedforward unless it is employed in a hybrid back-propagation configuration. The learning rules are based on non-stationary statistical processes, such as the Polya process or the processes leading to Bose-Einstein statistics, again derived from considerations of human language acquisition. The invention provides rapid, unsupervised processing of complex data sets, such as imagery or continuous human speech, and a means to capture successful processing or pattern classification constellations for implementation in other networks.", "Method and apparatus for neural networking using semantic attractor architecture "]
["An analyzing system analyzes object signals, particularly voice signals, by estimating a generation likelihood of an observation vector sequence being a time series of feature vectors with use of a Markov model having a plurality of states and given transition probabilities from state to state. A state designation section determines a state i at a time t stochastically using the Markov model. Plural predictors, each of which is composed of a neural network and is provided per each state of the Markov model, are provided for generating a predictional vector of the feature vector xt in the state i at the time t based on values of the feature vectors other than the feature vector xt. A first calculation section calculates an error vector of the predictional vector to the feature vector xT. A second calculation section calculates a generation likelihood of the error vector using a predetermined probability distribution of the error vector according to which the error vector is generated.", "Voice analyzing system using hidden Markov model and having plural neural network predictors "]
["A method and apparatus for using a neural network to process information includes multiple nodes arrayed in multiple layers for transforming input arrays from prior layers or the environment into output arrays for subsequent layers or output devices. Learning rules based on reinforcement are applied. Interconnections between nodes are provided in a manner whereby the number and structure of the interconnections are self-adjusted by the learning rules during learning. At least one of the layers is used as a processing layer, and multiple lateral inputs to each node of each processing layer are used to retrieve information. The invention provides rapid, unsupervised processing of complex data sets, such as imagery or continuous human speech, and captures successful processing or pattern classification constellations for implementation in other networks. The invention includes application-specific self-adjusting multi-layer architectures that employ reinforcement learning rules to create updated data arrays for computation.", "Self-adjusting multi-layer neural network architectures and methods therefor "]
["The present invention relates to adaptive information processing systems, and in particular to associative memories utilizing confidence-mediated associations, and especially neural network systems comprising an auto-organizational apparatus and processes for dynamically mapping an input onto a semantically congruous and contemporaneously-valid, learned response. In particular the present invention relates to such an associative memory system in which provision is made for improving the congruence between an associative memory, by impressing a desired response on an associative memory mapping based on complex polar values.", "Neural networks "]
["A neural network for processing sensory information. The network comprise one or more layers including interconnecting cells having individual states. Each cell is connected to one or more neighboring cells. Sensory signals and signals from interconnected neighboring cells control a current or a conductance within a cell to influence the cell's state. In some embodiments, the current or conductance of a cell can be controlled by a signal arising externally of the layer. Each cell can comprise an electrical circuit which receives an input signal and causes a current corresponding to the signal to pass through a variable conductance. The conductance is a function of the states of the one or more interconnecting neighboring cells. Proper interconnection of the cells on a layer can produce a neural network which is sensitive to predetermined patterns or the passage of such patterns across a sensor array whose signals are input into the network. The layers in the network can be made sensitive to distinct sensory parameters, so that networks which are sensitive to different wavelengths or polarizations of light energy can be produced.", "Optoelectronic sensory neural network "]
["A computing arrangement for identification of a current temporal input against one or more learned signals. The arrangement comprising a number of computational cores, each core comprises properties having at least some statistical independency from other of the computational, the properties being set independently of each other core, each core being able to independently produce an output indicating recognition of a previously learned signal, and at least one decision unit for receiving the produced outputs from the number of computational cores and making an identification of the current temporal input based the produced outputs.", "A computing device, a system and a method for parallel processing of data streams "]
["A waveform equalizer for equalizing a distorted signal, contains a sampling unit, a time series generating unit, and an equalization neural network unit. The sampling unit samples the level of a distorted signal at a predetermined rate. The time series generating unit serially receives the sampled level and outputs in parallel a predetermined number of the levels which have been last received. The equalization neural network unit receives the outputs of the time series generating unit, and generates an equalized signal of the distorted signal based on the outputs of the time series generating unit using a set of equalization network weights which are preset therein. The waveform equalizer may further contain a distortion characteristic detecting unit, an equalization network weight holding unit, and a selector unit. The distortion characteristic detecting unit detects a distortion characteristic of the distorted signal. The equalization network weight holding unit holds a plurality of sets of equalization network weights each for being set in the equalization neural network unit. The selector unit selects one of the plurality of sets of equalization network weights according to the distortion characteristic which is detected in the distortion characteristic detecting unit, and supplies the selected set in the equalization neural network unit to set the selected set therein.", "Waveform equalizer using a neural network "]
["A method of training an artificial neural network (ANN) involves receiving a likelihood distribution map as a teacher image, receiving a training image, moving a local window across sub-regions of the training image to obtain respective sub-region pixel sets, inputting the sub-region pixel sets to the ANN so that it provides output pixel values that are compared to output pixel values of corresponding teacher image pixel values to determine an error, and training the ANN to reduce the error. A method of detecting a target structure in an image involves scanning a local window across sub-regions of the image by moving the local window for each sub-region so as to obtain respective sub-region pixel sets, inputting the sub-region pixel sets to an ANN so that it provides respective output pixel values that represent likelihoods that respective image pixels are part of a target structure, the output pixel values collectively constituting a likelihood distribution map. Another method for detecting a target structure involves training N parallel ANNs on either (A) a same target structure and N mutually different non-target structures, or (B) a same non-target structure and N mutually different target structures, the ANNs outputting N respective indications of whether the image includes a target structure or a non-target structure, and combining the N indications to form a combined indication of whether the image includes a target structure or a non-target structure. The invention provides related apparatus and computer program products storing executable instructions to perform the methods.", "Massive training artificial neural network (MTANN) for detecting abnormalities in medical images "]
["A method (2000), device (2200) and article of manufacture (2300) provide, in response to lexical pronunciation information, efficient generation of postlexical pronunciation information. A method is presented for providing, in response to a lexical pronunciation, efficient generation of a postlexical pronunciation, including the steps of: determining lexical phones, lexical features, and boundary information for a predetermined portion of text; and utilizing a pretrained neural network that was pretrained using lexical phones, postlexical phones, lexical features, and boundary information to generate a neural network hypothesis for a postlexical pronunciation of the predetermined portion of text.", "Method device and article of manufacture for neural-network based generation of postlexical pronunciations from lexical pronunciations "]
["A sensor arrangement (1) comprising at least one measuring coil (2), at least one voltage source (3) for the measuring coil (2), and an evaluation unit (4) with means for detecting, processing, and evaluating measured signals. This sensor arrangement (1) is used to measure distances and thicknesses substantially independently of the material involved, without the user having to know the physicomathematical relations between the influencing quantities and the measured values. In order to evaluate the measured signals, the evaluation unit (4) of the sensor arrangement comprises a neural network (5) with an input layer, at least one hidden layer, an output layer, and connection weights for the individual layers. The connection weights are determined and stored in a learning phase by measurements taken on a plurality of different suitable learning objects with known actual values.", "Sensor arrangement including a neural network and detection method using same "]
["A computer implemented method, data processing system, and computer program product for monitoring system events and providing real-time response to security threats. System data is collected by monitors in the computing system. The expert system of the present invention compares the data against information in a knowledge base to identify a security threat to a system resource in a form of a system event and an action for mitigating effects of the system event. A determination is made as to whether a threat risk value of the system event is greater than an action risk value of the action for mitigating the system event. If the threat risk value is greater, a determination is made as to whether a trust value set by a user is greater than the action risk value. If the trust value is greater, the expert system executes the action against the security threat.", "Method for controlling risk in a computer security artificial neural network expert system "]
["A compact neural network architecture is trainable to sense and classify an optical image directly projected onto it. The system is based upon the combination of a two-dimensional amorphous silicon photoconductor array and a liquid-crystal spatial light modulator. Appropriate filtering of the incident optical image upon capture is incorporated into the net work training rules, through a modification of the standard backpropagation training algorithm. Training of the network on two image classification problems is described: the recognition of handprinted digits, and facial recognition. The network, once trained is capable of standalone operation, sensing an incident image and outputting a final classification signal in real time.", "Neural network that incorporates direct optical imaging "]
["Embodiments of the invention relate to canonical spiking neurons for spatiotemporal associative memory. An aspect of the invention provides a spatiotemporal associative memory including a plurality of electronic neurons having a layered neural net relationship with directional synaptic connectivity. The plurality of electronic neurons configured to detect the presence of a spatiotemporal pattern in a real-time data stream, and extract the spatiotemporal pattern. The plurality of electronic neurons are further configured to, based on learning rules, store the spatiotemporal pattern in the plurality of electronic neurons, and upon being presented with a version of the spatiotemporal pattern, retrieve the stored spatiotemporal pattern.", "Canonical spiking neuron network for spatiotemporal associative memory "]
["A predictive global model for modeling a system includes a plurality of local models, each having: an input layer for mapping into an input space, a hidden layer and an output layer. The hidden layer stores a representation of the system that is trained on a set of historical data, wherein each of the local models is trained on only a select and different portion of the set of historical data. The output layer is operable for mapping the hidden layer to an associated local output layer of outputs, wherein the hidden layer is operable to map the input layer through the stored representation to the local output layer. A global output layer is provided for mapping the outputs of all of the local output layers to at least one global output, the global output layer generalizing the outputs of the local models across the stored representations therein.", "Neural network model with clustering ensemble approach "]
["A low-order model (LOM) of biological neural networks and its mathematical equivalents including the clusterer interpreter probabilistic associative memory (CIPAM) are disclosed. They are artificial neural networks (ANNs) organized as networks of processing units (PUs), each PU comprising artificial neuronal encoders, synapses, spiking/nonspiking neurons, and a scheme for maximal generalization. If the weights in the artificial synapses in a PU have been learned (and then fixed) or can be adjusted by the unsupervised accumulation rule and the unsupervised covariance rule (or supervised covariance rule), the PU is called unsupervised (or supervised) PU. The disclosed ANNs, with these Hebbian-type learning rules, can learn large numbers of large input vectors with temporally/spatially hierarchical causes with ease and recognize such causes with maximal generalization despite corruption, distortion and occlusion. An ANN with a network of unsupervised PUs (called clusterer) and offshoot supervised PUs (called interpreter) is an architecture for many applications.", "Artificial Neural Networks based on a Low-Order Model of Biological Neural Networks "]
