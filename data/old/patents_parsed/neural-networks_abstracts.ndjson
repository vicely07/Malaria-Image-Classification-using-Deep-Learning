["A system for performing optical pattern recognition includes a first detector neural network for detecting the presence of a particular optical pattern in an input image and a second locator neural network for locating and/or removing the particular optical pattern from the input image. The detector network and the locator network both comprise nodes which can take on the -1, +1, or undefined states. The nodes are arranged in layers and each node in a layer has a location corresponding to a pixel in the input image. A particular application of this neural network is in finding the amount field on a check and removing the line which borders the amount field.", "Optical pattern recognition using detector and locator neural networks "]
["A system for monitoring the structural integrity of a mechanical structure. The system utilizes a trainable adaptive interpreter such as a neural network to analyze data from the structure to characterize the structure's health. An actuator is attached to the mechanical structure for generating vibrations in response to an input signal. A sensor, also attached to the mechanical structure, senses the vibrations and generates an output signal in response thereto. The sensor output signal is then coupled to a pre-trained adaptive interpreter for generating an output which characterizes the structural integrity of the mechanical structure. The system can provide continual health monitoring of a structural system to detect structural damage and pinpoint probable location of the damage. The system can operate while the structural system is in service there by significantly reducing structural inspection costs.", "Structural health monitoring using active members and neural networks "]
["A method and apparatus for predicting a signal value for a target element within a multi-element system is disclosed. The method includes modeling the multi-element system by defining fundamental physical relationships between the target element and other elements within the system. The resultant system model is in the form of a set of coupled non-linear differential equations. These differential equations are then approximated into linearized models about an operating point or series of operating points corresponding to the system behavior. The linearized differential equations are then subjected to a coupling analysis. The coupling analysis is employed to determine dynamic coupling between instruments. The coupling analysis assesses the degree of observability of the system and associated elements. The coupling analysis may be based upon observability tests, gramian analyses, or modal analyses. Based upon the coupling analysis, coupled elements are selected. The coupled elements correspond to system elements which are strongly coupled to the target element. A neural network is then trained using previous process values corresponding to the coupled elements. Thereafter, present operating system values corresponding to the coupled elements are fed to the trained neural network. The trained neural network processes the present operating system values to render a predicted value for the target element. This predicted value is then compared to the present system value to determine whether the target element is operating correctly.", "Method and apparatus utilizing neural networks to predict a specified signal value within a multi-element system "]
["Methods and apparatus for monitoring an arc welding process are disclosed. In a preferred embodiment, the present invention creates a digital representation of the arc created during welding and, using a neural network computer, determines if the arc is representative of normal or abnormal welding conditions. The neural network disclosed is trained to identify abnormal conditions and normal conditions and may be adaptively retrained to classify images that are not in the initial set of normal and abnormal images. In certain embodiments, other data, such as current, weld wire emission spectra, or shielding gas flow rate are also collected and the neural network is trained to monitor these data. Also, in certain embodiments, an audio signal is collected from the vicinity of the welding process and is used by the neural network computer to further classify the arc as normal or abnormal. The present invention is most preferably implemented in repetitive and continuous welding operations, such as those encountered in the manufacture and rebuilding of steam turbines.", "Automated rotor welding processes using neural networks "]
["A neural network is trained to transform distant-talking cepstrum coefficients, derived from a microphone array receiving speech from a speaker distant therefrom, into a form substantially similar to close-talking cepstrum coefficients that would be derived from a microphone close to the speaker, for providing robust hands-free speech and speaker recognition in adverse practical environments with existing speech and speaker recognition systems which have been trained on close-talking speech.", "Method and apparatus including microphone arrays and neural networks for speech/speaker recognition systems "]
["A GPS receiver includes a satellite receiver/processor having an input that receives input signals from at least one GPS satellite. The output of the receiver/processor provides satellite-related navigation information. A neural network receives the satellite-related information to obtain an output signal representative of receiver-related navigation information. The neural network includes a first node layer connected to a second node layer through a first connection layer and a third node layer connected to the second node layer through a second connection layer. Each of the node layers comprises a plurality of neurons.", "GPS navigation system using neural networks "]
["A virtual vehicle sensor includes a neural network which produces a sensor output based on a linear combination of non-linear physical signals generated by conventional physical sensors. Instead of determining an output directly, the neural network determines the polynomial coefficients as functions of the physical signals indicative of other engine operating parameters. The sensor is manufactured using relatively limited data collection to calibrate a simulation model. The output of the simulation model is used for model-based mapping to generate more comprehensive maps used for training the neural network. The trained neural network is embedded in a controller and acts as the virtual sensor to monitor engine parameters which are difficult to measure or for which conventional physical sensors do not currently exist. The virtual sensor may be used to sense parameters such as in-cylinder residual mass fraction, emission levels, in-cylinder pressure rise during combustion, and exhaust gas temperature.", "Virtual vehicle sensors based on neural networks trained using data generated by simulation models "]
["An information processing system having signal processors that are interconnected by processing junctions that simulate and extend biological neural networks. Each processing junction receives signals from one signal processor and generates a new signal to another signal processor. The response of each processing junction is determined by internal junction processes and is continuously changed with temporal variation in the received signal. Different processing junctions connected to receive a common signal from a signal processor respond differently to produce different signals to downstream signal processors. This transforms a temporal pattern of a signal train of spikes into a spatio-temporal pattern of junction events and provides an exponential computational power to signal processors. Each signal processing junction can receive a feedback signal from a downstream signal processor so that an internal junction process can be adjusted to learn certain characteristics embedded in received signals.", "Dynamic synapse for signal processing in neural networks "]
["The present invention relates to adaptive information processing systems, and in particular to associative memories utilizing confidence-mediated associations, and especially neural network systems comprising an auto-organizational apparatus and processes for dynamically mapping an input onto a semantically congruous and contemporaneously-valid, learned response. In particular the present invention relates to such an associative memory system in which provision is made for improving the congruence between an associative memory, by impressing a desired response on an associative memory mapping based on complex polar values.", "Neural networks "]
["In a neural network of N neuron circuits, having an engaged neuron's calculated p bit wide distance between an input vector and a prototype vector and stored in the weight memory thereof, an aggregate search/sort circuit (517) of N engaged neurons' search/sort circuits. The aggregate search/sort circuit determines the minimum distance among the calculated distances. Each search/sort circuit (502-1) has p elementary search/sort units connected in series to form a column, such that the aggregate circuit is a matrix of elementary search/sort units. The distance bit signals of the same bit rank are applied to search/sort units in each row. A feedback signal is generated by ORing in an OR gate (12.1) all local search/sort output signals from the elementary search/sort units of the same row. The search process is based on identifying zeroes in the distance bit signals, from the MSB's to the LSB's. As a zero is found in a row, all the columns with a one in that row are excluded from the subsequent row search. The search process continues until only one distance, the minimum distance, remains and is available at the output of the OR circuit. The above described search/sort circuit may further include a latch allowing the aggregate circuit to sort remaining distances in increasing order.", "Circuit for searching/sorting data in neural networks "]
["The present invention provides a drilling system that utilizes a neural network for predictive control of drilling operations. A downhole processor controls the operation of the various devices in a bottom hole assembly to effect changes to drilling parameters and drilling direction to autonomously optimize the drilling effectiveness. The neural network iteratively updates a prediction model of the drilling operations and provides recommendations for drilling corrections to a drilling operator.", "Method and apparatus for prediction control in drilling dynamics using neural networks "]
["Constructing and simulating artificial neural networks and components thereof within a spreadsheet environment results in user friendly neural networks which do not require algorithmic based software in order to train or operate. Such neural networks can be easily cascaded to form complex neural networks and neural network systems, including neural networks capable of self-organizing so as to self-train within a spreadsheet, neural networks which train simultaneously within a spreadsheet, and neural networks capable of autonomously moving, monitoring, analyzing, and altering data within a spreadsheet. Neural networks can also be cascaded together in self training neural network form to achieve a device prototyping system.", "Non-algorithmically implemented artificial neural networks and components thereof "]
["A method for computer-aided detection of anomalies in an image comprise the steps of: (1) dividing the image into a plurality of m\u00d7n regions; (2) subtracting the background from each of the regions; (3) for each of the regions, selecting a smaller p\u00d7q subregion; (4) normalizing the p\u00d7q subregion; (5) feeding the p\u00d7q subregions into a neural network system, the neural network system having plural member neural networks, each trained to recognize a particular preselected anomaly type; (6) comparing each output value of the plurality of member neural networks to a first threshold; (7) selecting a maximum value from the output values which are greater than the first threshold; (8) comparing the maximum value to a second threshold above which the presence of an anomaly is indicated, and storing the result; (9) clustering a plurality of the stored results to form clusters; and (10) marking the location of the clusters.", "Application of neural networks as an aid in medical diagnosis and general anomaly detection "]
["A base neural semiconductor chip (10) including a neural network or unit (11(#)). The neural network (11(#)) has a plurality of neuron circuits fed by different buses transporting data such as the input vector data, set-up parameters, and control signals. Each neuron circuit (11) includes logic for generating local result signals of the \"fire\" type (F) and a local output signal (NOUT) of the distance or category type on respective buses (NR-BUS, NOUT-BUS). An OR circuit (12) performs an OR function for all corresponding local result and output signals to generate respective first global result (R*) and output (OUT*) signals on respective buses (R*-BUS, OUT*-BUS) that are merged in an on-chip common communication bus (COM*-BUS) shared by all neuron circuits of the chip. In a multi-chip network, an additional OR function is performed between all corresponding first global result and output signals (which are intermediate signals) to generate second global result (R**) and output (OUT**) signals, preferably by dotting onto an off-chip common communication bus (COM**-BUS) in the chip's driver block (19). This latter bus is shared by all the base neural network chips that are connected to it in order to incorporate a neural network of the desired size. In the chip, a multiplexer (21) may select either the intermediate output or the global output signal to be fed back to all neuron circuits of the neural network, depending on whether the chip is used in a single or multi-chip environment via a feed-back bus (OR-BUS). The feedback signal is the result of a collective processing of all the local output signals.", "Neural semiconductor chip and neural networks incorporated therein "]
["The disclosure relates to the use of genetic learning techniques to evolve neural network architectures for specific applications in which a general representation of neural network architecture is linked with a genetic learning strategy to create a very flexible environment for the construction of custom neural networks.", "Genetic algorithm synthesis of neural networks "]
["In a system comprising a plurality of resources for performing useful work, a resource allocation controller function, which is customized to the particular system's available resources and configuration, dynamically allocates resources and/or alters configuration to accommodate a changing workload. Preferably, the resource allocation controller is part of the computer's operating system which allocates resources of the computer system. The resource allocation controller uses a controller neural network for control, and a separate system model neural network for modelling the system and training the controller neural network. Performance data is collected by the system and used to train the system model neural network. A system administrator specifies computer system performance targets which indicate the desired performance of the system. Deviations in actual performance from desired performance are propagated back through the system model and ultimately to the controller neural network to create a closed loop system for resource allocation.", "Adaptive resource allocation using neural networks "]
["The present invention relates to a method of embedding a neural network into an application program such as a spreadsheet program. The method comprises providing an application program in which information is stored in rows and columns or a database containing fields and records and embedding a neural network in the application program or database using the stored information. The embedding step includes allocating unused memory in the application program and creating both a neural network engine and an application interface structure from the unused memory. Once the neural network engine and an application interface structure have been created, the neural network may be trained using variable numerical and symbolic data stored within the application program. Once training is completed, the neural network is ready for use, merely by using a recall function built into the applications program.", "Embedding neural networks into spreadsheet applications "]
["An on-line training neural network for process control system and method trains by retrieving training sets from the stream of process data. The neural network detects the availability of new training data, and constructs a training set by retrieving the corresponding input data. The neural network is trained using the training set. Over time, many training sets are presented to the neural network.", "Historical database training method for neural networks "]
["A color management method/apparatus generates image color matching and International Color Consortium (ICC) color printer profiles using a reduced number of color patch measurements. Color printer characterization, and the generation of ICC profiles usually require a large number of measured data points or color patches and complex interpolation techniques. This invention provides an optimization method/apparatus for performing LAB to CMYK color space conversion, gamut mapping, and gray component replacement. A gamut trained network architecture performs LAB to CMYK color space conversion to generate a color profile lookup table for a color printer, or alternatively, to directly control the color printer in accordance with the a plurality of color patches that accurately. represent the gamut of the color printer. More specifically, a feed forward neural network is trained using an ANSI/IT-8 basic data set consisting of 182 data points or color patches, or using a lesser number of data points such as 150 or 101 data points when redundant data points within linear regions of the 182 data point set are removed. A 5-to-7 neuron neural network architecture is preferred to perform the LAB to CMYK color space conversion as the profile lookup table is built, or as the printer is directly controlled. For each CMYK signal, an ink optimization criteria is applied, to thereby control ink parameters such as the total quantity of ink in each CMYK ink printed pixel, and/or to control the total quantity of black ink in each CMYK ink printed pixel.", "Color printer characterization using optimization theory and neural networks "]
["A mask neutral network for processing that allows an external source of control to continuously direct state transition of the neural network toward selected states and away from other states. The network, through externally controlled masking, can focus attention on selected attributes of observed data, solutions or results. The masking is appliciable across three major categories of networks in that it facilitates augmented recall, directed learning and constrained optimization.", "Mask controled neural networks "]
["Adaptive control for a wide variety of complex processes is provided by an ANN controller with input and hidden layers having a plurality of neurons, and an output layer with a single neuron. The inputs to the ANN are a time sequence of error values, and the neuron paths are weighted as a function of these error values and the present-time process output. The present-time error value may be added to the output layer of the ANN to provide faster response to sudden input changes. The controller of this invention can efficiently handle processes with nonlinear, time-varying, coupled and variable-structure behaviors as well as process parameter and/or structure uncertainties. Large steady-state gains in the process can be compensated by attenuating the ANN block output.", "Universal process control using artificial neural networks "]
["A maximum-likelihood sequence estimator receiver includes a matched filter connected to a digital transmission channel and a sampler for providing sampled signals output by the matched filter. The sampled signals are input to an analog neural network to provide high-speed outputs representative of the transmission channel signals. The neural network outputs are also provided as inputs to a coefficient estimator which produces coefficients for feedback to the matched filter. For time-varying transmission channel characteristics, the coefficient estimator provides a second coefficient output which is utilized for changing the interconnection strengths of the neural network connection matrix to offset the varying transmission channel characteristics.", "Digital adaptive receiver employing maximum-likelihood sequence estimation with neural networks "]
["Disclosed is a method, system, and program for filtering a data object for content deemed unacceptable by a user. A data object requested by a viewer program is received. The data object is processed to determine predefined language statements. Information on the determined language statements is inputted into a neural network to produce an output value. A determination is then made as to whether the output value indicates that the data object is unacceptable. Viewer program access to the data object is inhibited upon determining that the data object is unacceptable.", "Method, system, and program for filtering content using neural networks "]
["According to the present invention, a method for recognition of normal and abnormal conditions can be performed with at least one neural network. First, trend data of an object system, before a recognition-step, are entered as input data to an input layer of each neural network and data of this system at the recognition-step are entered as objective output data to an output layer of the neural network. Thus, multiple sets of trend data showing at least one normal condition of this system are formed in the neural network in order to obtained learned weights and biases. Next, output data at every recognition-step are predicted by entering actual trend data as input data to the neural network, while the learned weights and biases are utilized. Then, the predicted output data are compared with actual output data at every recognition-step. Finally, the normal and abnormal conditions of this system can be recognized by real time interpretation of deviations between the predicted output data and the actual output data. The method of the present invention particularly can be applied to a control system requiring the recognition of abnormal conditions such as a control system for the operation of a plant, an automobile, a robot, an aircraft, a marine vessel, a medical apparatus, security apparatus and the like.", "Method for recognition of abnormal conditions using neural networks "]
["A method and apparatus for implementation of neural networks for face recognition is presented. A nonlinear filter or a nonlinear joint transform correlator (JTC) employs a supervised perceptron learning algorithm in a two-layer neural network for real-time face recognition. The nonlinear filter is generally implemented electronically, while the nonlinear joint transform correlator is generally implemented optically. The system implements perception learning to train with a sequence of facial images and then classifies a distorted input image in real-time. Computer simulations and optical experimental results show that the system can identify the input with the probability of error less than 3%. By using time multiplexing of the input image under investigation, that is, using more than one input image, the probability of error for classification can be reduced to zero.", "Method and apparatus for implementation of neural networks for face recognition "]
["A control system for controlling the output of at least one plant process output parameter is implemented by adaptive model predictive control using a neural network. An improved method and apparatus provides for sampling plant output and control input at a first sampling rate to provide control inputs at the fast rate. The MPC system is, however, provided with a network state vector that is constructed at a second, slower rate so that the input control values used by the MPC system are averaged over a gapped time period. Another improvement is a provision for on-line training that may include difference training, curvature training, and basis center adjustment to maintain the weights and basis centers of the neural in an updated state that can follow changes in the plant operation apart from initial off-line training data.", "Adaptive model predictive process control using neural networks "]
["There is provided a customized personal terminal device capable of operating in response to input data peculiar to the operator, comprising a speech recognition unit for recognizing inputted speech, an image recognition unit for recognizing inputted image, and an instruction recognition unit for recognizing an inputted instruction. Neural networks respectively provided in at least two of the speech, image and instruction recognition units, a bus operatively connected to the respective recognition units, a processor operatively connected to the bus to perform processing upon the speech, and image and instruction recognized by the recognition units. Also, memory is operatively connected to the bus, and a control unit exercises control over information exchange between respective recognition units and the memory under the control of the processor.", "Apparatus including a pair of neural networks having disparate functions cooperating to perform instruction recognition "]
["A method for determining rock formation permeability from wireline well logs utilizes neural networks. The neural networks provide consistency, accuracy and overall quality without bias to the calculations.", "Method for estimating formation permeability from wireline logs using neural networks "]
["The pulsating behavioral activity of a neural network such as that embodiedn a brain tissue slice is monitored by measurement of intervals between spontaneous events to identify the presence of a chaotic regime and determine by real-time calculation a waiting time for electrical pulse intervention pursuant to a behavioral modifying program having a control or anti-control strategy.", "Pulsating behavior monitoring and modification system for neural networks "]
["A plurality of neural networks are coupled to an output neural network, or judge network, to form a clustered neural network. Each of the plurality of clustered networks comprises a supervised learning rule back-propagated neural network. Each of the clustered neural networks are trained to perform substantially the same mapping function before they are clustered. Following training, the clustered neural network computes its output by taking an \"average\" of the outputs of the individual neural networks that make up the cluster. The judge network combines the outputs of the plurality of individual neural networks to provide the output from the entire clustered network. In addition, the output of the judge network may be fed back to each of the individual neural networks and used as a training input thereto, in order to provide for continuous training. The use of the clustered network increases the speed of learning and results in better generalization. In addition, clustering multiple back-propagation networks provides for increased performance and fault tolerance when compared to a single unclustered network having substantially the same computational complexity. The present invention may be used in applications that are amenable to neural network solutions, including control and image processing applications. Clustering of the networks also permits the use of smaller networks and provides for improved performance. The clustering of multiple back-propagation networks provides for synergy that improves the properties of the clustered network over a comparably complex non-clustered network.", "Clustered neural networks "]
["A method enabling a neural network, having weights organized into a weight vector, to learn, comprising the steps of: (a) assigning a first memory location for storing a first learning rate, a second memory location for storing a momentum factor, a memory block for storing the weight vector, and a third memory location for storing a second learning rate; (b) initializing the learning rate, momentum factor, and weight vector; (c) storing the first learning rate, the momentum factor, and the weight vector into their respective memory locations; (d) saving the first learning rate in the second learning rate by storing it into the third memory location; (e) using a search technique to adjust the first learning rate to adjust the weight vector, and updating the first memory location and the memory block; (f) adapting the momentum factor using the first learning rate and the second learning rate; and repeating steps (c) through (f) until a predetermined convergence criterion has been met.", "Method and apparatus for adaptive learning in neural networks "]
["A digital data storage device such as a rotating magnetic disk drive contains an on-board condition monitoring system, comprising a neural network coupled to multiple inputs derived from measured parameters of disk drive operation. The neural network uses a configurable set of weights to compute one or more quantities representing disk drive condition as a function of the various inputs. The weights are stored in a configuration table, which can be overwritten by a host computer. The drive is sold and installed with one set of weights, based on the then existing knowledge of the disk drive designers, and may be updated in the field as the designers acquire experience data by simply writing the weights to the configuration table of the disk drive, without altering disk drive control code or other disk drive features. Preferably, the disk drive designers include as input to the neural network any parameter which might conceivably be useful, even if the designers initially believe that the parameter has no significance. In this case, the designers can assign the parameter a weight of zero during initial release. If subsequent experience then shows that the parameter has some unexpected significance, the neural network can be corrected simply by changing weighting factors, without altering the control programming code.", "Self-monitoring storage device using neural networks "]
["A system for predicting and evading crash of a vehicle includes an image pick-up device mounted on the vehicle for picking up images of actual ever-changing views when the vehicle is on running to produce actual image data, a crash predicting device associated with said image pick-up device, said crash predicting device being successively supplied with the actual image data for predicting occurrence of crash between the vehicle and potentially dangerous objects on the roadway to produce an operational signal when there is possibility of crash and a safety drive ensuring device connected to said crash predicting device for actuating, in response to the operational signal, an occupant protecting mechanism which is operatively connected thereto and equipped in the vehicle. The crash predicting device includes a neural network which is previously trained with training data to predict the possibility of crash, the training data representing ever-changing views previously picked-up from said image picking-up device during driving of the vehicle for causing actual crash.", "Vehicle crash predictive and evasive operation system by neural networks "]
["Pulse trains are utilized for the transmission of information in a neural network. A squash function is achieved by logically OR'ing together pulsed outputs, giving f(x) approximately 1-e-x. For Back Propagation, as derived by Rumelhart, the derivative of the squash function is available by examining the time when no OR'ed together pulses are present, being 1-f(x), or e-x. Logically AND'ing of the two signals. Mulitplication of input frequencies by weights is accomplished by modulating the width of the output pulses, while keeping the frequency the same.", "Spike transmission for neural networks "]
["A method and apparatus is provided for processing a measurement process to estimate a signal process, even if the signal and/or measurement processes have large and/or expanding ranges. The method synthesizes training data comprising realizations of the signal and measurement processes into a primary filter for estimating the signal process and, if required, an ancillary filter for providing the primary filter's estimation error statistics. The primary and ancillary filters each comprise an artificial recurrent neural network (RNN) and at least one range extender or reducer. Their implementation results in the filtering apparatus. Many types of range extender and reducer are disclosed, which have different degrees of effectiveness and computational cost. For a neural filter under design, range extenders and/or reducers are selected from those types jointly with the architecture of the RNN in consideration of the filtering accuracy, the RNN size and the computational cost of each selected range extender and reducer so as to maximize the cost effectiveness of the neural filter. The aforementioned synthesis is performed through training RNNs together with range extenders and/or reducers.", "Optimal filtering by neural networks with range extenders and/or reducers "]
["A method for recognizing an object image comprises the steps of extracting a candidate for mineda predetermined object image from an overall image, and making a judgment as to whether the extracted candidate for the predetermined object image is or is not the predetermined object image. The candidate for the predetermined object image is extracted by causing the center point of a view window, which has a predetermined size, to travel to the position of the candidate for the predetermined object image, and determining an extraction area in accordance with the size and/or the shape of the candidate for the predetermined object image, the center point of the view window being taken as a reference during the determination of the extraction area. A learning method for a neural network comprises the steps of extracting a target object image, for which learning operations are to be carried out, from an image, feeding a signal, which represents the extracted target object image, into a neural network, and carrying out the learning operations of the neural network in accordance with the input target object image.", "Method for recognizing object images and learning method for neural networks "]
["A neural network apparatus and method for use in applications such as in a voltage/reactive-power controller in which a neuro control-object simulator and a neuro controller pre-learn so as to make input-output relations of the controller match the input-output relations of a control unit and so as to make input-output relations of the simulator match input-output relations of a control object. The controller re-learns so as to make the output of the simulator match an input corresponding to a desired output of the control object. After re-learning, the controller controls the control-object.", "Control method using neural networks and a voltage/reactive-power controller for a power system using the control method "]
["A method of optimizing performance of a well system utilizes a neural network. In a described embodiment, the method includes the step of accumulating data indicative of the performance of the well system in response to variable influencing parameters. The data is used to train a neural network to model an output of the well system in response to the influencing parameters. An output of the neural network may then be input to a valuing model, e.g., to permit optimization of a value of the well system. The optimization process yields a set of prospective influencing parameters which may be incorporated into the well system to maximize its value.", "Field/reservoir optimization utilizing neural networks "]
["A method for storing and searching documents also useful in disambiguating word senses and a method for generating a dictionary of context vectors. The dictionary of context vectors provides a context vector for each word stem in the dictionary. A context vector is a fixed length list of component values corresponding to a list of word-based features, the component values being an approximate measure of the conceptual relationship between the word stem and the word-based feature. Documents are stored by combining the context vectors of the words remaining in the document after uninteresting words are removed. The summary vector obtained by adding all of the context vectors of the remaining words is normalized. The normalized summary vector is stored for each document. The data base of normalized summary vectors is searched using a query vector and identifying the document whose vector is closest to that query vector. The normalized summary vectors of each document can be stored using cluster trees according to a centroid consistent algorithm to accelerate the searching process. Said searching process also gives an efficient way of finding nearest neighbor vectors in high-dimensional spaces.", "Method for document retrieval and for word sense disambiguation using neural networks "]
["A method and apparatus for constructing, training and utilizing an artificial neural network (also termed herein a \"neural network\", an ANN, or an NN) in order to transform a first color value in a first color coordinate system into a second color value in a second color coordinate system.", "Method and apparatus for color processing with neural networks "]
["A method and system for implementing a neuro-controller. One example of a neuro-controller is a brain-like stochastic search. Another example is a neuro-controller for controlling a hypersonic aircraft. Using a variety of learning techniques, the method and system provide adaptable control of external devices (e.g., airplanes, plants, factories, and financial systems).", "Neural networks for intelligent control "]
["Network interactions within a Boundary Contour (BC) System, a Feature Contour (FC) System, and an Object Recognition (OR) System are employed to provide a computer vision system capable of recognizing emerging segmentations. The BC System is defined by a hierarchy of orientationally tuned interactions, which can be divided into two successive subsystems called the OC filter and the CC loop. The OC filter contains oriented receptive fields or masks, which are sensitive to different properties of image contrasts. The OC filter generates inputs to the CC loop, which contains successive stages of spatially shore-range competitive interactions and spatially long-range cooperative interactions. Feedback between the competitive and cooperative stages synthesizes a global context-sensitive segmentation from among the many possible groupings of local featural elements.", "Neural networks for machine vision "]
["A generic algorithm search is applied to determine an optimum set of values (e.g., interconnection weights in a neural network), each value being associated with a pair of elements drawn from a universe of N elements, N an integer greater than zero, where the utility of any possible set of said values may be measured. An initial possible set of values is assembled, the values being organized in a matrix whose rows and columns correspond to the elements. A genetic algorithm operator is applied to generate successor matrices from said matrix. Matrix computations are performed on the successor matrices to generate measures of the relative utilities of the successor matrices. A surviving matrix is selected from the successor matrices on the basis of the metrics. The steps are repeated until the metric of the surviving matrix is satisfactory.", "Genetic algorithm technique for designing neural networks "]
["A teaching method for a recurrent neural network having hidden, output and input neurons calculates weighting errors over a limited number of propagations of the network. This process permits the use of conventional teaching sets, such as are used with feedforward networks, to be used with recurrent networks. The teaching outputs are substituted for the computed activations of the output neurons in the forward propagation and error correction stages. Back propagated error from the last propagation is assumed to be zero for the hidden neurons. A method of reducing drift of the network with respect to a modeled process is also described and a forced cycling method to eliminate the time lag between network input and output.", "Recurrent neural networks teaching system "]
["A method, system, and computer program product for modifying an appearance of an anatomical structure in a medical image, e.g., rib suppression in a chest radiograph. The method includes: acquiring, using a first imaging modality, a first medical image that includes the anatomical structure; applying the first medical image to a trained image processing device to obtain a second medical image, corresponding to the first medical image, in which the appearance of the anatomical structure is modified; and outputting the second medical image. Further, the image processing device is trained using plural teacher images obtained from a second imaging modality that is different from the first imaging modality. In one embodiment, the method also includes processing the first medical image to obtain plural processed images, wherein each of the plural processed images has a corresponding image resolution; applying the plural processed images to respective multi-training artificial neural networks (MTANNs) to obtain plural output images, wherein each MTANN is trained to detect the anatomical structure at one of the corresponding image resolutions; and combining the plural output images to obtain a second medical image in which the appearance of the anatomical structure is enhanced.", "Image modification and detection using massive training artificial neural networks (MTANN) "]
["An electrochemical synapse adapted for use in a neural network which includes an input terminal and an output terminal located at a distance of less than 100 microns from the input terminal. A permanent interconnect having controllable conductivity is located between the two inputs. The conductivity of the permanent interconnect is controlled by either growing or eliminating metallic whiskers between the inputs. The growth and elimination of whiskers provides a rapid and controllable electrochemical synapse. Partial neural network systems are disclosed utilizing the electrochemical synapse.", "Electrochemical synapses for artificial neural networks "]
["An automated speech recognition system converts a speech signal into a compact, coded representation that correlates to a speech phoneme set. A number of different neural network pattern matching schemes may be used to perform the necessary speech coding. An integrated user interface guides a user unfamiliar with the details of speech recognition or neural networks to quickly develop and test a neural network for phoneme recognition. To train the neural network, digitized voice data containing known phonemes that the user wants the neural network to ultimately recognize are processed by the integrated user interface. The digitized speech is segmented into phonemes with each segment being labelled with a corresponding phoneme code. Based on a user selected transformation method and transformation parameters, each segment is transformed into a series of multiple dimension vectors representative of the speech characteristics of that segment. These vectors are iteratively presented to a neural network to train/adapt that neural network to consistently distinguish and recognize these vectors and assign an appropriate phoneme code to each vector. Simultaneous display of the digitized speech, segments, vector sets, and a representation of the trained neural network assist the user in visually confirming the acceptability of the phoneme training set. A user may also selectively audibly confirm the acceptability of the digitization scheme, the segments, and the transform vectors so that satisfactory training data are presented to the neural network. If the user finds a particular step or parameter produces an unacceptable result, the user may modify one or more of the parameters and verify whether the modification effected an improvement in performance. The trained neural network is also automatically tested by presenting a test speech signal to the integrated user interface and observing both audibly and visually automatic segmentation of the speech, transformation into multidimensional vectors, and the resulting neural network assigned phoneme codes. A method of decoding such phoneme codes using the neural network is also disclosed.", "Operator interactions for developing phoneme recognition by neural networks "]
["A method and system for data modeling that incorporates the advantages of both traditional response surface methodology (RSM) and neural networks is disclosed. The invention partitions the parameters into a first set of s simple parameters, where observable data are expressible as low order polynomials, and c complex parameters that reflect more complicated variation of the observed data. Variation of the data with the simple parameters is modeled using polynomials; and variation of the data with the complex parameters at each vertex is analyzed using a neural network. Variations with the simple parameters and with the complex parameters are expressed using a first sequence of shape functions and a second sequence of neural network functions. The first and second sequences are multiplicatively combined to form a composite response surface, dependent upon the parameter values, that can be used to identify an accurate model.", "Method for constructing composite response surfaces by combining neural networks with polynominal interpolation or estimation techniques "]
["A real-time waveform analysis system utilizes neural networks to perform various stages of the analysis. The signal containing the waveform is first stored in a buffer and the buffer contents transmitted to a first and second neural network which have been previously trained to recognize the start point and the end point of the waveform respectively. A third neural network receives the signal occurring between the start and end points and classifies that waveform as comprising either an incomplete waveform, a normal waveform or one of a variety of predetermined characteristic classifications. Ambiguities in the output of the third neural network are arbitrated by a fourth neural network which may be given additional information which serves to resolve these ambiguities. In accordance with the preferred embodiment, the present invention is applied to a system analyzing respiratory waveforms of a patient undergoing anesthesia and the classifications of the waveform correspond to normal or various categories of abnormal features functioning in the respiratory signal. The system performs the analysis rapidly enough to be used in real-time systems and can be operated with relatively low cost hardware and with minimal software development required.", "Real-time waveform analysis using artificial neural networks "]
["A potential estimation apparatus estimates a potential of a photosensitive body of an image forming apparatus that carries out an electro-photography process using the photosensitive body. The potential estimation apparatus includes a sensor group for sensing and outputting data related to information which affects the electro-photography process, a storage unit for at least storing the data output from the sensor group and information related to charge of the photosensitive body, and an estimation circuit including a neural network for estimating a charged portion potential of the photosensitive body based on a charge retentivity of the photosensitive body learned by the neural network. The neural network in a learning mode receives at least one of the data output from the sensor group and time-sequentially sampled, and parameters which affect the charge retentivity of the photosensitive body as an input, and receives as a teaching value a charged portion potential which is obtained in advance with respect to at least an amount of charge and the charge retentivity of the photosensitive body.", "Potential estimating apparatus using a plurality of neural networks for carrying out an electrographic process "]
["Control of a process in accordance with both optimal process values (td), which may be fixed or slowly varying, and actual process output values (tr-1) generated during a previous interval (r-1) is accomplished by a differential process controller (10). The controller (10) employs two artificial neural networks (36 and 38), each generating a separate intermediate control vector for controlling the process in accordance with a separate one of the vectors td and tr-1. A first summing amplifier (42) computes the difference between the intermediate control vectors and generates a differential control vector which varies accordingly. A second summing amplifier (44) sums the differential control vector, together with the output signal of the summing amplifier generated during the immediately previous interval (r-1), to generate a control signal cr for controlling the process.", "Differential process controller using artificial neural networks "]
["An optimization system is provided utilizing a Bayesian neural network calculation of a derivative wherein an output is optimized with respect to an input utilizing a stochastical method that averages over many regression models. This is done such that constraints from first principal models are incorporated in terms of prior art distributions.", "Bayesian neural networks for optimization and control "]
["Microprocessor assemblies are disclosed, which include a plurality of preprogrammed, cellular automaton microprocessors, a common radiant energy data waveguide into which said microprocessors can emit and from which said microprocessors can absorb modulated radiant energy signals, and a power supply radiant energy waveguide. Means are provided for each microprocessor to demodulate the absorbed signal and to modulate the emitted signal according to codes assigned to each said microprocessor. The common data waveguide provides for exchange of signals among the plurality of microprocessors. The relative degree of communication between two given microprocessors is determined by the degree to which the respective codes match. Said codes are subject to change. The change is determined by the demodulated inputs of the microprocessors. Means are provided for selective activation and deactivation of microprocessors through selective activation power supply. Changeability of the codes and the selective power supply provide for the means to induce a wide range of the desired patterns of connectivity and the desired degree of connectivity among the plurality of the microprocessors. There are also disclosed methods for making such microprocessor assemblies.", "Microprocessor assemblies forming adaptive neural networks "]
["A neural network structure includes input units for receiving input data, and a plurality of neural networks connected in parallel and connected to the input units. The plurality of neural networks learn in turn correspondence between the input data and teacher data so that the difference between the input data and the teacher becomes small. The neural network structure further includes output units connected to the plurality of neural networks, for outputting a result of learning on the basis of the results of learning in the plurality of neural networks.", "Plural neural network system having a successive approximation learning method "]
["A neural network apparatus, and methods for training the neural network apparatus, for processing input information, supplied as a data array, for a prespecified application to indicate output categories characteristic of the processing for that application. In the invention, an input stage accepts the data array and converts it to a corresponding internal representation, and a data preprocessor analyzes the data array based on a plurality of feature attributes to generate a corresponding plurality of attribute measures. A neural network, comprising a plurality of interconnected neurons, processes the attribute measures to reach a neural state representative of corresponding category attributes; portions of the network are predefined to include a number of neurons and prespecified with a particular correspondence to the feature attributes to accept corresponding attribute measures for the data array, and portions of the network are prespecified with a particular correspondence to the category attributes. A data postprocessor indicates the category attributes by correlating the neural state with predefined category attribute measures, and an output stage combines the category measures in a prespecified manner to generate on output category for the input information.", "Neural networks with subdivision "]
["A machine for neural computation of acoustical patterns for use in real-time speech recognition, comprising a plurality of analog electronic neurons connected for the analysis and recognition of acoustical patterns, including speech. Input to the neural net is provided from a set of bandpass filters which separate the input acoustical patterns into frequency ranges. The neural net itself is organized into two parts, the first for performing the real-time decomposition of the input patterns into their primitives of energy, space (frequency) and time relations, and the second for decoding the resulting set of primitives into known phonemes and diphones. During operation, the outputs of the individual bandpass filters are rectified and fed to sets of neurons in an opponent center-surround organization of synaptic connections (\"on center\" and \"off center\"). These units compute maxima and minima of energy at different frequencies. The next sets of neurons compute the temporal boundaries (\"on\" and \"off\"), while the following sets of neurons compute the movement of the energy maxima (formants) up or down the frequency axis. Then, in order to recognize speech sounds at the phoneme or diphone level, the set of primitives belonging to the phoneme is decoded such that only one neuron or a non-overlapping group of neurons fire when a particular sound pattern is present at the input. The output from these neurons is then fed to an Erasable Programmable Read Only Memory (EPROM) decoder and computer for displaying in real-time a phonetic representation of the speech input.", "Neural networks for acoustical pattern recognition "]
["Neural network architectures are represented by symbol strings. An initial population of networks is trained and evaluated. The strings representing the fittest networks are modified according to a genetic algorithm and the process is repeated until an optimized network is produced.", "Optimized artificial neural networks "]
["The present invention relates to the interrelationships between nature (as mediated by evolution and genetic algorithms) and nurture (as mediated by gradient-descent supervised learning) in a population of neural networks for pattern recognition. The Baldwin effect is demonstrated that learning can change the rate of evolution of the population's genome - a \"pseudo-Lamarkian\" process, in which information learned is ultimately encoded in the genome by a purely Darwinian process. Selectivity is shown for this effect: too much learning or too little learning in each generation leads to slow evolution of the genome, whereas an intermediate amount leads to most rapid evolution. For a given number of learning trials throughout a population, the most rapid evolution occurs if different individuals each receive a different number of learning trials, rather than the same number. Because all biological networks possess structure due to evolution, it is important that such interactions between learning and evolution be understood. Hybrid systems can take advantage both of gradient descents (learning) and large jumps (genetic algorithms) in very complicated energy landscapes and hence may play an increasingly important role in the design of artificial neural systems.", "Evolution and learning in neural networks: the number and distribution of learning trials affect the rate of evolution "]
["The present invention comprises systems and methods for handling large amounts of data prone to ambiguity and artifact in real-time in order to ensure patient safety while performing a procedure involving a sedation and analgesia system. The invention utilizes neural networks to weight data which may be more accurate or more indicative of true patient condition such that the patient condition reported to the controller and the user of a sedation and analgesia system will have increased accuracy and the incidence of false positive alarms will be reduced.", "Neural networks in sedation and analgesia systems "]
["A pattern recognition apparatus includes a pattern input unit inputting pattern data and learning data, and a neural network system including a plurality of neural networks, each of the plurality of neural networks being assigned a corresponding one of a plurality of identification classes and having only two output units of a first unit (Uo1) and a second unit (Uo2). Learning for each of the plurality of neural networks is performed by using the learning data. The image recognition apparatus also includes judgment unit judging which one of the identification classes the pattern data input from the image reading unit belongs to on the basis of output values A and B from the two output units (Uo1) and (Uo2) of all neural networks.", "Pattern recognition apparatus using a neural network system "]
["An analog synapse circuit for an artificial neural network requiring less circuitry and interconnections than prior synapses, while affording better weight programming means uses two complementary floating-gate MOSFETs with tunneling injection in an inverter configuration, with each MOSFET storing a weight value. This weight value is set by storing a charge injected by Fowler-Nordheim tunneling, or other tunneling means, into the floating-gate, which shifts the threshold voltage of the device. A programming line applies a current pulse to the MOSFET floating gate to write or erase this stored charge, thereby adjusting the weight of the MOSFET. The two MOSFETs are connected with the gate electrodes connected together and the drain electrodes connected together to provide a common gate and common drain between the two MOSFETs. An input line is connected to the common gate, and an output line is connected to the common drain. The source electrodes of each MOSFET are connected to reference voltages. The synapse circuit may be used in either a feedforword or feedback network, and may be expanded from two to four quadrant operation. The synapse provides a single output current line which represents a function of the input voltage and the stored weights. A plurality of such synapses may be configured in a network, wherein the output lines of each synapse are connected at a current summing node at the input of a neuron. An active load in the input of the neuron allows for both excitatory and inhibitory output current from the synapse circuit.", "Programmable analog synapse and neural networks incorporating same "]
["The invention relates to an apparatus for detecting fraud using a neural network. The architecture of the system involves first employing a conceptual clustering technique to generate a collection of classes from historical data. Neural networks are provided for each class created by the clustering step and the networks are trained using the same historical data. This apparatus is particularly useful for detecting the incidence of fraudulent activity from very large amounts of data such as tax returns or insurance claims.", "Neural network/conceptual clustering fraud detection architecture "]
["A method and an apparatus are disclosed for processing a measurement process to estimate a signal process. The method synthesizes realizations of a signal process and a measurement process into a primary filter for estimating the signal process and, if required, an ancillary filter for providing the primary filter's estimation error statistics. Both the primary and the ancillary filters are made out of artificial recurrent neural networks (RNNs). Their implementation results in the filtering apparatus. The synthesis is performed through training RNNs. The weights/parameters and initial dynamic state of an RNN are determined by minimizing a training criterion by the variation of the same. The training criterion, which is constructed on the basis of a selected estimation error criterion, incorporates the aforementioned realizations. An alternative way to determine the initial dynamic state of an RNN is to simply set it equal to a canonical initial dynamic state. After adequate training, both the primary and the ancillary filters are recursive filters optimal for the given respective RNN architectures with the lagged feedbacks carrying the optimal conditional statistics. If appropriate RNN paradigms and estimation error criteria are selected, the primary and the ancillary filters of such paradigms are proven to approximate the respective optimal filters in performance (with respect to the selected estimation error criteria) to any desired degree of accuracy, provided that the RNNs that constitute the primary and ancillary filters are of sufficient sizes.", "Optimal filtering by recurrent neural networks "]
["The present invention provides an apparatus for decoding and classifying a digital audio input signal and for reconstructing the digital audio input signal, so that when the reconstructed signal is converted to an analog signal by a digital to analog converter (\"DAC\"), the analog signal can drive a preamplifier, power amplifier or speakers directly. In particular, the present invention proposes a digital filter than can be adapted to have appropriate filtering characteristics based on the signal being filtered. The invention uses a neural network to adjust coefficients of a digital filter, depending on whether the digital audio input signal is more periodic or more aperiodic. If the digital audio input signal is more periodic, the coefficients will configure the digital filter so that the filter has the characteristics of an analog brickwall filter. Whereas if the digital audio input signal is more aperiodic, the coefficients produced by the neural network will configure the digital filter to have more characteristics of an interpolation filter. The neural network is trained to recognize certain periodic and aperiodic signals and to produce digital filter parameters, preferably polynomial coefficients, correspondingly. The coefficients are selected to respond to the pure or blended periodic and aperiodic features of certain archetypal input signals.", "Dynamic digital filter using neural networks "]
["A learning system is provided, which includes network storage means for storing a network including a plurality of nodes, each of which holds a dynamics; and learning means for self-organizationally updating the dynamics of the network on the basis of measured time-series data.", "Apparatus and method for embedding recurrent neural networks into the nodes of a self-organizing map "]
["Seismic facies are identified in a volume of seismic data, wherein, first, a plurality of initial textural attributes representative of the volume of seismic data are calculated. Next, a probabilistic neural network is constructed from the calculated initial textural attributes. Then, final textural attributes are calculated throughout the volume of seismic data. Finally, the calculated final textural attributes are classified using the constructed probabilistic neural network.", "Method for seismic facies interpretation using textural analysis and neural networks "]
["A method of global optimization of complex, highly nonlinear, multivariant systems is described. An artificial neural network (ANN) is trained to create an approximate inverse model. The desired behavior for a particular system is then input to the inverse model to derive approximate model parameters for the particular system. Optimization of the approximate model parameters yields optimal model parameters. The method is applied to the synthesis of mechanical linkages where examples of a type of linkage mechanism are used to train an ANN and derive the approximate inverse model. Inverse models for a number of linkage mechanism types are derived and stored. For a linkage mechanism with unknown linkage parameters, a power spectrum representation of the coupler curve is developed and the inverse model for the type of linkage mechanism retrieved. The representation of the desired coupler curve is input and the approximate linkage parameters derived. Optimization further refines the linkage parameters.", "System and method of global optimization using artificial neural networks "]
["A method, system, and computer program product of selecting a set of training images for a massive training artificial neural network (MTANN). The method comprises selecting the set of training images from a set of domain images; training the MTANN with the set of training images; applying a plurality of images from the set of domain images to the trained MTANN to obtain a corresponding plurality of scores; and determining the set of training images based on the plurality of images, the corresponding plurality of scores, and the set of training images. The method is useful for the reduction of false positives in computerized detection of abnormalities in medical images. In particular, the MTAAN can be used for the detection of lung nodules in low-dose CT (LDCT). The MTANN consists of a modified multilayer artificial neural network capable of operating on image data directly.", "Method of training massive training artificial neural networks (MTANN) for the detection of abnormalities in medical images "]
["Neural networks for optimal estimation (including prediction) and/or control involve an execution step and a learning step, and are characterized by the learning step being performed by neural computations. The set of learning rules cause the circuit's connection strengths to learn to approximate the optimal estimation and/or control function that minimizes estimation error and/or a measure of control cost. The classical Kalman filter and the classical Kalman optimal controller are important examples of such an optimal estimation and/or control function. The circuit uses only a stream of noisy measurements to infer relevant properties of the external dynamical system, learn the optimal estimation and/or control function, and apply its learning of this optimal function to input data streams in an online manner. In this way, the circuit simultaneously learns and generates estimates and/or control output signals that are optimal, given the network's current state of learning.", "Neural networks for prediction and control "]
["A system, method and computer program product for visualization of context-based search results, including a plurality of neurons, the neurons being associated with words and documents; a plurality of connections between the neurons; a map that displays at least some of the neurons to a user, wherein the display of the neurons on the map corresponds to their relevance to a search query; a display of the links to the relevant documents; and means for changing positions of the neurons relative to each other based on input from the user. Changing a position of one neuron relative to other neurons also changes positions of other contextually relevant neurons, and displays different relevant documents. The map displays the neurons with their relevance identified by any of font type, color, transparency and font size. The map includes icons in proximity to the displayed word neurons for identifying those neurons as irrelevant. Links to the documents are obtained from a search engine having an input query. The map displays annotations and/or keywords to the documents next to the displayed documents.", "Context-based search visualization and context management using neural networks "]
["A hybrid network 100 which combines a neural network of the self-organized type 110 with a plurality of neural networks of the supervised learning type 150,160,170 to successfully retrieve building address information from a database using imperfect textual retrieval keys. Generally, the self-organized type is a Kohonen Feature Map network, whereas each supervised learning type is a Back Propagation network. A user query 105 produces an activation response 111,112,113 from the self-organized network 110 and this response, along with a new query 151,161,171 derived from the original query 105, activates a selected one of the learning networks R1,R2,RM to retrieve the requested information.", "Hybrid multi-layer neural networks "]
["A method and system for is disclosed for speech synthesis using deep neural networks. A neural network may be trained to map input phonetic transcriptions of training-time text strings into sequences of acoustic feature vectors, which yield predefined speech waveforms when processed by a signal generation module. The training-time text strings may correspond to written transcriptions of speech carried in the predefined speech waveforms. Subsequent to training, a run-time text string may be translated to a run-time phonetic transcription, which may include a run-time sequence of phonetic-context descriptors, each of which contains a phonetic speech unit, data indicating phonetic context, and data indicating time duration of the respective phonetic speech unit. The trained neural network may then map the run-time sequence of the phonetic-context descriptors to run-time predicted feature vectors, which may in turn be translated into synthesized speech by the signal generation module.", "Speech synthesis using deep neural networks "]
["A method of estimating the chromaticity of illumination of a colored image consisting of a plurality of color-encoded pixels. The image colors are first mapped into an intensity-independent chromaticity space which is then divided into a plurality of separate regions. For each region, a first binary value is assigned to the region if the region contains no chromaticity value; or, a second binary value is assigned to the region if it does contain a chromaticity value. The assigned values are then applied as inputs to a pre-trained neural network having two output ports and at least one intermediate layer containing a plurality rality of ports connectible between selected input ports and the output ports. The chromaticity space values which characterize the input image's chromaticity of illumination are then derived at the output ports. The network is pretrained trained by initially connecting an arbitrary number of the intermediate layer ports to selected input layer ports. A weight value is associated with each connection. The weight values, which have the effect of altering signals transmitted along each connection by a selected amount, are initialized with random values. Each one of a plurality of pre-stored data sets, each containing values characterizing presence or absence of color in selected regions of one of a corresponding plurality of known colored images, are sequentially presented as inputs to the network and the chromaticity space values derived at the output ports are compared with known chromaticity space values characterizing illumination of the known colored image to derive an error value representative of difference therebetween. The weight values are adjusted in response to the inputs in accordance with the well known back propagation algorithm. After the weights are adjusted the intermediate layer ports are adaptively reconnected to the input layer ports to eliminate connections to input layer ports which repeatedly receive zero value inputs. The training process continues until the error value is less than a selected threshold.", "Method of estimating chromaticity of illumination using neural networks "]
["Neural network type information processing devices have been proposed. In these devices, a matrix structure is utilized with impedance at the matrix intersection points. It has been found that excellent versatility in design is achieved by utilizing photoconductors at these intersection points and thus affording the possibility of controlling impedance by, in turn, controlling the level of incident light.", "Neural networks "]
["Artificial neural networks include a plurality of artificial neurons and a plurality of Boolean-complete compartments, a respective one of which couples a respective pair of artificial neurons. By providing Boolean-complete compartments, spurious complement memories can be avoided. A Boolean-complete compartment includes a collection of at least four Boolean functions that represent input vectors to the respective pair of artificial neurons. The collection of at least four Boolean functions are selected from sixteen possible Boolean functions that can represent input vectors to the respective pair of artificial neurons. A count for each of the at least four Boolean functions is also provided. The count represents a number of occurrences of each of the at least four Boolean functions in input vectors to the respective pair of artificial neurons. In order to read the artificial neural network, the network also includes a collection of transfer functions, a respective one of which is associated with a respective one the sixteen possible Boolean functions.", "Artificial neural networks including Boolean-complete compartments "]
["A system and method of detecting an aberrant message is provided. An ordered set of words within the message is detected. The set of words found within the message is linked to a corresponding set of expected words, the set of expected words having semantic attributes. A set of grammatical structures represented in the message is detected, based on the ordered set of words and the semantic attributes of the corresponding set of expected words. A cognitive noise vector comprising a quantitative measure of a deviation between grammatical structures represented in the message and an expected measure of grammatical structures for a message of the type is then determined. The cognitive noise vector may be processed by higher levels of the neural network and/or an external processor.", "Intelligent control with hierarchical stacked neural networks "]
["A system and method for training a neural network that ceases training at or near the optimally trained point is presented. A neural network having an input layer, a hidden layer, and an output layer with each layer having one or more nodes is presented. Each node in the input layer is connected to each node in the hidden layer and each node in the hidden layer is connected to each node in the output layer. Each connection between nodes has an associated weight. All nodes in the input layer are connected to a different historical datum from the set of historical data. The neural network being operative by outputting a prediction or classification, the output of the output layer nodes, when presented with input data. The weights associated with the connections of the neural network are first adjusted by a training device. The training device then iteratively applies a training set to the neural network, the training set consisting of historical data. After each iteration the weights associated with the connections are adjusted according to the difference between the prediction or classification produced by the neural network given the training data and the known prediction or classification of the historical data. Additionally, after each iteration, a test set, consisting of different historical data from that in the training set, is presented to the neural network. The training device then determines the difference between the known result from the test set and the result from the presentation of the test set to the neural network. This difference, herein referred to as the variance, is then recorded along with the weights in the neural network. The variance is monitored at each iteration to determine if it is monotonically, within a given margin of error, decreasing. That is the prediction or classification resulting from the test set being presented to the neural network is getting successively closer to matching the known result from the test set. When the variance hits the inflection point where it begins to increase, training is ceased. At this point the neural network is no longer learning the pattern underlying the input data, but is instead over fitting the input data.", "Optimum cessation of training in neural networks "]
["A discriminant neural network and a method of training the network are disclosed. The network includes a set of hidden nodes having associated weights, and the number of hidden nodes is minimized by the training method of the invention. The training method includes the steps of 1) loading a training data set and assigning it to a residual data set, 2) computing a vector associated with a first hidden node using the residual data set, 3) projecting training data onto a hyperplane associated with said first hidden node, 4) determining the number and locations of hard-limiter thresholds associated with the first node, and 5) repeating the above for successive hidden nodes after removing satisfied subsets from the training data until all partitioned regions of the input data space are satisfied.", "Discriminant neural networks "]
["A fatigue monitoring system and method is disclosed in which a stream of data relating to the stresses experienced at a plurality of locations over the structure during operation is applied to a neural network trained to remove data stream values deemed to be in error. The data from the neural network is then processed to determine the fatigue life.", "Fatigue monitoring systems and methods incorporating neural networks "]
["Based on the encoding of deterministic finite-state automata (DFA) in discrete-time, second-order recurrent neural networks, an algorithm constructs an augmented recurrent neural network that encodes a FFA and recognizes a given fuzzy regular language with arbitrary accuracy.", "Deterministic encoding of fuzzy finite state automata in continuous recurrent neural networks "]
["A pattern searching method using neural networks and correlation. This method combines the quickness and adaptiveness of neural networks with the accuracy of the mathematical correlation approach. Images are divided into small sub-images which are presented to the trained neural network. Sub-images that may contain the pattern or partial pattern are selected by the neural network. The neural network also provides the approximate location of the pattern, therefore the selected sub-images can be adjusted to contain the complete pattern. Desired patterns can be located by measuring the new sub-images' correlation values against the reference models in a small area. Experiments show that this superior method is able to find the desired patterns. Moreover, this method is much faster than traditional pattern searching methods which use only correlation.", "Pattern searching method using neural networks and correlation "]
["A novel class of information-processing systems called a cellular neural network is discussed. Like a neural network, it is a large-scale nonlinear analog circuit which processes signals in real time. Like cellular automata, it is made of a massive aggregate of regularly spaced circuit clones, called cells, which communicate with each other directly only through its nearest neighbors. Each cell is made of a linear capacitor, a nonlinear voltage-controlled current source, and a few resistive linear circuit elements. Cellular neural networks share the best features of both worlds; its continuous time feature allows real-time signal processing found within the digital domain and its local interconnection feature makes it tailor made for VLSI implementation. Cellular neural networks are uniquely suited for high-speed parallel signal processing.", "Cellular neural network "]
["Neural networks are constructed (programmed), trained on historical data, and used to predict any of (1) optimal patient dosage of a single drug, (2) optimal patient dosage of one drug in respect of the patient's concurrent usage of another drug, (3a) optimal patient drug dosage in respect of diverse patient characteristics, (3b) sensitivity of recommended patient drug dosage to the patient characteristics, (4a) expected outcome versus patient drug dosage, (4b) sensitivity of the expected outcome to variant drug dosage(s), (5) expected outcome(s) from drug dosage(s) other than the projected optimal dosage. Both human and economic costs of both optimal and sub-optimal drug therapies may be extrapolated from the exercise of various optimized and trained neural networks. Heretofore little recognized sensitivities\u2014such as, for example, patient race in the administration of psychotropic drugs\u2014are made manifest. Individual prescribing physicians employing deviant patterns of drug therapy may be recognized. Although not intended to prescribe drugs, nor even to set prescription drug dosage, the neural networks are very sophisticated and authoritative \u201chelps\u201d to physicians, and to physician reviewers, in answering \u201cwhat if\u201d questions.", "Neural network drug dosage estimation "]
["An architecture and data processing method for a neural network that can approximate any mapping function between the input and output vectors without the use of hidden layers. The data processing is done at the sibling nodes (second row). It is based on the orthogonal expansion of the functions that map the input vector to the output vector. Because the nodes of the second row are simply data processing stations, they remain passive during training. As a result the system is basically a single-layer linear network with a filter at its entrance. Because of this it is free from the problems of local minima. The invention also includes a method that reduces the sum of the square of errors over all the output nodes to zero (0.000000) in fewer than ten cycles. This is done by initialization of the synaptic links with the coefficients of the orthogonal expansion. This feature makes it possible to design a computer chip which can perform the training process in real time. Similarly, the ability to train in real time allows the system to retrain itself and improve its performance while executing its normal testing functions. Because the second synaptic link values represent the frequency spectrum of the signal appearing on a given output node, by training the ONN with all N sibling nodes and using only some of them in testing, we can create a low pass, a high pass or a band pass filter.", "Artificial neural network method and architecture adaptive signal filtering "]
["The method of making the tool, for process system identification that is based on the general purpose learning capabilities of neural networks. The method can be used for a wide variety of system identification problems with little or no analytic effort. A neural network is trained using a process model to approximate a function which relates process input and output data to process parameter values. Once trained, the network can be used as a system identification tool. In principle, this approach can be used for linear or nonlinear processes, for open or closed loop identification, and for identifying any or all process parameters.", "Method for process system identification using neural network "]
["A method for recognizing an object image comprises the steps of extracting a candidate for a predetermined object image from an overall image, and making a judgment as to whether the extracted candidate for the predetermined object image is or is not the predetermined object image. The candidate for the predetermined object image is extracted by causing the center point of a view window, which has a predetermined size, to travel to the position of the candidate for the predetermined object image, and determining an extraction area in accordance with the size and/or the shape of the candidate for the predetermined object image, the center point of the view window being taken as a reference during the determination of the extraction area. A learning method for a neural network comprises the steps of extracting a target object image, for which learning operations are to be carried out, from an image, feeding a signal, which represents the extracted target object image, into a neural network, and carrying out the learning operations of the neural network in accordance with the input target object image.", "Method for recognizing object images and learning method for neural networks "]
["A neural network, which can be implemented either in hardware or software, is constructed of neurons or neuron circuits each having only one significant processing element in the form of a multiplier. The number of training examples is compared to the number of neurons in the neural network to effectuate training. The neural network utilizes a training algorithm which does not require repetitive training and which yields a global minimum to each given set of input vectors.", "Neural network and method of using same "]
["A method of motion control for robotics and other automatically controlled machinery using a neural network controller with real-time environmental feedback. The method is illustrated with a two-finger robotic hand having proximity sensors and force sensors that provide environmental feedback signals. The neural network controller is taught to control the robotic hand through training sets using back- propagation methods. The training sets are created by recording the control signals and the feedback signal as the robotic hand or a simulation of the robotic hand is moved through a representative grasping motion. The data recorded is divided into discrete increments of time and the feedback data is shifted out of phase with the control signal data so that the feedback signal data lag one time increment behind the control signal data. The modified data is presented to the neural network controller as a training set. The time lag introduced into the data allows the neural network controller to account for the temporal component of the robotic motion. Thus trained, the neural network controlled robotic hand is able to grasp a wide variety of different objects by generalizing from the training sets.", "Method for neural network control of motion using real-time environmental feedback "]
["A physical neural network is disclosed, which includes a connection network comprising a plurality of molecular conducting connections suspended within a connection gap formed between one or more input electrodes and one or more output electrodes. One or more molecular connections of the molecular conducting connections can be strengthened or weakened according to an application of an electric field across said connection gap. Thus, a plurality of physical neurons can be formed from said molecular conducting connections of said connection network. Additionally, a gate can be located adjacent said connection gap and which comes into contact with said connection network. The gate can be connected to logic circuitry which can activate or deactivate individual physical neurons among said plurality of physical neurons.", "Nanotechnology neural network methods and systems "]
["A learning algorithm for the N-dimensional Coulomb network is disclosed which is applicable to multi-layer networks. The central concept is to define a potential energy of a collection of memory sites. Then each memory site is an attractor of other memory sites. With the proper definition of attractive and repulsive potentials between various memory sites, it is possible to minimize the energy of the collection of memories. By this method, internal representations may be \"built-up\" one layer at a time. Following the method of Bachmann et al. a system is considered in which memories of events have already been recorded in a layer of cells. A method is found for the consolidation of the number of memories required to correctly represent the pattern environment. This method is shown to be applicable to a supervised or unsupervised learning paradigm in which pairs of input and output patterns are presented sequentially to the network. The resulting learning procedure develops internal representations in an incremental or cumulative fashion, from the layer closest to the input, to the output layer.", "N-dimensional coulomb neural network which provides for cumulative learning of internal representations "]
["Disclosed is an integrated imaging sensor/neural network controller for combustion control systems. The controller uses electronic imaging sensing of chemiluminescence from a combustion system, combined with neural network image processing, to sensitively identify and control a complex combustion system. The imaging system used is not adversely affected by the normal emissions variations caused by changes in burner load and flame position. By incorporating neural networks to learn emission patterns associated with combustor performance, control using image technology is fast enough to be used in a real time, closed loop control system. This advance in sensing and control strategy allows use of the spatial distribution of important parameters in the combustion system in identifying the overall operation condition of a given combustor and in formulating a control response accorded to a pre-determined control model.", "Integrated imaging sensor/neural network controller for combustion systems "]
["A plant (72) is operable to receive control inputs c(t) and provide an output y(t). The plant (72) has associated therewith state variables s(t) that are not variable. A control network (74) is provided that accurately models the plant (72). The output of the control network (74) provides a predicted output which is combined with a desired output to generate an error. This error is back propagated through an inverse control network (76), which is the inverse of the control network (74) to generate a control error signal that is input to a distributed control system (73) to vary the control inputs to the plant (72) in order to change the output y(t) to meet the desired output. The control network (74) is comprised of a first network NET 1 that is operable to store a representation of the dependency of the control variables on the state variables. The predicted result is subtracted from the actual state variable input and stored as a residual in a residual layer (102). The output of the residual layer (102) is input to a hidden layer (108) which also receives the control inputs to generate a predicted output in an output layer (106). During back propagation of error, the residual values in the residual layer (102) are latched and only the control inputs allowed to vary.", "Residual activation neural network "]
["A system and method for forecasting that combines a neural network with a statistical forecast is presented. A neural network having an input layer, a hidden layer, and an output layer with each layer having one or more nodes is presented. Each node in the input layer is connected to each node in the hidden layer and each node in the hidden layer is connected to each node in the output layer. Each connection between nodes has an associated weight. One node in the input layer is connected to a statistical forecast that is produced by a statistical model. All other nodes in the input layer are connected to a different historical datum from the set of historical data. The neural network being operative by outputting a forecast, the output of the output layer nodes, when presented with input data. The weights associated with the connections of the neural network are first adjusted by a training device. The training device applies a plurality of training sets to the neural network, each training set consisting of historical data, an associated statistical output and a desired forecast, with each set of training data the training device determines a difference between the forecast produced by the neural network given the training data and the desired forecast, the training device then adjusts the weights of the neural network based on the difference.", "Forecasting using a neural network and a statistical forecast "]
["A neural network system is provided that models the system in a system model (12) with the output thereof providing a predicted output. This predicted output is modified or controlled by an output control (14). Input data is processed in a data preprocess step (10) to reconcile the data for input to the system model (12). Additionally, the error resulted from the reconciliation is input to an uncertainty model to predict the uncertainty in the predicted output. This is input to a decision processor (20) which is utilized to control the output control (14). The output control (14) is controlled to either vary the predicted output or to inhibit the predicted output whenever the output of the uncertainty model (18) exceeds a predetermined decision threshold, input by a decision threshold block (22). Additionally, a validity model (16) is also provided which represents the reliability or validity of the output as a function of the number of data points in a given data region during training of the system model (12). This predicts the confidence in the predicted output which is also input to the decision processor (20). The decision processor (20) therefore bases its decision on the predicted confidence and the predicted uncertainty. Additionally, the uncertainty output by the data preprocess block (10) can be utilized to train the system model (12).", "Method for operating a neural network with missing and/or incomplete data "]
["A job scheduler makes decisions concerning the order and frequency of access to a resource according to a substantially optimum delay cost function. The delay cost function is a single value function of one or more inputs, where at least one of the inputs is a delay time which increases as a job waits for service. The job scheduler is preferably used by a multi-user computer operating system to schedule jobs of different classes. The delay cost functions are preferably implemented by neural networks. The user specifies desired performance objectives for each job class. The computer system runs for a specified period of time, collecting data on system performance. The differences between the actual and desired performance objectives are computed, and used to adaptively train the neural network. The process repeats until the delay cost functions stabilize near optimum value. However, if the system configuration, workload, or desired performance objectives change, the neural network will again start to adapt.", "Adaptive job scheduling using neural network priority functions "]
["An on-line process control neural network using data pointers allows the neural network to be easily configured to use data in a process control environment. The inputs, outputs, training inputs and errors can be retrieved and/or stored from any available data source without programming. The user of the neural network specifies data pointers indicating the particular computer system in which the data resides or will be stored; the type of data to be retrieved and/or stored; and the specific data value or storage location to be used. The data pointers include maximum, minimum, and maximum change limits, which can also serve as scaling limits for the neural network. Data pointers indicating time-dependent data, such as time averages, also include time boundary specifiers. The data pointers are entered by the user of the neural network using pop-up menus and by completing fields in a template. An historical database provides both a source of input data and a storage function for output and error data.", "On-line process control neural network using data pointers "]
["A method and system for controlling a dynamic nonlinear plant. An input signal controls the plant and an output signal represents a state of the plant in response to the received input signal. A memory stores input and output signals corresponding to m consecutive past states of the plant. A computer neural network predicts a set of future output states representative of the output signal corresponding to the next n consecutive future states of the plant in response to a set of trial control inputs. The trial control inputs represent the input signal corresponding to the next n consecutive future states of the plant. The neural network predicts the future output states based on the past input and output signals and the future trial control inputs. A processor generates the trial control inputs and determines a performance index, indicative of plant performance over time in response to the trial control inputs, as a function of the future output states. The processor generates the input signal for controlling the plant and modifies it as a function of the trial control inputs so that the performance index reaches a desired value.", "Neural network predictive control method and system "]
["A method of processing information is provided. The method involves receiving a message; processing the message with a trained artificial neural network based processor, having at least one set of outputs which represent information in a non-arbitrary organization of actions based on an architecture of the artificial neural network based processor and the training; representing as a noise vector at least one data pattern in the message which is incompletely represented in the non-arbitrary organization of actions; analyzing the noise vector distinctly from the trained artificial neural network; searching at least one database; and generating an output in dependence on said analyzing and said searching.", "Intelligent control with hierarchical stacked neural networks "]
["A signal processing apparatus and concomitant method for learning and integrating features from multiple resolutions for detecting and/or classifying objects. The signal processing apparatus comprises a hierarchical pyramid of neural networks (HPNN) having a \u201cfine-to-coarse\u201d structure or a combination of the \u201cfine-to-coarse\u201d and the \u201ccoarse-to-fine\u201d structures.", "Method and apparatus for training a neural network to detect objects in an image "]
["Physical neural network systems and methods are disclosed. A physical neural network can be configured utilizing molecular technology, wherein said physical neural network comprises a plurality of molecular conductors, which form neural network connections thereof. A training mechanism can be provided for training said physical neural network to accomplish a particular neural network task based on a neural network training rule. The neural network connections are formed between pre-synaptic and post-synaptic components of said physical neural network. The neural network generally includes dynamic and modifiable connections for adaptive signal processing. The neural network training mechanism can be based, for example, on the Anti-Hebbian and Hebbian (AHAH) rule and/or other plasticity rules.", "Training of a physical neural network "]
["Apparatus and processes for recognizing and identifying materials. Characteristic spectra are obtained for the materials via spectroscopy techniques including nuclear magnetic resonance spectroscopy, infrared absorption analysis, x-ray analysis, mass spectroscopy and gas chromatography. Desired portions of the spectra may be selected and then placed in proper form and format for presentation to a number of input layer neurons in an offline neural network. The network is first trained according to a predetermined training process; it may then be employed to identify particular materials. Such apparatus and processes are particularly useful for recognizing and identifying organic compounds such as complex carbohydrates, whose spectra conventionally require a high level of training and many hours of hard work to identify, and are frequently indistinguishable from one another by human interpretation.", "Neural network system and methods for analysis of organic materials and structures using spectral data "]
["Analysis and evaluation of outage effects on the dynamic security of power systems is made with a neural network using composite contingency severity indices. A preferably small number of indices describes the power system characteristics immediately post-contingency. These indices are then used as classifiers of the safety of the power system. Using the values of the severity indices, an artificial neural network distinguishes between safe, stable contingencies and potentially unstable contingencies. The severity of the contingency is evaluated based upon a relatively small fixed set of severity indices that are calculated based on a partial time domain simulation. Because a fixed set of severity indices is used, the size and architecture of the neural network is problem independent, thus permitting its use with large scale power systems. Further, the amount of required time domain simulation for the selection of the potentially harmful unstable contingencies is reduced by screening out benign, stable appearing contingencies. The network is trained off-line using training cases that concentrate around the security boundary to reduce the number of cases required to train the neural network.", "Neural network for contingency ranking dynamic security indices for use under fault conditions in a power distribution system "]
["The present invention is directed to a neural network-based system for detecting the presence of a vehicle within a traffic scene. The vehicle detection system comprises an apparatus for producing an image signal representative of an image of the traffic scene and a trainable neural network for identifying the presence of a vehicle within the traffic scene. The present invention is also directed to a method for detecting the presence of a vehicle within a traffic scene. The vehicle detection method includes the steps of producing an image signal representative of an image of the traffic scene, collecting a training set of these image signals, training a neural network from this training set of image signals to correctly identify the presence of a vehicle within the traffic scene and performing surveillance of the traffic scene with the trained neural network to detect the presence of a vehicle.", "Neural network-based vehicle detection system and method "]
["Apparatus, and an accompanying method, for a neural network, particularly one suited for use in optical character recognition (OCR) systems, which through controlling back propagation and adjustment of neural weight and bias values through an output confidence measure, smoothly, rapidly and accurately adapts its response to actual changing input data (characters). Specifically, the results of appropriate actual unknown input characters, which have been recognized with an output confidence measure that lies within a pre-defined range, are used to adaptively re-train the network during pattern recognition. By limiting the maximum value of the output confidence measure at which this re-training will occur, the network re-trains itself only when the input characters have changed by a sufficient margin from initial training data such that this re-training is likely to produce a subsequent noticeable increase in the recognition accuracy provided by the network. Output confidence is measured as a ratio between the highest and next highest values produced by output neurons in the network. By broadening the entire base of training data to include actual dynamically changing input characters, the inventive neural network provides more robust performance than which heretofore occurs in neural networks known in the art.", "Neural network with back propagation controlled through an output confidence measure "]
["An architecture and data processing method for a neural network that can approximate any mapping function between the input and output vectors without the use of hidden layers. The data processing is done at the sibling nodes (second row). It is based on the orthogonal expansion of the functions that map the input vector to the output vector. Because the nodes of the second row are simply data processing stations, they remain passive during training. As a result the system is basically a single-layer linear network with a filter at its entrance. Because of this it is free from the problems of local minima. The invention also includes a method that reduces the sum of the square of errors over all the output nodes to zero (0.000000) in fewer than ten cycles. This is done by initialization of the synaptic links with the coefficients of the orthogonal expansion. This feature makes it possible to design a computer chip which can perform the training process in real time. Similarly, the ability to train in real time allows the system to retrain itself and improve its performance while executing its normal testing functions.", "Artificial neural network method and architecture "]
["A method for performing a variety of expert system functions on any continuous-state feedforward neural network. These functions include decision-making, explanation, computation of confidence measures, and intelligent direction of information acquisition. Additionally, the method converts the knowledge implicit in such a network into a set of explicit if-then rules.", "Neural network having expert system functionality "]
["A computer neural network regulatory process control system and method allows for the elimination of a human operator from real time control of the process. The present invention operates in three modes: training, operation (prediction), and retraining. In the training mode, training input data is produced by the control adjustment made to the process by the human operator. The neural network of the present invention is trained by producing output data using input data for prediction. The output data is compared with the training input data to produce error data, which is used to adjust the weight(s) of the neural network. When the error data is less than a preselected criterion, training has been completed. In the operation mode, the neutral network of the present invention provides output data based upon predictions using the input data. The output data is used to control a state of the process via an actuator. In the retraining mode, retraining data is supplied by monitoring the supplemental actions of the human operator. The retraining data is used by the neural network for adjusting the weight(s) of the neural network.", "Computer neural network regulatory process control system and method "]
["A method and device for indirect, quantitative estimation of blood pressure attributes and similar variable physiological parameters utilizing indirect techniques. The method of practice includes (i) generating a sequence of signals which are quantitative dependent upon the variable parameter, (ii) transmitting and processing the signals within a computer system and associated neural network capable of generating a single output signal for the combined input signals, (iii) directly determining an actual value for the parameter concurrent with the indirect generation of signals of the previous steps, (iv) applying weighting factors within the neural network at interconnecting nodes to force the output signal of the neural network to match the true value of the parameter as determined invasively, (v) recording the input signals, weighting factors and true value as training data within memory of the computer, and (vi) repeating the previous steps to develop sufficient training data to enable the neural network to accurately estimate parameter value upon future receipt of on-line input signals. Procedures are also described for preclassification of signals and artifact rejection. Following training of the neural network, further direct measurement is unnecessary and the system is ready for diagnostic application and noninvasive estimation of parameter values.", "Method for determining blood pressure utilizing a neural network "]
["The present invention provides a method and system for characterizing the sounds of ocean captured by passive sonar listening devices. The present invention accomplishes this by first generating a spectrogram from the received sonar signal. The spectrogram is characterized in terms of textural features and signal processing parameters. The textural features and signal processing parameters are fed into a neural network ensemble that has been trained to favor specific features and/or parameters. The trained neural network ensemble classifies the signal as either Type-I or clutter.", "Feedforward neural network system for the detection and characterization of sonar signals with characteristic spectrogram textures "]
["An economic phenomenon predicting and/or analyzing system using a neural network. In the disclosed system, time series data indicating economic phenomena are input to preparation modules, and moving-average values and their differences are generated. One of the preparation modules performs a predetermined process over the time series data indicating an economic phenomenon, i.e. the change of TOPIX, to remove trends. A pattern sorter sorts the trend-free data into a certain number of groups. Average values of various time series data, their differences and the result of pattern sorting are input to input layer neurons of the network. The network is provided in advance with learning information of the change of TOPIX in the past. The output of the output layer neurons will be a value of prediction of the change of TOPIX. For the output of hidden layer neurons, principal components are obtained by principal analysis modules. A correlation analysis module obtains a distribution of frequency of principal component rankings and analyzes the correlation between the explanation variants and the output of the neural network based on the obtained distribution of frequency.", "Economic phenomenon predicting and analyzing system using neural network "]
["A method and system for training an artificial neural network (\u201cANN\u201d) are disclosed. One embodiment of the method of the present invention initializes an artificial neural network by assigning values to one or more weights. An adaptive learning rate is set to an initial starting value and training patterns for an input layer and an output layer are stored. The input layer training pattern is processed in the ANN to obtain an output pattern. An error is calculated between the output layer training pattern and the output pattern and used to calculate an error ratio, which is used to adjust the value of the adaptive learning rate. If the error ratio is less than a threshold value, the adaptive learning rate can be multiplied by a step-up factor to increase the learning rate. If the error ratio is greater than the threshold value, the adaptive learning rate can be multiplied by a step-down factor to reduce the learning rate. The value of the weights used to initialize the ANN are adjusted based on the calculated error and the adaptive learning rate. The training method of the present invention is repeated until ANN achieves a final trained state.", "Method and system for training an artificial neural network "]
["A neural network for adjusting a setpoint in process control replaces a human operator. The neural network operates in three modes: training, operation, and retraining. In operation, the neural network is trained using training input data along with input data. The input data is from the sensor(s) monitoring the process. The input data is used by the neural network to develop output data. The training input data are the setpoint adjustments made by a human operator. The output data is compared with the training input data to produce error data, which is used to adjust the weights of the neural network so as to train it. After training has been completed, the neural network enters the operation mode. In this mode, the present invention uses the input data to predict output data used to adjust the setpoint supplied to the regulatory controller. Thus, the operator is effectively replaced. The present invention in the retraining mode utilizes new training input data to retrain the neural network by adjusting the weight(s).", "Computer neural network supervisory process control system and method "]
["In general, the invention is directed to a technique for selection of parameter configurations for a neurostimulator using neural networks. The technique may be employed by a programming device to allow a clinician to select parameter configurations, and then program an implantable neurostimulator to deliver therapy using the selected parameter configurations. The parameter configurations may include one or more of a variety of parameters, such as electrode configurations defining electrode combinations and polarities for an electrode set implanted in a patient. The electrode set may be carried by one or more implanted leads that are electrically coupled to the neurostimulator. In operation, the programming device executes a parameter configuration search algorithm to guide the clinician in the selection of parameter configurations. The search algorithm relies on a neural network that identifies potential optimum parameter configurations.", "Selection of neurostimulator parameter configurations using neural network "]
["A neural network system and method for diagnosing patients' medical conditions provide an efficient aid in identifying and interpreting factors which are significant in the medical diagnosis. The neural network is trained to recognize medical conditions by being provided with input data that is available for a number of patients, and diagnosis made by physicians in each case. Upon completion of the training period the neural network system uses input measurement and interview data to produce a score, or a graded classification, of a patient's medical condition that is accompanied with a diagnosis interpretation. The interpretation is a sorted catalogue of individual factors and interactions that influenced the score. The interpretive facility of the present invention is based on comparison with a set of nominal values for each input factor or interaction. It can assist the physician in making a diagnosis of the patient's condition and can further provide a \"second opinion\" that may confirm the physician's findings or point to ambiguities that call for a more detailed analysis.", "Computer-based neural network system and method for medical diagnosis and interpretation "]
["A predictive dialing system having a computer connected to a telephone switch stores a group of call records in its internal storage. Each call record contains a group of input parameters, including the date, the time, and one or more workload factors. Workload factors can indicate the number of pending calls, the number of available operators, the average idle time, the connection delay, the completion rate, and the nuisance call rate, among other things. In the preferred embodiment, each call record also contains a dial action, which indicates whether a call was initiated or not. These call records are analyzed by a neutral network to determine a relationship between the input parameters and the dial action stored in each call record. This analysis is done as part of the training process for the neutral network. After this relationship is determined, the computer system sends a current group of input parameters to the neural network, and, based on the analysis of the previous call records, the neural network determines whether a call should be intiated or not. The neural network bases its decision on the complex relationship it has learned from its training data--perhaps several thousand call records spanning several days, months, or even years. The neural network is able to automatically adjust--in a look ahead, proactive manner--for slow and fast periods of the day, week, month, and year.", "Look ahead method and apparatus for predictive dialing using a neural network "]
["A facial feature extraction method and apparatus uses the variation in light intensity (gray-scale) of a frontal view of a speaker's face. The sequence of video images are sampled and quantized into a regular array of 150\u00d7150 pixels that naturally form a coordinate system of scan lines and pixel position along a scan line. Left and right eye areas and a mouth are located by thresholding the pixel gray-scale and finding the centroids of the three areas. The line segment joining the eye area centroids is bisected at right angle to form an axis of symmetry. A straight line through the centroid of the mouth area that is at right angle to the axis of symmetry constitutes the mouth line. Pixels along the mouth line and the axis of symmetry in the vicinity of the mouth area form a horizontal and vertical gray-scale profile, respectively. The profiles could be used as feature vectors but it is more efficient to select peaks and valleys (maximas and minimas) of the profile that correspond to the important physiological speech features such as lower and upper lip, mouth corner, and mouth area positions and pixel values and their time derivatives as visual vector components. Time derivatives are estimated by pixel position and value changes between video image frames. A speech recognition system uses the visual feature vector in combination with a concomitant acoustic vector as inputs to a time-delay neural network.", "Facial feature extraction method and apparatus for a neural network acoustic and visual speech recognition system "]
["A system for automatically detecting and recognizing the identity of a deformable object such as a human face, within an arbitrary image scene. The system comprises an object detector implemented as a probabilistic DBNN, for determining whether the object is within the arbitrary image scene and a feature localizer also implemented as a probabilistic DBNN, for determining the position of an identifying feature on the object such as the eyes. A feature extractor is coupled to the feature localizer and receives coordinates sent from the feature localizer which are indicative of the position of the identifying feature and also extracts from the coordinates information relating to other features of the object such as the eyebrows and nose, which are used to create a low resolution image of the object. A probabilistic DBNN based object recognizer for determining the identity of the object receives the low resolution image of the object inputted from the feature extractor to identify the object.", "Neural network for locating and recognizing a deformable object "]
["The system of the present invention applies self-organizing and/or  supervd learning network methods to the problem of segmentation. The segmenter receives a visual field, implemented as a sliding window and distinguishes occurrences of complete characters from occurrences of parts of neighboring characters. Images of isolated whole characters are true objects and the opposite of true objects are anti-objects, centered on the space between two characters. The window is moved across a line of text producing a sequence of images and the segmentation system distinguishes true objects from anti-objects. Frames classified as anti-objects demarcate character boundaries, and frames classified as true objects represent detected character images. The system of the present invention may be a feedforward adaption using a symmetric triggering network. Inputs to the network are applied directly to the separate associative memories of the network. The associative memories produce a best match pattern output for each part of the input data. The associative memories provide two or more subnetworks which define data subsets, such as objects or anti-objects, according to previously learned examples. Multi-layer perceptron architecture may also be used in the system of the present invention rather than the symmetrically triggered feedforward adaptation with tradeoffs in training time but advantages in speed.", "Object/anti-object neural network segmentation "]
["A hybrid analyzer having a data derived primary analyzer and an error correction analyzer connected in parallel is disclosed. The primary analyzer, preferably a data derived linear model such as a partial least squares model, is trained using training data to generate major predictions of defined output variables. The error correction analyzer, preferably a neural network model is trained to capture the residuals between the primary analyzer outputs and the target process variables. The residuals generated by the error correction analyzer is summed with the output of the primary analyzer to compensate for the error residuals of the primary analyzer to arrive at a more accurate overall model of the target process. Additionally, an adaptive filter can be applied to the output of the primary analyzer to further capture the process dynamics. The data derived hybrid analyzer provides a readily adaptable framework to build the process model without requiring up-front knowledge. Additionally, the primary analyzer, which incorporates the PLS model, is well accepted by process control engineers. Further, the hybrid analyzer also addresses the reliability of the process model output over the operating range since the primary analyzer can extrapolate data in a predictable way beyond the data used to train the model. Together, the primary and the error correction analyzers provide a more accurate hybrid process analyzer which mitigates the disadvantages, and enhances the advantages, of each modeling methodology when used alone.", "Hybrid linear-neural network process control "]
["A method (2000), device (2200) and article of manufacture (2300) provide, in response to orthographic information, efficient generation of a phonetic representation. The method provides for, in response to orthographic information, efficient generation of a phonetic representation, using the steps of: inputting an orthography of a word and a predetermined set of input letter features; utilizing a neural network that has been trained using automatic letter phone alignment and predetermined letter features to provide a neural network hypothesis of a word pronunciation.", "Method, device and article of manufacture for neural-network based orthography-phonetics transformation "]
["A self-organizing pattern classification neural network system includes means for receiving incoming pattern of signals that were processed by feature extractors that extract feature vectors from the incoming signal. These feature vectors correspond to information regarding certain features of the incoming signal. The extracted feature vectors then each pass to separate self-organizing neural network classifiers. The classifiers compare the feature vectors to templates corresponding to respective classes and output the results of their comparisons. The output from the classifier for each class enter a discriminator. The discriminator generates a classification response indicating the best class for the input signal. The classification response includes information indicative of whether the classification is possible and also includes the identified best class. Lastly, the system includes a learning trigger for transferring a correct glass signal to the self-organizing classifiers so that they can determine the validity of their classification results.", "Self-organizing pattern classification neural network system "]
["A control system and method for a continuous process in which a trained neural network predicts the value of an indirectly controlled process variable and the values of directly controlled process variables are changed to cause the predicted value to approach a desired value.", "Process control using neural network "]
["A distributed control system (14) receives on the input thereof the control inputs and then outputs control signals to a plant (10) for the operation thereof. The measured variables of the plant and the control inputs are input to a predictive model (34) that operates in conjunction with an inverse model (36) to generate predicted control inputs. The predicted control inputs are processed through a filter (46) to apply hard constraints, the values of which are received from a control parameter block (22). During operation, predetermined criterion stored in the control parameter block (22) are utilized by a cost minimization block (42) to generate an error control signal which is minimized by the inverse model (36) to generate the control signals. The system works in two modes, an analyze mode and a runtime mode. In the analyze mode, the predictive model (34) and the inverse model (36) are connected to either training data or simulated data from the analyzer (30) and the operation of the plant (10) evaluated. The values of the hard constraints in filter (46) and the criterion utilized for the cost minimization (42) can then be varied to change the constraints on the control signals input to the control network, the predicted output of the predictive model (34) and the hard constraints stored in the filter (46). Cost coefficients can be utilized as the criterion to set the input values in accordance with predetermined cost constraints.", "Method and apparatus for analyzing a neural network within desired operating parameter constraints "]
["A computer neural network process measurement and control system and method uses real-time output data from a neural network to replace a sensor or laboratory input to a controller. The neural network can use readily available, inexpensive and reliable measurements from sensors as inputs, and produce predicted values of product properties as output data for input to the controller. The system and method overcome process deadtime, measurement deadtime, infrequent measurements, and measurement variability in laboratory data, thus providing improved control. An historical database can be used to provide a history of sensor and laboratory measurements to the neural network. The neural network can detect the appearance of new laboratory measurements in the history and automatically initiate retraining, on-line and in real-time. The system and method can use either a regulatory controller or a supervisory control architecture. A modular software implementation simplifies the building of multiple neural networks, and also optionally provides other control functions, such as supervisory controllers, expert systems, and statistical data filtering, thus allowing powerful extensions of the system and method. Template specification for the neural network, and data specification using data pointers allow the system and method to be more easily implemented.", "Neural network process measurement and control "]
["A neural network associative memory which has a single layer of primatives and which utilizes a variant of the generalized delta for calculating the connection weights between the primatives. The delta rule is characterized by its utilization of predetermined values for the primitive and an error index which compares, during iterations, the predetermined primative values with actual primative values until the delta factor becomes a predetermined minimum value.", "Neural network auto-associative memory with two rules for varying the weights "]
["The present invention is a direct digitally implemented network system in which neural nodes 24, 26 and 28 which output to the same destination node 22 in the network share the same channel 30. If a set of nodes does not output any data to any node to which a second set of nodes outputs data (the two sets of nodes to not overlap or intersect), the two sets of nodes are independent and do not share a channel and have separate channels 120 and 122. The network is configured as parallel operating non-intersecting segments or independent sets where each segment has a segment communication channel or bus 30. Each node in the independent set or segment is sequentially activated to produce an output by a daisy chain control signal. The outputs are thereby time division multiplexed over the channel 30 to the destination node 22. The nodes are implemented on integrated circuits 158 with multiple nodes per circuit. The outputs of the nodes on the circuits in a segment are connected to the segment channel. Each node includes a memory array 136 that stores the weights applied to each input via a multiplier 152. The multiplied inputs are accumulated and applied to a lookup table 132 that performs any threshold comparison operation. The output of the lookup table 134 is placed on a common bus serving as the channel for the independent set of nodes by a tristate driver 44 controlled by the daisy chain control signal.", "Segmented neural network with daisy chain control "]
["A method of automatically extracting metadata from a document. The method of the invention provides a computer readable document that includes blocks comprised of words, an authority list that includes common uses of a set of words, and a neural network trained to extract metadata from groupings of data called compounds. Compounds are created with one compound describing each of the blocks. Each compound includes the words making up the block, descriptive information about the blocks, and authority information associated with some of the words. The descriptive information may include such items as bounding box information, describing the size and position of the block, and font information, describing the size and type of font the words of the block use. The authority information is located by comparing each the words from the block to the authority list. The compounds are processed through the neural network to generate metadata guesses including word guesses, compound guesses and document guesses along with confidence factors associated with the guesses indicating the likelihood that each of the guesses is correct. The method may additionally include providing a document knowledge base of positioning information and size information for metadata in known documents. If the document knowledge base is provided, then the method includes deriving analysis data from the metadata guess and comparing the analysis data to the document knowledge base to determine metadata output.", "Automatic extraction of metadata using a neural network "]
["An extension directed integrated circuit device having a learning function on a Boltzmann model, includes a plurality of synapse representing units arrayed in a matrix, a plurality of neuron representing units, a plurality of educator signal control circuits, and a plurality of buffer circuits. Each synapse representing unit is connected to a pair of axon signal transfer lines and a pair of dendrite signal transfer lines. Each synapse representing unit includes a learning control circuit which derives synapse load change value data in accordance with predetermined learning rules in response to a first axon signal Si and a second axon signal Sj, a synapse load representing circuit which corrects a synapse load in response to the synapse load change valued data and holds the corrected synapse load value Wij, a first synapse coupling operating circuit which derives a current signal indicating a product Wij\u00b7Si from the synapse load Wij and the first axon signal Si and transfers the same to a first dendrite signal line, and a second product signal indicating a product Wij\u00b7Sj from the synapse load Wij and the second axon signal Sj and transfers the same onto a second dendrite signal line.", "Neural network integrated circuit device having self-organizing function "]
["A method and apparatus for disease, injury or condition screening or sensing wherein biopotentials are received from a plurality of measuring sensors located in the area of a suspected disease, injury or condition change site. These potentials are then processed and the processed values are provided to a particular type of neural network or a combination of neural networks uniquely adapted to receive and analyze data of an identifiable type to provide an indication of specific conditions.", "Neural network method and apparatus for disease, injury and bodily condition screening or sensing "]
["A neural network/expert system process control system and method combines the decision-making capabilities of expert systems with the predictive capabilities of neural networks for improved process control. Neural networks provide predictions of measurements which are difficult to make, or supervisory or regulatory control changes which are difficult to implement using classical control techniques. Expert systems make decisions automatically based on knowledge which is well-known and can be expressed in rules or other knowledge representation forms. Sensor and laboratory data is effictively used. In one approach, the output data from the neural network can be used by the controller in controlling the process, and the expert system can make a decision using sensor or lab data to control the controller(s). In another approach, the output data of the neural network can be used by the expert system in making its decision, and control of the process carried out using lab or sensor data. In another approach, the output data can be used both to control the process and to make decisions.", "Neural network/expert system process control system and method "]
["An on-line process control neural network using data pointers allows the neural network to be easily configured to use data in a process control environment. The inputs, outputs, training inputs and errors can be retrieved and/or stored from any available data source without programming. The user of the neural network specifies data pointers indicating the particular computer system in which the data resides or will be stored; the type of data to be retrieved and/or stored; and the specific data value or storage location to be used. The data pointers include maximum, minimum, and maximum change limits, which can also serve as scaling limits for the neural network. Data pointers indicating time-dependent data, such as time averages, also include time boundary specifiers. The data pointers are entered by the user of the neural network using pop-up menus and by completing fields in a template. An historical database provides both a source of input data and a storage function for output and error data.", "On-line process control neural network using data pointers "]
["A method of training an artificial neural network (ANN) involves receiving a likelihood distribution map as a teacher image, receiving a training image, moving a local window across sub-regions of the training image to obtain respective sub-region pixel sets, inputting the sub-region pixel sets to the ANN so that it provides output pixel values that are compared to output pixel values of corresponding teacher image pixel values to determine an error, and training the ANN to reduce the error. A method of detecting a target structure in an image involves scanning a local window across sub-regions of the image by moving the local window for each sub-region so as to obtain respective sub-region pixel sets, inputting the sub-region pixel sets to an ANN so that it provides respective output pixel values that represent likelihoods that respective image pixels are part of a target structure, the output pixel values collectively constituting a likelihood distribution map. Another method for detecting a target structure involves training N parallel ANNs on either (A) a same target structure and N mutually different non-target structures, or (B) a same non-target structure and N mutually different target structures, the ANNs outputting N respective indications of whether the image includes a target structure or a non-target structure, and combining the N indications to form a combined indication of whether the image includes a target structure or a non-target structure. The invention provides related apparatus and computer program products storing executable instructions to perform the methods.", "Massive training artificial neural network (MTANN) for detecting abnormalities in medical images "]
["An input to a complex multi-input process, such as injection molding, is optimized to produce a target output from that process through the use of a neural network trained to that process. A trial input is forward-propagated through the neural network and the output of the network compared to the target output. The difference is back-propagated through the network to determine an input error value in the network. This error value is used to correct the trial input. This correction process is repeated until the trial input produces the target output to within a predetermined degree of accuracy.", "Process optimization using a neural network "]
["A multi-layered pattern recognition neural network (30) is disclosed that comprises an input layer (50) that is operable to be mapped onto an input space that includes a scan window (32). Two hidden layers (54) and (58) map the input space to an output layer (34). The hidden layers utilize a local receptor field architecture and store representations of objects within the scan window (32) for mapping into one of a plurality of output nodes. Further, the output layer (34) is also operable to store representations of desired distances between the center of the scan window (32) and the next adjacent object thereto and also the distance between the center of the scan window (32) and the center of the current object. A scanning system can then utilize the information regarding the distance to the next adjacent object, which is stored in an output vector (40) to incrementally jump to the center of the next adjacent character rather than scan the entire distance therebetween. This is referred to as a saccade operation. Once the scan window ( 32) is disposed over the next object, a corrective saccade can be performed by utilizing the information output by the neural network (30) relating to the distance between the center of the scan window (32) and the current character. This information is output as an output vector (38) from the neural network (30).", "Pattern recognition neural network with saccade-like operation "]
["The neural engine (20) is a hardware implementation of a neural network for use in real-time systems. The neural engine (20) includes a control circuit (26) and one or more multiply/accumulate circuits (28). Each multiply/accumulate circuit (28) includes a parallel/serial arrangement of multiple multiplier/accumulators (84) interconnected with weight storage elements (80) to yield multiple neural weightings and sums in a single clock cycle. A neural processing language is used to program the neural engine (20) through a conventional host personal computer (22). The parallel processing permits very high processing speeds to permit real-time pattern classification capability.", "Neural engine for emulating a neural network "]
["A data processing system and method for selecting securities and constructing an investment portfolio is based on a set of artificial neural networks which are designed to model and track the performance of each security in a given capital market and output a parameter which is related to the expected risk adjusted return for the security. Each artificial neural network is trained using a number of fundamental and price and volume history input parameters about the security and the underlying index. The system combines the expected return/appreciation potential data for each security via an optimization process to construct an investment portfolio which satisfies predetermined aggregate statistics. The data processing system receives input from the capital market and periodically evaluates the performance of the investment portfolio, rebalancing it whenever necessary to correct performance degradations.", "Predictive neural network means and method for selecting a portfolio of securities wherein each network has been trained using data relating to a corresponding security "]
["A system and method are provided for the automated prediction of lightning strikes in a set of different spatial regions for different times in the future. In a preferred embodiment, the system utilizes measurements of many weather phenomena. The types of measurements that can be utilized in approximately the same geographical region as that for which the strike predictions are made. This embodiment utilizes a correlation network to relate these weather measurements to future lightning strikes.", "Neural network for predicting lightning "]
["An apparatus and method for implementing a neural network having N nodes coupled to one another by interconnections having interconnect weights Tij that quantify the influence of node j on node i. The apparatus comprises a node circuit for each node and a data processor. The data processor receives one or more library members, and transmits the interconnect weights to the node circuits. The data processor also stores a current state vector, and receives input data representing a library member to be retrieved. The data processor then performs an iteration in which the current state vector is sent to the node circuits, and an updated state vector is received from the node circuits, the iteration being commenced by setting the current state vector equal to the input data. Each node circuit comprises one or more stochastic processors for multiplying the state vector elements by the corresponding interconnect weights, to determine the updated state vector. Each stochastic processor preferably includes means for generating a pseudorandom sequence of numbers, and using such sequence to encode the interconnect weights and state vector elements into stochastic input signals that are then multiplied by a stochastic multiplier comprising delay means and an AND gate.", "Neural network using stochastic processing "]
["An autonomic system for updating a fuzzy neural network includes a process of calculating an estimated value based on fuzzy inference by using a neural network structure, wherein a parameter to be adjusted or identified by fuzzy inference and outputted from the neural network is made to correspond to coupling loads which are updated by learning, i.e., fuzzy rules and membership functions are adjusted by learning. This system is characterized in that the addition and deletion of fuzzy rules are conducted based on changes in output errors in an autonomic manner, thereby effectively obtaining appropriate numbers of fuzzy rules optimal for an object such as a vehicle engine having strong non-linearity. Fuzzy rules are formed by a combination of membership functions representing variables such as an engine speed and a throttle angle.", "Autonomic system for updating fuzzy neural network and control system using the fuzzy neural network "]
["An analog-digital crosspoint-network includes a plurality of rows and columns, a plurality of synaptic nodes, each synaptic node of the plurality of synaptic nodes disposed at an intersection of a row and column of the plurality of rows and columns, wherein each synaptic node of the plurality of synaptic nodes includes a weight associated therewith, a column controller associated with each column of the plurality of columns, wherein each column controller is disposed to enable a weight change at a synaptic node in communication with said column controller, and a row controller associated with each row of the plurality of rows, wherein each row controller is disposed to control a weight change at a synaptic node in communication with said row controller.", "Hardware analog-digital neural networks "]
["A method for generating an artificial neural network ensemble for determining stimulation design parameters. A population of artificial neural networks is trained to produce one or more output values in response to a plurality of input values. The population of artificial neural networks is optimized to create an optimized population of artificial neural networks. A plurality of ensembles of artificial neural networks is selected from the optimized population of artificial neural networks and optimized using a genetic algorithm having a multi-objective fitness function. The ensemble with the desired prediction accuracy based on the multi-objective fitness function is then selected.", "Determining stimulation design parameters using artificial neural networks optimized with a genetic algorithm "]
["A method and apparatus for using a neural network to process information includes multiple nodes arrayed in multiple layers for transforming input arrays from prior layers or the environment into output arrays for subsequent layers or output devices. Learning rules based on reinforcement are applied. Interconnections between nodes are provided in a manner whereby the number and structure of the interconnections are self-adjusted by the learning rules during learning. At least one of the layers is used as a processing layer, and multiple lateral inputs to each node of each processing layer are used to retrieve information. The invention provides rapid, unsupervised processing of complex data sets, such as imagery or continuous human speech, and captures successful processing or pattern classification constellations for implementation in other networks. The invention includes application-specific self-adjusting multi-layer architectures that employ reinforcement learning rules to create updated data arrays for computation.", "Self-adjusting multi-layer neural network architectures and methods therefor "]
["Systems and methods for implementing a touch user interface using an artificial neural network are described. A touch sensor with a touch surface produces tactile sensing data responsive to human touch made by a user to the touch surface. At least one processor performs calculations on the tactile sensing data and produces processed sensor data provided to at least one artificial neural network. The artificial neural networks perform operations on the processed sensor data to produce interpreted data that has user interface information responsive to the human touch. The artificial neural networks are able to distinguish among a plurality of gestures made by a user. In various implementations the touch sensor can include a capacitive matrix, pressure sensor array, LED array, or a video camera.", "Touch-based user interfaces employing artificial neural networks for hdtp parameter and symbol derivation "]
["Apparatus and methods for high-level neuromorphic network description (HLND) using tags. The framework may be used to define nodes types, define node-to-node connection types, instantiate node instances for different node types, and/or generate instances of connection types between these nodes. The HLND format may be used to define nodes types, define node-to-node connection types, instantiate node instances for different node types, dynamically identify and/or select network subsets using tags, and/or generate instances of one or more connections between these nodes using such subsets. To facilitate the HLND operation and disambiguation, individual elements of the network (e.g., nodes, extensions, connections, I/O ports) may be assigned at least one unique tag. The tags may be used to identify and/or refer to respective network elements. The HLND kernel may comprises an interface to Elementary Network Description.", "Tag-based apparatus and methods for neural networks "]
["The present invention is predicated upon the fact that an emission trace from a plasma glow used in fabricating integrated circuits contains information about phenoma which cause variations in the fabrication process such as age of the plasma reactor, densities of the wafers exposed to the plasma, chemistry of the plasma, and concentration of the remaining material. In accordance with the present invention, a method for using neural networks to determine plasma etch end-point times in an integrated circuit fabrication process is disclosed. The end-point time is based on in-situ monitoring of the optical emission trace. The back-propagation method is used to train the network. More generally, a neural network can be used to regulate control variables and materials in a manufacturing process to yield an output product with desired quality attributes. An identified process signature which reflects the relation between the quality attribute and the process may be used to train the neural network.", "Active neural network control of wafer attributes in a plasma etch process "]
["A real-time learning (RTL) neural network is capable of indicating when an input feature vector is novel with respect to feature vectors contained within its training data set, and is capable of learning to generate a correct response to a new data vector while maintaining correct responses to previously learned data vectors without requiring that the neural network be retrained on the previously learned data. The neural network has a sensor for inputting a feature vector, a first layer and a second layer. The feature vector is supplied to the first layer which may have one or more declared and unused nodes. During training, the input feature vector is clustered to a declared node only if it lies within a hypervolume defined by the declared node's automatically selectable reject radius, else the input feature vector is clustered to an unused node. Clustering in overlapping hypervolumes is determined by a decision surface. During testing of the RTL network, the same strategy is applied to cluster an input feature vector to declared (existing) nodes. If clustering occurs, then a classification signal corresponding to the node is generated. However, if the input feature vector is not clustered to a declared node, then the second layer outputs a signal indicating novelty. The RTL neural network is used in a perceptive system which alternatively selects the RTL network novelty output or the output of a classifier trained on historical target data if the input vector is a subset of the historical target data.", "Perceptive system including a neural network "]
["Apparatus and methods for high-level neuromorphic network description (HLND) framework that may be configured to enable users to define neuromorphic network architectures using a unified and unambiguous representation that is both human-readable and machine-interpretable. The framework may be used to define nodes types, node-to-node connection types, instantiate node instances for different node types, and to generate instances of connection types between these nodes. To facilitate framework usage, the HLND format may provide the flexibility required by computational neuroscientists and, at the same time, provides a user-friendly interface for users with limited experience in modeling neurons. The HLND kernel may comprise an interface to Elementary Network Description (END) that is optimized for efficient representation of neuronal systems in hardware-independent manner and enables seamless translation of HLND model description into hardware instructions for execution by various processing modules.", "Round-trip engineering apparatus and methods for neural networks "]
["Random access memory is used to store synaptic information in the form of a matrix of rows and columns of binary digits. N rows read in sequence are processed through switches and resistors, and a summing amplifier to N neural amplifiers in sequence, one row for each amplifier, using a first array of sample-and-hold devices S/H1 for commutation. The outputs of the neural amplifiers are stored in a second array of sample-and-hold devices S/H2 so that after N rows are processed, all of said second array of sample-and-hold devices are updated. A second memory may be added for binary values of 0 and -1, and processed simultaneously with the first to provide for values of 1, 0, and -1, the results of which are combined in a difference amplifier.", "Hybrid analog-digital associative neural network "]
["A signal processing apparatus and concomitant method for learning and integrating features from multiple resolutions for detecting and/or classifying objects are presented. Neural networks in a pattern tree structure with tree-structured descriptions of objects in terms of simple sub-patterns, are grown and trained to detect and integrate the sub-patterns. A plurality of objective functions and their approximations are presented to train the neural networks to detect sub-patterns of features of some class of objects. Objective functions for training neural networks to detect objects whose positions in the training data are uncertain and for addressing supervised learning where there are potential errors in the training data are also presented.", "Method and apparatus for training a neural network to learn hierarchical representations of objects and to detect and classify objects with uncertain training data "]
["A waveform analysis assembly (10) includes a sensor (12) for detecting physiological electrical and mechanical signals produced by the body. An extraction neural network (22, 22') will learn a repetitive waveform of the electrical signal, store the waveform in memory (18), extract the waveform from the electrical signal, store the location times of occurrences of the waveform, and subtract the waveform from the electrical signal. Each significantly different waveform in the electrical signal is learned and extracted. A single or multilayer layer neural network (22, 22') accomplishes the learning and extraction with either multiple passes over the electrical signal or accomplishes the learning and extraction of all waveforms in a single pass over the electrical signal. A reducer (20) receives the stored waveforms and times and reduces them into features characterizing the waveforms. A classifier neural network (36) analyzes the features by classifying them through nonliner mapping techniques within the network representing diseased states and produces results of diseased states based on learned features of the normal and patient groups.", "Waveform analysis apparatus and method using neural network techniques "]
["A system, method and computer program product for information searching includes (a) a first layer with a first plurality of neurons, each of the first plurality of neurons being associated with a word and with a set of connections to at least some neurons of the first layer; (b) a second layer with a second plurality of neurons, each of the second plurality of neurons being associated with an object and with a set of connections to at least some neurons of the second layer, and with a set of connections to some neurons of the first layer; (c) a third layer with a third plurality of neurons, each of the third plurality of neurons being associated with a sentence and with a set of connections to at least some neurons of the third layer, and with a set of connections to at least some neurons of the first layer and to at least some neurons of the second layer; and (d) a fourth layer with a fourth plurality of neurons, each of the fourth plurality of neurons being associated with a document and with a set of connections to at least some neurons of the fourth layer, and with a set of connections to at least some neurons of other layers. A query to the first layer identifies to a user, through the fourth layer, a set of documents that are contextually relevant to the query. Each connection has a corresponding weight and optional flags.", "Neural network for electronic search applications "]
["A diagnostic tester evaluates at least one inputted test signal corresponding to test data relating to at least one predetermined parameter of a system being tested, to produce first and second candidate signals corresponding respectively to first and second possible diagnoses of the condition of the system respectively having the first and second highest levels of certainty of being valid, and first and second certainty signals corresponding respectively to values of the first and second highest levels of certainty. The diagnostic tester further determines the sufficiency of the testing that has taken place responsive to the first and second certainty signals, and produces an output signal indicative of whether sufficient test data has been evaluated to declare a diagnosis. Preferably, an uncertainty signal corresponding to a measure of the uncertainty that the evaluated at least one test signal can be validly evaluated is also produced and used to produce the output signal.", "Method and apparatus for diagnostic testing including a neural network for determining testing sufficiency "]
["A system for automatically classification of human fingerprints. An unidentified fingerprint is processed to produce a direction map. The direction map is processed to generate a course direction map. The coarse direction map is input to a locally connected, highly constrained feed-forward neural network. The neural network has a highly structured architecture well-suited to exploit the rotational symmetries and asymmetries of human fingerprints. The neural network classifies the unidentified fingerprint into one of five classifications: Whorl, Double Loop, Left Loop, Right Arch and Arch.", "Neural network system for classifying fingerprints "]
["A method, system and machine-readable storage medium for monitoring an engine using a cascaded neural network that includes a plurality of neural networks is disclosed. In operation, the method, system and machine-readable storage medium store data corresponding to the cascaded neural network. Signals generated by a plurality of engine sensors are then inputted into the cascaded neural network. Next, a second neural network is updated at a first rate, with an output of a first neural network, wherein the output is based on the inputted signals. In response, the second neural network outputs at a second rate, at least one engine control signal, wherein the second rate is faster than the first rate.", "Engine control system using a cascaded neural network "]
["A neural network system and method that can adaptively recognize each of many pattern configurations from a set. The system learns and maintains accurate associations between signal pattern configurations and pattern classes with training from a teaching mechanism. The classifying system consists of a distributed input processor and an adaptive association processor. The input processor decomposes an input pattern into modules of localized contextual elements. These elements in turn are mapped onto pattern classes using a self-organizing associative neural scheme. The associative mapping determines which pattern class best represents the input pattern. The computation is done through gating elements that correspond to the contextual elements. Learning is achieved by modifying the gating elements from a true/false response to the computed probabilities for all classes in the set. The system is a parallel and fault tolerant process. It can easily be extended to accommodate an arbitrary number of patterns at an arbitrary degree of precision. The classifier can be applied to automated recognition and inspection of many different types of signals and patterns.", "Self organizing neural network method and system for general classification of patterns "]
["The present invention is predicated upon the fact that a process signature from a plasma process used in fabricating integrated circuits contains information about phenomena which cause variations in the fabrication process such as age of the plasma reactor, densities of the wafers exposed to the plasma, chemistry of the plasma, and concentration of the remaining material. In accordance with the present invention, a method for using neural networks to determine plasma etch end-point times in an integrated circuit fabrication process is disclosed. The end-point time is based on in-situ monitoring of at least two parameters during the plasma etch process. After the neural network is trained to associate a certain condition or set of conditions with the endpoint of the process, the neural network is used to control the process.", "Active neural network determination of endpoint in a plasma etch process "]
["An A pattern recognition subsystem responds to an A feature representation input to select A-category-representation and predict a B-category-representation and its associated B feature representation input. During learning trials, a predicted B-category-representation is compared to that obtained through a B pattern recognition subsystem. With mismatch, a vigilance parameter of the A-pattern-recognition subsystem is increased to cause reset of the first-category-representation selection. Inputs to the pattern recognition subsystems may be preprocessed to complement code the inputs.", "Predictive self-organizing neural network "]
["A system and method for controlling information output based on user feedback about the information that includes a plurality of information sources. At least one neural network module selects one or more of a plurality of objects to receive information from the plurality of information sources based on a plurality of inputs and a plurality of weight values during that epoch. At least one server, associated with the neural network module, provides one or more of the objects to a plurality of recipients. The recipients provide feedback during an epoch. At the conclusion of an epoch, the neural network takes the feedback that has been provided from the recipients and generates a rating value for each of the objects. Based on the rating value and selections made, the neural network redetermines the weight values. The neural network then selects the objects to receive information during a subsequent epoch.", "Neural network system and method for controlling information output based on user feedback "]
["A neural network system comprises a memory for storing in binary code the synaptic coefficients indicative of the interconnections among the neurons. Means are provided for simultaneously supplying all the synaptic coefficients associated with a given neuron. Digital multipliers are provided for determining the product of the supplied synaptic coefficients and the relevant neuron states of the neurons connected to said given neuron. The multipliers deliver their results into an adder tree for determining the sum of the products. As a result of the parallel architecture of the system high operating speeds are attainable. The modular architecture enables extension of the system.", "Neural network system and circuit for use therein "]
["A preprocessing device is disclosed which performs a linear transformation or power series expansion transformation on the input signals to a neural network node. The outputs of the preprocessing device are combined as a product of these linear transformations and compared to a threshold. This processing element configuration, combining a transformation with a product and threshold comparison, performs non-linear transformations between input data and output results. As a result, this processing element will, by itself, produce both linearly and non-linearly separable boolean logic functions. When this processing element is configured in a network, a two layer neural network can be created which will solve any arbitrary decision making function. This element can be configured in a probability based binary tree neural network which is validatable and verifiable in which the threshold comparison operation can be eliminated. The element can also be implemented in binary logic for ultra high speed. If the linkage element performs the power series expansion, a universal or general purpose element is created.", "Digital neural network processing elements "]
["A dynamically stable associative learning neural system includes a plurality of neural network architectural units. A neural network architectural unit has as input both condition stimuli and unconditioned stimulus, an output neuron for accepting the input, and patch elements interposed between each input and the output neuron. The patches in the architectural unit can be modified and added. A neural network can be formed from a single unit, a layer of units, or multiple layers of units.", "Dynamically stable associative learning neural network system "]
["A physical neural network based on nanotechnology, including methods thereof. Such a physical neural network generally includes one or more neuron-like nodes, which are formed from a plurality of interconnected nanoconnections formed from nanoconductors. Each neuron-like node sums one or more input signals and generates one or more output signals based on a threshold associated with the input signal. The physical neural network also includes a connection network formed from the interconnected nanoconnections, such that the interconnected nanoconnections used thereof by one or more of the neuron-like nodes are strengthened or weakened according to an application of an electric field.", "Physical neural network design incorporating nanotechnology "]
["A spot welder comprises a neural network for processing, in real time, current and voltage energizing a weld in progress. The neural network generates a predicted time of optimal weld strength and/or nugget size for the weld in progress. A controller terminates the weld in progress at the predicted time. A method for controlling a spot welder comprises the steps of: sensing in real time current and voltage energizing a spot weld in progress; predicting a time of optimal weld strength and/or nugget size with a neural network responsive to the sensed current and voltage; and, terminating the weld in progress at the predicted time. A sensor for electromotive forces (EMF) induced by the spot welder can generate a signal for canceling out a large fraction of EMF components in at least one or both of the current and voltage signals. EMF components are substantially precluded in the current signal if the current sensor uses a buried shunt. Termination of the weld in progress at the predicted time is prevented when the predicted time precedes a predetermined minimum weld duration. The weld in progress is terminated at a predetermined maximum weld duration when the predicted time is after the predetermined maximum weld duration.", "Neural network control of spot welding "]
["Designs for cognitive memory systems storing input data, images, or patterns, and retrieving it without knowledge of where stored when cognitive memory is prompted by query pattern that is related to sought stored pattern. Retrieval system of cognitive memory uses autoassociative neural networks and techniques for pre-processing query pattern to establish relationship between query pattern and sought stored pattern, to locate sought pattern, and to retrieve it and ancillary data. Cognitive memory, when connected to computer or information appliance introduces computational architecture that applies to systems and methods for navigation, location and recognition of objects in images, character recognition, facial recognition, medical analysis and diagnosis, video image analysis, and to photographic search engines that when prompted with a query photograph containing faces and objects will retrieve related photographs stored in computer or other information appliance, and will identify URL's of related photographs and documents stored on the World Wide Web.", "Cognitive memory and auto-associative neural network based search engine for computer and network located images and photographs "]
["A neural network system includes means for accomplishing artificial intelligence functions in three formerly divergent implementations. These functions include: supervised learning, unsupervised learning, and associative memory storage and retrieval. The subject neural network is created by addition of a non-linear layer to a more standard neural network architecture. The non-linear layer functions to expand a functional input space to a signal set including orthonormal elements, when the input signal is visualized as a vector representation. An input signal is selectively passed to a non-linear transform circuit, which outputs a transform signal therefrom. Both the input signal and the transform signal are placed in communication with a first layer of a plurality of processing nodes. An improved hardware implementation of the subject system includes a highly parallel, hybrid analog/digital circuitry. Included therein is a digitally addressed, random access memory means for storage and retrieval of an analog signal.", "Neural network with non-linear transformations "]
["A method and apparatus for providing fast charging of secondary cells in an electronic device. The charging process is under the control of a microcontroller which contains a read-only-memory (ROM) in which is embedded code which determines the charging method. The charge method controls the charge provided to a battery back by a variable current source. An intelligent control scheme based on a neural network fuzzy logic methodology is used to optimize the charging current in response to measured characteristics of the battery.", "Method and apparatus for fast battery charging using neural network fuzzy logic based control "]
["Apparatus, and an accompanying method, for use in an optical character recognition (OCR) system (5) for locating, e.g., center positions (\"hearts\") of all desired characters within a field (310; 510) of characters such that the desired characters can be subsequently recognized using an appropriate classification process. Specifically, a window (520) is slid in a step-wise convolutional-like fashion (5201, 5202, 5203) across a field of preprocessed, specifically uniformly scaled, characters. Each pixel in the window is applied as an input to a positioning neural network (152) that has been trained to produce an output activation whenever a character \"heart\" is spatially coincident with a pixel position within an array (430) centrally located within the window. As the window is successively moved across the field, in a stepped fashion, the activation outputs of the neural network are averaged, on a weighted basis, for each different window position and separately for each horizontal pixel position in the field. The resulting averaged activation output values, typically in the form of a Gaussian distribution for each character, are then filtered, thresholded and then used, via a weighted average calculation with horizontal pixel positions being used as the weights, to determine the character \"heart\" position as being the center pixel position in the distribution.", "Neural network based character position detector for use in optical character recognition "]
["The present invention provides systems and methods for prosthesis fitting in joints that employ a trained neural network to predict at least one unknown set of data, such as position and contact force. The unknown data is predicted based on at least one known sensor value that is obtained intraoperatively. The predicted neural network data is made available to a physician and aids in the determination of whether to resect additional bone, release soft tissues, and/or select sizes for prosthetic components. Advantageously, increased data may be provided to a physician without the need to acquire numerous samples from a patient, and fewer sensors may be employed.", "Application of neural networks to prosthesis fitting and balancing in joints "]
["A method of managing the processing of information using a first neural network, the information relating to the transmission of messages in a telecommunications network, uses the steps of:", "Monitoring and retraining neural network "]
["A two-layer network according to the present invention is comprised of a first-layer array of electrically-adaptable synaptic elements, inter-layer connection circuitry comprised of electrically adaptable elements, and a second-layer array of electrically-adaptable synaptic elements. Electrons may be placed onto and removed from a floating node associated with at least one MOS transistor in each electrically adaptable element, usually comprising the gate of the transistor, in an analog manner, by application of first and second electrical control signals. A first electrical control signal controls the injection of electrons onto the floating node from an electron injection structure and the second electrical control signal controls the removal of electrons from the floating node by an electron removal structure. Each synaptic element in the synaptic array comprises an adaptable CMOS inverter or other amplifier circuit. The inputs to all first-layer synaptic elements in a row are connected to a common row input line. Adapt inputs to all synaptic elements in a column are connected together to a common column adapt line. The outputs of all first layer synaptic elements in a column are connected to a common sense amplifier on a sense line. The outputs of the sense amplifiers are connected to the inputs of the synaptic elements of the second layer of the array. The outputs of all synaptic elements in a given row in the second layer of the array are connected to a common row output line. In order to adapt the synaptic elements in the array, the voltages to which the synaptic elements in a given column of the first layer of the array is to be adapted are placed onto the input voltage lines, and the synaptic elements in column n are then simultaneously adapted by assertion of an adapt signal on the adapt line for the column. The voltages to which the synaptic elements of the second layer of the array are to be adapted are placed on the row outputs lines.", "Two layer neural network comprised of neurons with improved input range and input offset "]
["A system for diagnosing medical conditions, such as low back pain (LBP), is provided, whereby a neural network is trained by presentation of large amounts of clinical data and diagnostic outcomes. Following training, the system is able to produce the diagnosis from the clinical data. While the present invention may be useful in diagnosing LBP in one embodiment, other applications of the present invention, both in the medical field and in other fields, are also envisioned. This intelligent diagnostic system is less expensive and more accurate than conventional diagnostic methods, and has the unique capability to improve its accuracy over time as more data is analyzed.", "System for diagnosing medical conditions using a neural network "]
["An automated screening system and method for cytological specimen classification in which a neural network is utilized in performance of the classification function. Also included is an automated microscope and associated image processing circuitry.", "Neural network based automated cytological specimen classification system and method "]
["A computer operated apparatus estimates values needed by an optimizer in a database management system (DBMS). The DBMS has one or more tables for storing data, each table having zero or more columns of user-definable data types and zero or more associated user-defined routines (UDRs). The apparatus has a feature vector extractor connected to the database tables for converting the UDR inputs into a base type representation. A neural network receives the feature vector and generates estimated values which are provided to the optimizer of the DBMS. The neural network can be trained periodically using randomly generated queries, or it can be trained dynamically by capturing data generated during a query. During operation, the optimizer dynamically invokes the neural network to generate estimates such as selectivity and cost per call for determining optimum query search sequence.", "Optimizer with neural network estimator "]
["A system and a method for tracking long term performance of a vibrating body such as a gas turbine, includes a vibration sensor who time domain outputs are transformed to the frequency domain, using a fast Fourier transform processing. Frequency domain outputs are provided as inputs to a fuzzy adaptive resonance theory neural network. Outputs from the network can be coupled to an expert system for analysis, to display devices for presentation to an operator or are available for other control and information purposes.", "Neural network based analysis system for vibration analysis and condition monitoring "]
["A multi-kernel neural network computing architecture configured to learn correlations among feature values 34, 38 as the network monitors and imputes measured input values 30 and also predicts future output values 46. This computing architecture, referred to as a concurrent-learning information processor (CIP 10), includes a multi-kernel neural network array 14 with the capability to learn and predict in real time. The CIP 10 also includes a manager 16 and an input-output transducer 12 that may be used for input-output refinement. These components allow the computing capacity of the multi-kernel array 14 to be reassigned in response to measured performance or other factors. The output feature values 46 computed by the multi-kernel array 14 and processed by an output processor 44 of the transducer 12 are supplied to a response unit 18 that may be configured to perform a variety of monitoring, forecasting, and control operations in response to the computed output values. Important characteristics of the CIP 10, such as feature function specifications 35 and 49, connection specifications 42, learning weight schedules 55, and the like may be set by a technician through a graphical user interface 20. Refinement processes also allow the CIP 10 be reconfigured in accordance with user commands for application to different physical applications.", "Multi-kernel neural network concurrent learning, monitoring, and forecasting system "]
["An interface for a neural network includes a generalized data translator and a certainty filter in the data path including the neural network for rendering a decision on raw data, possibly from a data processing application. The data translator is controlled with user-definable parameters and procedures contained in a property list in order to manipulate translation, truncation, mapping (including weighting) and other transformations of the raw data. The neuron to which the output of the data translator is applied is controlled by a code index contained in an action list. An external certainty threshold is also provided, preferably by the action list to filter the output of the neural network. The core program used with the ConExNS neurons for system maintenance also includes further core operations and size maintenance operations responsive to commands from the user of an application to cause operations to be performed with in the neural network as well as to create and update the property and action lists.", "Enhanced interface for a neural network engine "]
["An adaptive control system uses a neural network to provide adaptive control when the plant is operating within a normal operating range, but shifts to other types of control as the plant operating conditions move outside of the normal operating range. The controller uses a structure which allows the neural network parameters to be determined from minimal information about plant structure and the neural network is trained on-line during normal plant operation. The resulting system can be proven to be stable over all possible conditions. Further, with the inventive techniques, the tracking accuracy can be controlled by appropriate network design.", "Stable adaptive neural network controller "]
["Character images which are to be sent to a neural network trained to recognize a predetermined set of symbols are first processed by an optical character recognition pre-processor which normalizes the character images. The output of the neural network is processed by an optical character recognition post-processor. The post-processor corrects erroneous symbol identifications made by the neural network. The post-processor identifies special symbols and symbol cases not identifiable by the neural network following character normalization. For characters identified by the neural network with low scores, the post-processor attempts to find and separate adjacent characters which are kerned and characters which are touching. The touching characters are separated in one of nine successively initiated processes depending upon the geometric parameters of the image. When all else fails, the post-processor selects either the second or third highest scoring symbol identified by the neural network based upon the likelihood of the second or third highest scoring symbol being confused with the highest scoring symbol.", "Optical character recognition neural network system for machine-printed characters "]
["The apparatus for the recognition of speech includes an acoustic preprocessor, a visual preprocessor, and a speech classifier that operates on the acoustic and visual preprocessed data. The acoustic preprocessor comprises a log mel spectrum analyzer that produces an equal mel bandwidth log power spectrum. The visual processor detects the motion of a set of fiducial markers on the speaker's face and extracts a set of normalized distance vectors describing lip and mouth movement. The speech classifier uses a multilevel time-delay neural network operating on the preprocessed acoustic and visual data to form an output probability distribution that indicates the probability of each candidate utterance having been spoken, based on the acoustic and visual data. The training system includes the speech recognition apparatus and a control processor with an associated memory. Noisy acoustic input training data together with visual data is used to generate acoustic and visual feature training vectors for processing by the speech classifier. A control computer adjusts the synaptic weights of the speech classifier based upon the noisy input training data and exemplar output vectors for producing a robustly trained classifier based on the analogous visual counterpart of the Lombard effect.", "Neural network acoustic and visual speech recognition system training method and apparatus "]
["Neural network algorithms have impressively demonstrated the capability of modelling spatial information. On the other hand, the application of parallel distributed models to processing of temporal data has been severely restricted. The invention introduces a novel technique which adds the dimension of time to the well known back-propagatio", "Neural network for processing both spatial and temporal data with time based back-propagation "]
["A data processing system and method for solving pattern classification problems and function-fitting problems includes a neural network in which N-dimensional input vectors are augmented with at least one element to form an N+j-dimensional projected input vector, whose magnitude is then preferably normalized to lie on the surface of a hypersphere. Weight vectors of at least a lowest intermediate layer of network nodes are preferably also constrained to lie on the N+j-dimensional surface.", "Rapidly converging projective neural network "]
["A fault diagnostics system for monitoring the operating condition of a host system, e.g., an aircraft, which includes a plurality of subsystems. The fault diagnostics system is preferably implemented in software running on a high-speed neural network processor. The fault diagnostics system constructs a neural network model of the performance of each subsystem in a normal operating mode and each of a plurality of different possible failure modes. The system preferably dynamically predicts the performance of each subsystem based upon the response of each of the neural network models to dynamically changing operating conditions, compares the actual performance of each subsystem with the dynamically predicted performance thereof in each of the normal and possible failure modes, and determines the operating condition of the host system on the basis of these comparisons. In a preferred embodiment, the determining step is carried out by performing a statistical analysis of the comparisons made in the comparing step, e.g., by emloying a comparison voting technique. A related method is also disclosed.", "Neural network fault diagnostics systems and related method "]
["Detecting harmful or illegal intrusions into a computer network or into restricted portions of a computer network uses a process of synthesizing anomalous data to be used in training a neural network-based model for use in a computer network intrusion detection system. Anomalous data for artificially creating a set of features reflecting anomalous behavior for a particular activity is performed. This is done in conjunction with the creation of normal-behavior feature values. A distribution of users of normal feature values and an expected distribution of users of anomalous feature values are then defined in the form of histograms. The anomalous-feature histogram is then sampled to produce anomalous-behavior feature values. These values are then used to train a model having a neural network training algorithm where the model is used in the computer network intrusion detection system. The model is trained such that it can efficiently recognize anomalous behavior by users in a dynamic computing environment where user behavior can change frequently.", "Method and apparatus for training a neural network model for use in computer network intrusion detection "]
["A digital neural network architecture including a forward cascade of layers of neurons, having one input channel and one output channel, for forward processing of data examples that include many data packets. Backward cascade of layers of neurons, having one input channel and one output channel, for backward propagation learning of errors of the processed data examples. Each packet being of a given size. The forward cascade is adapted to be fed, through the input channel, with a succession of data examples and to deliver a succession of partially and fully processed data examples each consisting of a plurality of packets. The fully processed data examples are delivered through the one output channel. Each one of the layers is adapted to receive as input in its input channel a first number of data packets per time unit and to deliver as output in its output channel a second number of data packets per time unit. The forward cascade of layers is inter-connected to the backward cascade of layers by means that include inter-layer structure, such that, during processing phase of the forward cascade of neurons, any given data example that is fed from a given layer in the forward cascade to a corresponding layer in the backward cascade, through the means, is synchronized with the error of the given processed data example that is fed to the corresponding layer from a preceding layer in the backward cascade. The first number of data packets and the second number of data packets being the same for all the layers.", "Digital hardware architecture for realizing neural network "]
["A digital artificial neural network (ANN) reduces memory requirements by storing sample transfer function representing output values for multiple nodes. Each nodes receives an input value representing the information to be processed by the network. Additionally, the node determines threshold values indicative of boundaries for application of the sample transfer function for the node. From the input value received, the node generates an intermediate value. Based on the threshold values and the intermediate value, the node determines an output value in accordance with the sample transfer function.", "Method and apparatus for processing data in a neural network "]
["An image processor employing a camera, frame grabber and a new algorithm for detecting straight edges in optical images is disclosed. The algorithm is based on using a self-organizing unsupervised neural network learning to classify pixels on a digitized image and then extract the corresponding line parameters. The image processor is demonstrated on the specific application of edge detection for linewidth measurement in semiconductor lithography. The results are compared to results obtained by a standard straight edge detector based on the Radon transform; good consistency is observed; however, superior speed is achieved for the proposed image processor. The results obtained by the proposed approach are also shown to be in agreement with Scanning Electron Microscope (SEM) measurements, which is known to have excellent accuracy but is an invasive measurement instrument. The method can thus be used for on-line measurement and control of microlithography processes and for alignment tasks as well.", "Method of edge detection in optical images using neural network classifier "]
["A fuzzy data comparator receives a fuzzy data digital data bit stream and compares each frame thereof with multiple sets of differing known data stored in a plurality of pattern memories, using a selected comparison metric. The results of the comparisons are accumulated as error values. A first neural postprocessing network ranks error values less than a preselected threshold. A second neural network receives the first neural network solutions and provides an expansion bus for interconnecting to additional comparators.", "Fuzzy data comparator with neural network postprocessor "]
["Methods are developed on a digital computer for performing work order scheduling activity in a dynamic factory floor environment, in a manner which enables scheduling heuristic knowledge from a scheduler to be encoded through an adaptive learning process, thus eliminating the need to define these rules explicitly. A sequential assignment paradigm incrementally builds up a final schedule from a partial schedule, assigning each work order to appropriate resources in turns, taking advantage of the parallel processing capability of neural networks by selecting the most appropriate resource combination (i.e. schedule generation) for each work order under simultaneous interaction of multiple scheduling constraints.", "Neural network system and method for factory floor scheduling "]
["A network is provided for the detection and correction of local boundary misalignments in a two-dimensional pixel space between a reference and transformed image. An input layer has input layer sections, each of which contains a plurality of input nodes associated with a cell. The cell is centered on a pixel and divided along a straightline orientation into first and second cell sections. Each of the input nodes outputs a digital signal indicative of the presence or absence of a contrast gradient as measured by the two cell sections. A second layer has a plurality of second layer sections, each of which is associated with one of the input layer sections and contains a plurality of second layer nodes. Each second layer node is responsive to a combination of input nodes to indicate the presence or absence of a boundary misalignment between the reference and transformed images. Presence of a contrast gradient at the combination of nodes defines a local boundary misalignment. A third layer has a plurality of third layer nodes, each of which is associated with one of the second layer sections. Each third layer node weights and combines outputs of the second layer nodes to output a signal defining a direction to shift the transformed image perpendicular to the straightline orientation. The third layer are outputs a signal defining the local correction of the local boundary misalignment between the reference and transformed images for the centered pixel.", "Neural network for detection and correction of local boundary misalignments between images "]
["A recurrent, neural network-based fuzzy logic system includes in a rule base layer and a membership function layer neurons which each have a recurrent architecture with an output-to-input feedback path including a time delay element and a neural weight. Further included is a recurrent, neural network-based fuzzy logic rule generator wherein a neural network receives and fuzzifies input data and provides data corresponding to fuzzy logic membership functions and recurrent fuzzy logic rules.", "Recurrent neural network-based fuzzy logic system and method "]
["Feedback control of a process to reduce process variations is advantageously accomplished by the combination of a signal processor (26) and an artificial neural network (27). The signal processor (26) first determines which of a plurality of process outputs has the greatest deviation from a corresponding desired value for that output. Having determined which of the process outputs has the greatest deviation from its corresponding desired value, the process controller (25) then adjusts the output having the greatest deviation to yield an estimated process output vector Tm n supplied to the artificial neural network (27) trained to represent an inverse model of the process. In response to the estimated process output vector Tm n, the artificial neural network (27) generates a process control vector cn that controls the process in accordance with the first order variation between the actual process output and a desired value therefor to reduce process variations.", "Feedback process control using a neural network parameter estimator "]
["A neural network radar processor (10) comprises a multilayer perceptron neural network (100.1) comprising an input layer (102), a second layer (122), and at least a third layer (124), wherein each layer has a plurality of nodes (108), and respective subsets of nodes (108) of the second (122) and third (124) layers are interconnected so as to form mutually exclusive subnetworks (120). In-phase and quadrature phase time series from a sampled down-converted FMCW radar signal (19) are applied to the input layer, and the neural network (100) is trained so that the nodes of the output layer (106) are responsive to targets in corresponding range cells, and different subnetworks (120) are responsive to respectively different non-overlapping sets of target ranges. The neural network is trained with signals that are germane to an FMCW radar, including a wide range of target scenarios as well as leakage signals, DC bias signals, and background clutter signals.", "Neural network radar processor "]
["The present invention is predicated upon the fact that an emission trace from a plasma glow used in fabricating integrated circuits contains information about phenoma which cause variations in the fabrication process such as age of the plasma reactor, densities of the wafers exposed to the plasma, chemistry of the plasma, and concentration of the remaining material. In accordance with the present invention, a method for using neural networks to determine plasma etch end-point times in an integrated circuit fabrication process is disclosed. The end-point time is based on in-situ monitoring of the optical emission trace. The back-propagation method is used to train the network. More generally, a neural network can be used to regulate control variables and materials in a manufacturing process to yield an output product with desired quality attributes. An identified process signature which reflects the relation between the quality attribute and the process may be used to train the neural network.", "Active neural network control of wafer attributes in a plasma etch process "]
["A self-organizing neural network having input and output neurons mutually coupled via bottom-up and top-down adaptive weight matrics performs pattern recognition while using substantially fewer neurons and being substantially immune from pattern distortion or rotation. The network is first trained in accordance with the adaptive resonance theory by inputting reference pattern data into the input neurons for clustering within the output neurons. The input neurons then receive subject pattern data which are transferred via a bottom-up adaptive weight matrix to a set of output neurons. Vigilance testing is performed and multiple computed vigilance parameters are generated. A predetermined, but selectively variable, reference vigilance parameter is compared individually against each computed vigilance parameter and adjusted with each comparison until each computed vigilance parameter equals or exceeds the adjusted reference vigilance parameter, thereby producing an adjusted reference vigilance parameter for each output neuron. The input pattern is classified according to the output neuron corresponding to the maximum adjusted reference vigilance parameter. Alternatively, the original computed vigilance parameters can be used by classifying the input pattern according to the output neuron corresponding to the maximum computer vigilance parameter.", "Neural network apparatus and method for pattern recognition "]
["A type of neural network called a self-organizing map (SOM) is useful in pattern classification. The ability of the SOM to map the density of the input distribution is improved with two techniques. In the first technique, the SOM is improved by monitoring the frequency for which each node is the winning node, and splitting frequently winning nodes into two nodes, while eliminating infrequently winning nodes. Topological order is preserved by inserting a link between the preceding and following nodes so that such preceding and following nodes are now adjacent in the output index space. In the second technique, the SOM is trained by applying a weight correction to each node based on the frequencies of that node and its neighbors. If any of the adjacent nodes have a frequency greater than the frequency of the present node, then the weight vector of the present node is adjusted towards the highest-frequency neighboring node. The topological order of the nodes is preserved because the weight vector is moved along a line of connection from the present node to the highest-frequency adjacent node. This second technique is suitable for mapping to an index space of any dimension, while the first technique is practical only for a one-dimensional output space.", "Neural network for classification of patterns with improved method and apparatus for ordering vectors "]
["A closed loop neural network based autotuner develops optimized proportional, integral and/or derivative parameters based on the outputs of other elements in the loop. Adjustments are initiated by making a step change in the setpoint which may be done by a user or automatically. A Smith predictor may also be employed.", "Closed loop neural network automatic tuner "]
["A multi-layered neural network is disclosed that converts an incoming temporally coded spike train into a spatially distributed topographical map from which interspike-interval and bandwidth information may be extracted. This neural network may be used to decode multiplexed pulse-coded signals embedded serially in an incoming spike train into parallel distributed topographically mapped channels. A signal processing and code conversion algorithm not requiring learning is provided.", "Interspike interval decoding neural network "]
["A neural network unit is described which has a plurality of neurons. The network comprises a RAM, which provides a plurality of storage locations for each of the neurons and an integrated circuit. The integrated circuit including means for defining an algorithm for the operation of the neurons and a control unit for causing the neurons to produce outputs on the basis of data stored in the storage locations and the algorithm. The integrated circuit may have a random number generator and a comparator. In effect, the neurons are virtual pRAMs (probabilistic RAMs).", "Neural network architecture with connection pointers "]
["An analog neural network element includes one or more EEPROMs as analog, reprogrammable synapses applying weighted inputs to positive and negative term outputs which are combined in a comparator. In one embodiment a pair of EEPROMs is used in each synaptic connection to separately drive the positive and negative term outputs. In another embodiment, a single EEPROM is used as a programmable current source to control the operation of a differential amplifier driving the positive and negative term outputs. In a still further embodiment, an MNOS memory transistor replaces the EEPROM or EEPROMs. These memory elements have limited retention or endurance which is used to simulate forgetfulness to emulate human brain function. Multiple elements are combinable on a single chip to form neural net building blocks which are then combinable to form massively parallel neural nets.", "Neural network elements "]
["A method of evolving a neural network that includes a plurality of processing elements interconnected by a plurality of weighted connections includes the step of obtaining a definition for the neural network by evolving a plurality of weights for the plurality of weighted connections, and evolving a plurality of activation function parameters associated with the plurality of processing elements. Another step of the method includes determining whether the definition for the neural network may be simplified based upon at least one activation function parameter of the plurality of activation function parameters. Yet another step of the method includes updating the definition for the neural network in response to determining that the definition for the neural network may be simplified. The method utilizes particle swarm optimization techniques to evolve the plurality of weights and the plurality of activation parameters. Moreover, the method simplifies activation functions of processing elements in response to corresponding activation parameters meeting certain criteria, and removes processing elements from the definition of the neural network in response to corresponding activation parameters satisfying certain criteria. Various apparatus are also disclosed for implementing network evolution and simplification.", "Method and apparatus for evolving a neural network "]
["An apparatus for retrieving signals embedded in noise and analyzing the signals. The apparatus includes an input device for receiving input signals having noise. At least one noise filter retrieves data signals embedded in the input signals. At least one adaptive pattern recognition filter generates coefficients of a polynomial expansion representing the pattern of the filtered data signals. A storage device stores the coefficients generated. It is determined when an event has occurred, the event being located at any position within the data signals. An adaptive autoregressive moving average pattern recognition filter generates coefficients of a polynomial expansion representing an enhanced pattern of filtered data signals. At least one weighting filter compares the stored patterns with the enhanced pattern of data signals.", "Neural network method and apparatus for retrieving signals embedded in noise and analyzing the retrieved signals "]
["A method for detecting a departure from normal operation of an electric motor comprises obtaining a set of normal current measurements for a motor being monitored; forming clusters of the normal current measurements; training a neural network auto-associator using the set of normal current measurements; making current measurements for the motor in operation; comparing the input and output of the auto-associator; and indicating abnormal operation whenever the current measurements deviate more than a predetermined amount from the normal current measurements. The method models a set of normal current measurements for the motor being monitored, and indicates a potential failure whenever measurements from the motor deviate significantly from a model. The model takes the form of an neural network auto-associator which is \"trained\"--using clusters of current measurements collected while the motor is known to be in a normal operating condition--to reproduce the inputs on the output. A new set of FFT's of current measurements are classified as \"good\" or \"bad\" by first transforming the measurement using a Fast Fourier Transform (FFT) and an internal scaling procedure, and then applying a subset of the transformed measurements as inputs to the neural network auto-associator. A decision is generated based on the difference between the input and output of the network.", "Radial basis function neural network autoassociator and method for induction motor monitoring "]
["The present invention relates to methods and systems for devising and implementing automated artificial neural networks to predict market performance and direction movements of the U.S. Treasury market, mortgage option-adjusted spreads (OAS), interest rate swap spreads, and U.S. Dollar/Mexican Peso exchange rate. The methods and systems of the present invention employ techniques used in actual neural networks naturally occurring in biological organisms to develop artificial neural network models for predicting movements in the financial market that are capable of extracting in a very consistent fashion non-linear relationships among input variables of the models that are readily apparent to the human traders.", "Method and system for artificial neural networks to predict price movements in the financial markets "]
["A vector neural network (VNN) of interconnected neurons is provided in transition mappings of potential targets wherein the threshold (energy) of a single frame does not provide adequate information (energy) to declare a target position. The VNN enhances the signal-to-noise ratio (SNR) by integrating target energy over multiple frames including the steps of postulating massive numbers of target tracks (the hypotheses), propagating these target tracks over multiple frames, and accommodating different velocity target by pixel quantization. The VNN then defers thresholding to subsequent target stages when higher SNR's are prevalent so that the loss of target information is minimized, and the VNN can declare both target location and velocity. The VNN can further include target maneuver detection by a process of energy balancing hypotheses.", "Vector neural network for low signal-to-noise ratio detection of a target "]
["An information processing system and method to calculate output values for a group of neurons. The method comprises transmitting input values for the neurons to a memory unit of a processing section, and then calculating a multitude of series of neuron output values over a multitude of cycles. During a first period of each cycle, a first series of neuron output values are calculated from neuron input values stored in a first memory area of the memory unit; and during a second period of each cycle, a second series of neuron output values are calculated from neuron input values stored in a second memory area of the memory unit. The transmitting step includes the steps of storing in the first memory area of the memory unit, neuron input values transmitted to the memory unit during the period immediately preceding the first period of each cycle; and storing in the second memory area of the memory unit neuron input values transmitted to the memory unit, during the first period of each cycle.", "Neural network with memory cycling "]
["A cell employing floating gate storage device particularly suited for neural networks. The floating gate from the floating gate device extends to and becomes part of a second, field effect device. Current through the second device is affected by the charge on the floating gate. The weighting factor for the cell is determined by the amount of charge on the floating gate. By charging the floating gate to various levels, a continuum of weighting factors is obtained. Multiplication is obtained since the current through the second device is a function of the weighting factor.", "Semiconductor cell for neural network and the like "]
["A signal processing system and method for accomplishing signal processing using a neural network that incorporates adaptive weight updating and adaptive pruning for tracking non-stationary signal is presented. The method updates the structural parameters of the neural network in principal component space (eigenspace) for every new available input sample. The non-stationary signal is recursively transformed into a matrix of eigenvectors with a corresponding matrix of eigenvalues. The method applies principal component pruning consisting of deleting the eigenmodes corresponding to the smallest saliencies, where a sum of the smallest saliencies is less than a predefined threshold level. Removing eigenmodes with low saliencies reduces the effective number of parameters and generally improves generalization. The output is then computed by using the remaining eigenmodes and the weights of the neural network are updated using adaptive filtering techniques.", "Method and system for training a neural network with adaptive weight updating and adaptive pruning in principal component space "]
["A neural network is used in a system to detect abnormalities in cells, including cancer in bladder tissue cells. The system has an image analysis system for generating data representative of imaging variables from an image of stained cells. The set of data is provided to a neural network which has been trained to detect abnormalities from known tissue cells with respect to the data from the same set of imaging variables. A conventional sigmoid-activated neural network, or alternatively, a hybrid neural network having a combination of sigmoid, gaussian and sinusoidal activation functions may be utilized. The trained neural network applies a set of weight factors obtained during training to the data to classify the unknown tissue cell as normal or abnormal.", "Neural network for cell image analysis for identification of abnormal cells "]
["A method and a system for causing a neural circuit model to learn typical past control results of a process and using the neural circuit model for supporting an operation of the process. The neural circuit model is caused to learn by using, as input signals, a typical pattern of values of input variables at different points in time and, as a teacher signal, its corresponding values of the control variable. An unlearned pattern of input variables is inputted to the thus-learned neuron circuit model, whereby a corresponding value of the control variable is determined. Preferably, plural patterns at given time intervals can be simultaneously used as patterns to be learned.", "Supporting neural network method for process operation "]
["A classification neural network for piecewise linearly separating an input space to classify input patterns is described. The multilayered neural network comprises an input node, a plurality of difference nodes in a first layer, a minimum node, a plurality of perceptron nodes in a second layer and an output node. In operation, the input node broadcasts the input pattern to all of the difference nodes. The difference nodes, along with the minimum node, identify in which vornoi cell of the piecewise linear separation the input pattern lies. The difference node defining the vornoi cell localizes input pattern to a local coordinate space and sends it to a corresponding perceptron, which produces a class designator for the input pattern.", "Facet classification neural network "]
["A Neural Network using interconnecting weights each with two values, one of which is selected for use, can be taught to map a set of input vectors to a set of output vectors.", "Discrete weight neural network "]
["A low-rate voice coding method and apparatus uses vocoder-embedded neural network techniques. A neural network controlled speech analysis processor includes a neural network which manages speech characterization, encoding , decoding, and reconstruction methodologies. The voice coding method and apparatus uses multi-layer perceptron (MLP) based neural network structures in single or multi-stage arrangements.", "Method and apparatus for encoding speech using neural network technology for speech classification "]
["Improved speaker independent speech recognition system and method are disclosed in which an utterance by an unspecified person into an electrical signal is input through a device such as a telephone, the electrical signal from the input telephone converting the electrical signal into a time series of characteristic multidimensional vectors, the time series of characteristic multidimensional vectors are received, each of the vectors being converted into a plurality of candidates so that the plurality of phonemes constitutes a plurality of strings of phonemes in time series as a plurality of candidates, the plurality of candidates of phonemes are compared simultaneously (one at a time) with a reference pattern of a reference string of phonemes for each word previously stored in a dictionary to determine which string of phonemes derived from the phoneme recognition means has a highest similarity to one of the reference strings of the phonemes for the respective words stored in the dictionary using a predetermined word matching technique, and at least one candidate of the words as a result of word recognition on the basis of one of the plurality of the strings of phonemes which has the highest similarity to the corresponding one of the reference strings of the respective words is output as the result of speech recognition.", "Speaker independent speech recognition system and method using neural network and DTW matching technique "]
["An adaptive hierarchical neural network based system with online adaptation capabilities has been developed to automatically adjust the display window width and center for MR images. Our windowing system possesses the online training capabilities that make the adaptation of the optimal display parameters to personal preference as well as different viewing conditions possible. The online adaptation capabilities are primarily due to the use of the hierarchical neural networks and the development of a new width/center mapping system. The large training image set is hierarchically organized for efficient user interaction and effective re-mapping of the width/center settings in the training data set. The width/center values are modified in the training data through a width/center mapping function, which is estimated from the new width/center values of some representative images adjusted by the user. The width/center mapping process consists of a global spline mapping for the entire training images as well as a first-order polynomial sequence mapping for the image sequences selected in the user's new adjustment procedure.", "Neural network based auto-windowing system for MR images "]
["A computerized method and system using a shift-invariant artificial neural network (SIANN) for the quantitative analysis of image data. A series of digitized medical images are used to train an artificial neural network to differentiate between diseased and normal tissue. The sum of the weights in groups between layers is constrained to be substantially zero so as to avoid saturation of layers which would otherwise be saturated by low frequency background noise. The method and system also include utilizing training-free zones to exclude from training the center portions of microcalcifications in the digitized images. The method and system further include rule-based selection criteria for providing a more accurate diagnosis.", "Shift-invariant artificial neural network for computerized detection of clustered microcalcifications in mammography "]
["An autonomous navigation system for a mobile vehicle arranged to move within an environment includes a plurality of sensors arranged on the vehicle and at least one neural network including an input layer coupled to the sensors, a hidden layer coupled to the input layer, and an output layer coupled to the hidden layer. The neural network produces output signals representing respective positions of the vehicle, such as the X coordinate, the Y coordinate, and the angular orientation of the vehicle. A plurality of patch locations within the environment are used to train the neural networks to produce the correct outputs in response to the distances sensed.", "Autonomous navigation apparatus with neural network for a mobile vehicle "]
["Neural net with spatially distributed functionalities. An information processing system comprises a neural net with fully distributed neuron and synapse functionalities in a spatially inhomogeneous medium to propagate a response field from an input to an output. The response field is a reaction of the medium to a plurality of input signals and depends non-linearly on the input signals. The response field is also determined by the inhomogeneities. The value of the field at one or more particular locations is indicative of one or more output signals of the neural net.", "Neural network using inhomogeneities in a medium as neurons and transmitting input signals in an unchannelled wave pattern through the medium "]
["Neural network apparatus and methods for implementing reinforcement learning. In one implementation, the neural network is a spiking neural network, and the apparatus and methods may be used for example to enable an adaptive signal processing system to effect focused exploration by associative adaptation, including providing a negative reward signal to the network, which may increase excitability of the neurons in combination with decrease in excitability of active neurons. In certain implementations, the increase is gradual and of smaller magnitude, compared to the excitability decrease. In some implementations, the increase/decrease of the neuron excitability is effectuated by increasing/decreasing an efficacy of the respective synaptic connections delivering presynaptic inputs into the neuron. The focused exploration may be achieved for instance by non-associative potentiation configured based at least on the input spike rate. The non-associative potentiation may further comprise depression of connections that provide input in excess of a desired limit.", "Apparatus and methods for reinforcement learning in artificial neural networks "]
["In a method for tranining a neural network with the non-deterministic behavior of a technical system, weightings for the neurons of the neural network are set during the training using a cost function. The cost function evaluates a beneficial system behavior of the technical system to be modeled, and thereby intensifies or increases the weighting settings which contribute to the beneficial system behavior, and attenuates or minimizes weightings which produce a non-beneficial behavior. Arbitrary or random disturbances are generated by disturbing the manipulated variable with noise having a known noise distribution, these random disturbances significantly faciliating the mathematical processing of the weightings which are set, because the terms required for that purpose are simplified. The correct weighting setting for the neural network is thus found on the basis of a statistical method and the application of a cost function to the values emitted by the technical system or its model.", "Method for training a neural network with the non-deterministic behavior of a technical system "]
["A method for detecting a departure from normal operation of an electric motor comprises obtaining a set of normal current measurements for a motor being monitored; training a neural network auto-associator using the set of normal current measurements; making current measurements for the motor in operation; comparing the current measurements with the normal current measurements; and indicating abnormal operation whenever the current measurements deviate more than a predetermined amount from the normal current measurements. The method models a set of normal current measurements for the motor being monitored, and indicates a potential failure whenever measurements from the motor deviate significantly from a model. The model takes the form of an neural network auto-associator which is \"trained\"--using current measurements collected while the motor is known to be in a normal operating condition--to reproduce the inputs on the output. A new set of current measurements are classified as \"good\" or \"bad\" by first transforming the measurement using a Fast Fourier Transform (FFT) and an internal scaling procedure, and then applying a subset of the transformed measurements as inputs to the neural network auto-associator. A decision is generated based on the difference between the input and output of the network.", "Neural network auto-associator and method for induction motor monitoring "]
["A method and a device for recognition of isolated words in large vocabularies are described, wherein recognition is performed through two sequential steps using neural networks and Markov models techniques, respectively, and the results of both techniques are adequately combined so as to improve recognition accuracy. The devices performing the combination also provide an evaluation of recognition reliability.", "Method of and a device for speech recognition employing neural network and markov model recognition techniques "]
[null, "Method of processing information in neural networks "]
["An air/fuel ratio control apparatus for executing auxiliary control of an air/fuel ratio by compensating an injected fuel amount set by a control system for maintaining the air/fuel ratio at a preset value. The air/fuel ratio control apparatus includes a state detecting unit for detecting a plurality of physical values which can be measured at low temperature and which show a state of an engine, an air/fuel ratio estimating unit for receiving a plurality of physical values detected by the state detecting means as input parameters and for estimating the air/fuel ratio using a neural network, and a compensatory fuel amount calculating unit for calculating a compensatory fuel amount for the injected fuel amount from the estimated air/fuel ratio. Here, low temperature refers to a temperature at which an air/fuel sensor cannot operate.", "Air/fuel ratio control apparatus that uses a neural network "]
["A sequence processor for rapidly learning, recognizing and recalling temporal sequences. The processor, called the Katamic system, is a biologically inspired artificial neural network based on a model of the functions of the cerebellum in the brain. The Katamic system utilizes three basic types of neuron-like elements with different functional characteristics called predictrons, recognitrons and bi-stable switches. The Katamic System is clock operated, processing input sequences pattern by pattern to produce an output pattern which is a prediction of the next pattern in the input sequence. The Katamic System learns rapidly, has a large memory capacity, exhibits sequence completion and sequence recognition capability, and is fault and noise tolerant. The system's modular construction permits straightforward scaleability.", "Neural network for learning, recognition and recall of pattern sequences "]
["A four-layer neural network is trained with data of midinfrared absorption by nerve and blister agent compounds (and simulants of this chemical group) in a standoff detection application. Known infrared absorption spectra by these analyte compounds and their computed first derivative are scaled and then transformed into binary or decimal arrays for network training by a backward-error-propagation (BEP) algorithm with gradient descent paradigm. The neural network transfer function gain and learning rate are adjusted on occasion per training session so that a global minimum in final epoch convergence is attained. Three successful neural network filters have been built around an architecture design containing: (1) an input layer of 350 neurons, one neuron per absorption intensity spanning 700\u2266\u03bd\u22661400 wavenumbers with resolution \u0394\u03bd=2; (2) two hidden layers in 256- and 128-neuron groups, respectively, providing good training convergence and adaptable for downloading to a configured group of neural IC chips; and (3) an output layer of one neuron per analyte--each analyte defined by a singular vector in the training data set. Such a neural network is preferably implemented with a network of known microprocessor chips.", "Neural network computing system for pattern recognition of thermoluminescence signature spectra and chemical defense "]
["A neural network based universal time series prediction system for financial securities includes a pipelined recurrent ANN architecutre having a plurality of identical modules to first adjust internal weights and biases in response to a first training set representing a nonlinear financial time series of samples of a financial quantity and a target value, and then determine and store an estimated prediction error of the ANN in order to adjust short time stock price predictions in accordance with the stored prediction error. The prediction system is also designed to output upper and lower prediction bounds within a confidence region.", "Artificial neural network based universal time series "]
["Classification procedure implemented in a tree-like neural network which, in the course of learning steps, determines with the aid of a tree-like structure the number of neurons and their synaptic coefficients required for the processing of problems of classification of multi-class examples. Each neuron tends to distinguish, from the examples, two groups of examples approximating as well as possible to a division into two predetermined groups of classes. This division can be obtained through a principal component analysis of the distribution of examples. The neural network comprises a directory of addresses of successor neurons which is loaded in learning mode then read in exploitation mode. A memory stores example classes associated with the ends of the branches of the tree.", "Classification procedure implemented in a hierarchical neural network, and hierarchical neural network "]
["A method and apparatus for speeding and enhancing the \"learning\" function of a computer configured as a multilayered, feed format artificial neural network using logistic functions as an activation function. The enhanced learning method provides a linear probing method for determining local minima values computed first along the gradient of the weight space and then adjusting the slope and direction of a linear probe line after determining the likelihood that a \"ravine\" has been encountered in the terrain of the weight space.", "Method and apparatus for learning in a neural network "]
["An artificial neural network is provided using a modular, self-organizing approach wherein a separate neural field is contained within each module for recognition and synthesis of particular characteristics of respective input and output signals thereby allowing several of these modules to be interconnected to perform a variety of operations. The first output and second input of one module is respectively coupled to the first input and second output of a second module allowing each module to perform a bi-directional transformation of the information content of the first and second input signals for creating first and second output signals having different levels of information content with respect thereto. In the upward direction, the first low-level input signal of each module is systematically delayed to create a temporal spatial vector from which a lower frequency, high-level first output signal is provided symbolic of the incoming information content. Since the first output signal contains the same relevant information as the first input signal while operating at a lower frequency, the information content of the latter is said to be compressed into a first high-level output signal. In the downward direction, a second output signal having a low-level of information content is synthesized from a second input signal having a high-level of information content. The second input signal is the best prediction of the first output signal available from the knowledge base of the module, while similarly the second output signal is the prediction of the first input signal.", "Spann: sequence processing artificial neural network "]
["A method and apparatus are disclosed for implementing a neural network having a sleep mode during which capacitively stored synaptic connectivity weights are refreshed. Each neuron outputs an analog activity level, represented in a preferred embodiment by the frequency of digital pulses. Feed-forward synaptic connection circuits couple the activity level outputs of first level neurons to inputs of second level neurons, and feed-back synaptic connection circuits couple outputs of second level neurons to inputs of first level neurons, the coupling being weighted according to connectivity weights stored on respective storage capacitors in each synaptic connection circuit. The network learns according to a learning algorithm under which the connections in both directions between a particular first level neuron and a particular second level neuron are strengthened to the extent of concurrence of high activity levels in both the first and second level neurons, and weakened to the extent of concurrence of a high activity level in the second level neuron and a low activity level in the first level neuron. The network is put to sleep by disconnecting all environmental inputs and providing a non-specific low activity level signal to each of the first level neurons. This causes the network to randomly traverse its state space with low intensity resonant firings, each state being visited with a probability responsive to the initial connectivity weights of the connections which abut the second level neuron representing such state. Refresh is accomplished since the learning algorithm remains active during sleep. Thus, the sleep refresh mechanism enhances the contrast in the connectivity terrain and strengthens connections that would otherwise wash out due to lack of visitation while the system is awake. A deep sleep mechanism is also provided for preventing runaway strengthening of favored states, and also to encourage Weber Law compliance.", "Sleep refreshed memory for neural network "]
["A method for adapting a decision directed adaptive neural network (10). The method finds the best matches between a plurality of input data vectors (16) and an associated plurality of input portion of weight vectors. The input portion of the weight vectors are adapted. The identification codes (12) which represent the sequence of best matched weight vectors are stored in a memory (12) and the associated output portion of weight vectors (22) are output. A sequence of output portion of weight vectors (22) is matched with predetermined models (21). A sequence of labels (24) associated with the best matched model is stored which identifies the categories of match data. The labels (24) are sequentially combined with the identification codes (12) to build adaptation vectors. The adaptation vectors are then used to sequentially adapt the output portion of weight vectors (22).", "Decision directed adaptive neural network "]
["An information processing apparatus using a neural network learning function has, in one embodiment, a computer system and a pattern recognition apparatus associated with each other via a communication cable. The computer system includes a learning section having a first neural network and serves to adjust the weights of connection therein as a result of learning with a learning data signal supplied thereto from the pattern recognition apparatus via the communication cable. The pattern recognition apparatus includes an associative output section having a second neural network and receives data on the adjusted weights from the learning section via the communication cable to reconstruct the second neural network with the data on the adjusted weights. The pattern recognition apparatus with the associative output section having the reconstructed second neural network performs pattern recognition independently of the computer system with the communication cable being brought into an electrical isolation mode.", "Information processing system using neural network learning function "]
["Method for obtaining information about an occupying item in a space in a vehicle in accordance with the invention includes obtaining images of an area above a seat in the vehicle in which the occupying item is situated and classifying the occupying item by inputting signals derived from the images into a trained neural network form which is trained to output an indication of the class of occupying item from one of a predetermined number of possible classes. The method is applicable for various vehicles including automobiles, trucks, buses, airplanes and boats. The images may be pre-processed to remove background portions of the images and then converted into signals for input into the neural network form.", "Neural network systems for vehicles "]
["A recurrent, neural network-based fuzzy logic system includes neurons in a rule base layer which each have a recurrent architecture with an output-to-input feedback path including a time delay element and a neural weight. Further included is a neural network-based, fuzzy logic finite state machine wherein the neural network-based, fuzzy logic system has a recurrent architecture with an output-to-input feedback path including at least a time delay element. Still further included is a recurrent, neural network-based fuzzy logic rule generator wherein a neural network receives and fuzzifies input data and provides data corresponding to fuzzy logic membership functions and recurrent fuzzy logic rules.", "Recurrent neural network-based fuzzy logic system "]
["A neuron device network is provided with a speech input layer, a context layer, a hidden layer, a speech output layer and a hypothesis layer. A phoneme to be learned is spectral-analyzed by an FFT unit and a vector row at a time point t is input to a speech input layer. Also, a vector state of the hidden layer at a time t-1 is input to the context layer, the vector row at a time t+1 is input to the speech output layer as an instructor signal, and a code row for hypothesizing the phoneme, or the code row, is input to the hypothesis layer. The time series relation of the vector rows and the phoneme are hypothetically learned. Alternatively, a spectrum, a cepstrum or a speech vector row based on outputs from the hidden layer of an auto-associative neural network is input to the speech input layer, and the code row is output from the hypothesis layer, taking into account the time series relation. The speech is recognized when a CPU reads the stored output values of the hidden layer and the connection weights of the hidden layer and the hypothesis layer from a memory of the neuron device network and calculates output values of the respective neuron devices of the hypothesis layer based on the output values and the connection weights. The corresponding phoneme is determined by collating the output values of the respective neuron devices of the hypothesis layer with the code rows in an instructor signal table.", "Neural network, a method of learning of a neural network and phoneme recognition apparatus utilizing a neural network "]
["A plant controller using reinforcement learning for controlling a plant includes action and critic networks with enhanced learning for generating a plant control signal. Learning is enhanced within the action network by using a neural network configured to operate according to unsupervised learning techniques based upon a Kohonen Feature Map. Learning is enhanced within the critic network by using a distance parameter which represents the difference between the actual and desired states of the quantitative performance, or output, of the plant when generating the reinforcement signal for the action network.", "Intelligent controller with neural network and reinforcement learning "]
["Pattern recognition, for instance optical character recognition, is achieved by training a neural network, scanning an image, segmenting the image to detect a pattern, preprocessing the detected pattern, and applying the preprocessed detected pattern to the trained neural network. The preprocessing includes determining a centroid of the pattern and centrally positioning the centroid in a frame containing the pattern. The training of the neural network includes randomly displacing template patterns within frames before applying the template patterns to the neural network.", "Training a neural network using centroid dithering by randomly displacing a template "]
["An electronic engine control (EEC) module executes both open loop and closed loop neural network processes to control the air/fuel mixture ratio of a vehicle engine to hold the fuel mixture at stoichiometry. The open loop neural network provides transient air/fuel control to provide a base stoichiometric air/fuel mixture ratio signal in response to throttle position under current engine speed and load conditions. The base air/fuel mixture ratio signal from the open loop network is additively combined with a closed loop trimming signal which varies the air/fuel mixture ratio in response to variations in the sensed exhaust gas oxygen level. Each neural network function is defined by a unitary data structure which defines the network architecture, including the number of node layers, the number of nodes per layer, and the interconnections between nodes. In addition, the data structure holds weight values which determine the manner in which network signals are combined. The network definition data structures are created by a network training system which utilizes an external training processor which employs gradient methods to derive network weight values in accordance with a cost function which quantitatively defines system objectives and an identification network which is pretrained to provide gradient signals representative of the behavior of the physical plant. The training processor executes training cycles asynchronously with the operation of the EEC module in a representative test vehicle.", "Trained Neural network air/fuel control system "]
["The overall invention categorizes patients with suspected acute myocardial infarction (AMI) with regard to a) AMI/non-AMI; b) infarct size (e.g. Major/Minor); c) time since onset of infarction; and d) non-AMI with/without minor myocardial damage (MMD). Generally, the above categorization is based on frequent timed blood sampling and measurement of selected biochemical markers of AMI with different rates of appearance in circulating blood. The computations are performed by using specially designed artificial neural networks. According to a first main aspect of the invention, early, i.e. generally within 3 hours from admission of the patient, detection/exclusion of acute myocardial infarction is provided. Furthermore, early prediction of the infarct size and early estimation of the time from onset are also provided.", "Detection/exclusion of acute myocardial infarction using neural network analysis of measurements of biochemical markers "]
["A method for correcting an output from an electrochemical cell sensor, having an output responsive to a concentration of sensed species in a stream, a pressure and a temperature in an interactive non-linear relationship, comprising the steps of providing a pressure sensor for producing an output responsive to a pressure in proximity to the electrochemical sensor; providing a temperature sensor for producing an output responsive to a temperature of said electrochemical sensor; and processing the outputs of the electrochemical sensor, pressure sensor and temperature sensor in a neural network having an output function which compensates said electrochemical sensor for changes in pressure and temperature to indicate a concentration of the sensed species. An apparatus is also provided, for compensating an electrochemical sensing apparatus, comprising an electrochemical sensor, being responsive to a sensed species, and an environmental variable; an environmental variable sensor, being responsive to said environmental variable; and a compensation network for producing a compensated output based on an output of said electrochemical sensor based and an output of said environmental variable sensor, said network comprising a neural network.", "Neural network compensation for sensors "]
[null, "Training convolutional neural network on graphics processing units "]
["In a method for classifying IEGM waveforms in dependence of the workload of a patient, a predetermined number of IEGM-signals, each signal extending over at least one segment of one heart beat cycle, are registered, whereafter the IEGM-signals are fed to a neural network and an encoded form of the signals is formed using said neural network. This encoded form is stored in a memory, for use in classifying further registered IEGM-signals. In an active cardiac implant connectable to an implantable electrode arrangement adapted for in vivo delivery of stimulation pulses to a heart, IEGM signals present are obtained from one or more of the electrodes. A pulse generator connected to the electrode arrangement, generates and emits stimulation pulses with a variable stimulation interval between successive stimulation pulses. The implant also contains a classifying device for classification of a predetermined number of IEGM-signals registered during predetermined time intervals at predetermined points of time according to a predetermined classification stored in the classification device, this classification being related to preregistered waveforms of measured IEGM-signals. A control unit in the implant supplies a control signal to a control input of the pulse generator dependent on the classification of each of the registered IEGM signals. The control signal causes the pulse generator to adjust the stimulation rate dependent on each of the registered IEGM-signals.", "Rate responsive heart stimulation device using neural network and IEGM classifier "]
["A speech recognition apparatus has a speech input unit for inputting a speech; a speech analysis unit for analyzing the inputted speech to output the time series of a feature vector; a candidates selection unit for inputting the time series of a feature vector from the speech analysis unit to select a plurality of candidates of recognition result from the speech categories; and a discrimination processing unit for discriminating the selected candidates to obtain a final recognition result. The discrimination processing unit includes three components in the form of a pair generation unit for generating all of the two combinations of the n-number of candidates selected by said candidate selection unit, a pair discrimination unit for discriminating which of the candidates of the combinations is more certain for each of all n C2 -number of combinations (or pairs) on the basis of the extracted result of the acoustic feature intrinsic to each of said candidate speeches, and a final decision unit for collecting all the pair discrimination results obtained from the pair discrimination unit for each of all the n C2 -number of combinations (or pairs) to decide the final result. The pair discrimination unit handles the extracted result of the acoustic feature intrinsic to each of the candidate speeches as fuzzy information and accomplishes the discrimination processing on the basis of fuzzy logic algorithms, and the final decision unit accomplishes its collections on the basis of the fuzzy logic algorithms.", "Speech recognition apparatus using neural network and fuzzy logic "]
["This application discloses hardware suitable for use in a neural network system. It makes use of Z-technology modules, each containing densely packaged electronic circuitry. The modules provide access planes which are electrically connected to circuitry located on planar surfaces interfacing with such access planes. One such planar surface comprises a resistive feedback network. By combining two Z-technology modules, whose stacked chips are in planes perpendicular to one another, and using switching networks between the two modules, the system provides bidirectional accessibility of each individual electronic element in the neural network to most or all of the other individual electronic elements in the system.", "Hardware for electronic neural network "]
["A neural network comprising an input layer, two hidden layers for generating an number of outcome class component values, and an output layer for classifying input vectors to an outcome class, under the assumption that the outcome classes are characterized by mixtures of component populations with each component population having a multivariate Gaussian likelihood distribution. The first hidden layer includes a number of first layer nodes each connected receive input vector components from the input layer and generates in response a first layer output value representing the absolute value of the sum of a function of the difference between each input vector component and a threshold value. The second hidden layer includes a plurality of second layer nodes each for generating an outcome class component value, each second layer node being connected to predetermined ones of the first layer nodes and generating in response to the first layer output values an outcome class component value representing a function related to the exponential of the negative square of the sum of first layer output values connected thereto. The output layer includes a plurality of output layer nodes each associated with an outcome class. Each output layer node uses the output class component values from the second layer nodes in combination with weighting values to generate the likelihood that the input vector is properly classified to the output layer node's outcome class.", "Neural network for maximum likelihood classification with supervised and unsupervised training capability "]
["A semiconductor neural network includes a coupling matrix having coupling elements arranged in a matrix which couple with specific coupling strengths internal data input lines to internal data output lines. The internal data output lines are divided into groups. The neural network further comprises weighting addition circuits provided corresponding to the groups of the internal data output lines. A weighting addition circuit includes weighing elements for adding weights to signals on the internal data output lines in the corresponding group and outputting the weighted signals, and an addition circuit for outputting a total sum of the outputs of those weighting elements. The internal data output lines are arranged to form pairs and the addition circuit has a first input terminal for receiving one weighting element output of each of the pairs in common, a second input terminal for receiving the other weighting element output of each of the pairs in common, and a sense amplifier for differentially amplifying signals at the first and second input terminals. The neural network further includes a circuit for detecting a change time of an input signal, a circuit responsive to an input signal change for equalizing the first and second input terminals for a predetermined period, and a circuit for activating the sense amplifier after the equalization is completed. The information retention capability of each coupling element is set according to the weight of an associated weighting element. This neural network can provide multi-valued expression of coupling strength with less number of coupling elements.", "Semiconductor neural network and operating method thereof "]
["The apparatus for the recognition of speech comprises an acoustic preprocessor, a visual preprocessor, and a speech classifier that operates on the acoustic and visual preprocessed data. The acoustic preprocessor comprises a log mel spectrum analyzer that produces an equal mel bandwidth log power spectrum. The visual processor detects the motion of a set of fiducial markers on the speaker's face and extracts a set of normalized distance vectors describing lip and mouth movement. The speech classifier uses a multilevel time-delay neural network operating on the preprocessed acoustic and visual data to form an output probability distribution that indicates the probability of each candidate utterance having been spoken, based on the acoustic and visual data.", "Neural network acoustic and visual speech recognition system "]
["A method and apparatus for training neural networks using evolutionary programming. A network is adjusted to operate in a weighted configuration defined by a set of weight values and a plurality of training patterns are input to the network to generate evaluations of the training patterns as network outputs. Each evaluation is compared to a desired output to obtain a corresponding error. From all of the errors, an overall error value corresponding to the set of weight values is determined. The above steps are repeated with different weighted configurations to obtain a plurality of overall error values. Then, for each set of weight values, a score is determined by selecting error comparison values from a predetermined variable probability distribution and comparing them to the corresponding overall error value. A predetermined number of the sets of weight values determined to have the best scores are selected and copies are made. The copies are mutated by adding random numbers to their weights and the above steps are repeated with the best sets and the mutated copies defining the weighted configurations. This procedure is repeated until the overall error values diminish to below an acceptable threshold. The random numbers added to the weight values of copies are obtained from a continuous random distribution of numbers having zero mean and variance determined such that it would be expected to converge to zero as the different sets of weight values in successive iterations converge toward sets of weight values yielding the desired neural network performance.", "Method and apparatus for training a neural network using evolutionary programming "]
["A noise reduction system used for transmission and/or recognition of speech includes a speech analyzer for analyzing a noisy speech input signal thereby converting the speech signal into feature vectors such as autocorrelation coefficients, and a neural network for receiving the feature vectors of the noisy speech signal as its input. The neural network extracts from a codebook an index of prototype vectors corresponding to a noise-free equivalent to the noisy speech input signal. Feature vectors of speech are read out from the codebook on the basis of the index delivered as an output from the neural network, thereby causing the speech input to be reproduced on the basis of the feature vectors of speech read out from the codebook.", "Noise reduction system using neural network "]
["A traffic volume estimating apparatus 1A estimates the traffic volumes of traffic apparatus, and a traffic flow presuming apparatus 1B presumes the traffic flows generating the estimated traffic volumes. A presumption function constructing apparatus 1C corrects the presumption functions of the traffic flow presuming apparatus 1B on actually measured traffic volumes, traffic flow presumption results and control results. A control result detecting apparatus 1G detects the control results and the drive results of the traffic apparatus. Further, a control parameter setting apparatus 1D sets control parameters on traffic flow presumption results, and corrects the control parameters according to the control results and the drive results.", "Transportation system traffic controlling system using a neural network "]
["A physical neural network is disclosed, which comprises a liquid state machine. The physical neural network is configured from molecular connections located within a dielectric solvent between pre-synaptic and post-synaptic electrodes thereof, such that the molecular connections are strengthened or weakened according to an application of an electric field or a frequency thereof to provide physical neural network connections thereof. A supervised learning mechanism is associated with the liquid state machine, whereby connections strengths of the molecular connections are determined by pre-synaptic and post-synaptic activity respectively associated with the pre-synaptic and post-synaptic electrodes, wherein the liquid state machine comprises a dynamic fading memory mechanism.", "Physical neural network liquid state machine utilizing nanotechnology "]
["A system for classifying vehicles based on the sound waved produced by the vehicles receives analog sound pressure levels and converts them to a power spectrum. Fuzzification functions, such as asymmetric wedge shaped functions, are convoluted with the power spectrum to create a vector that characterizes the power spectrum while reducing the dimensionality of the characterizing vector. A neural network analyzes the characterizing vector and produces a classification designator indicative of the class of the object associated with the analog sound pressure levels received by the system.", "Vehicle classification system using a passive audio input to a neural network "]
["A system and method of image processing using neural networks to control image processing elements. Neural network parameters are defined by genotypes consisting of network vectors. Genotypes may be selectively mutated and cross-bred to provide a mechanism for modifying the behavior of the neural networks, or phenotypes. Genetic modeling processes are used to perform such mutation and cross-over. User feedback concerning output images, is used to select particular genotypes for further mutation and exploration. Preconditioning is employed to extract structural information from source images prior to network processing. Genetic morphing and subnet fusion are also available, to provide additional variations on image processing operations.", "Image processing using genetic mutation of neural network parameters "]
["An analog neural network composed of an array of capacitors for storing weighted electric charges. Electric charges, or voltages, on the capacitors control the impedance (resistance) values of a corresponding plurality of MOSFETs which selectively couple input signals to one input of a summing amplifier. A plurality of semiconductor gating elements (e.g. MOSFETs) selectively couple to the capacitor's weighted analog voltage values received serially over an input line. The weighted voltage on the input line are periodically applied to the proper capacitors in the neural network via the gating elements so as to refresh the weighted electric charges on the capacitors, and at a multiplex rate that maintains the voltages on the capacitors within acceptable tolerance levels.", "Neural network with dynamic refresh capability "]
["An apparatus, article and method containing an artificial neural network that, after training, produces new trainable nodes such that input data representative of a first event and input data representative of a second event both activate a subset of the new trainable nodes. The artificial neural network can generate an output that is influenced by the input data of both events. In various embodiments, the new trainable nodes are sequentially produced and show decreasing trainability over time such that, at a particular point in time, newer produced nodes are more trainable than earlier produced nodes. The artificial neural network can be included in various embodiments of methods, apparatus and articles for use in predicting or profiling events.", "Temporally dynamic artificial neural networks "]
["A computer network-based customer acquisition server and method of selecting preferred products includes using a neural network-based decision engine that automatically generate queries and select preferred products as a function of responses to the queries.", "Neural network based decision processor and method "]
["A neural network system includes an input unit, an operation control unit, a parameter setting unit, a neural network group unit, and a display unit. The network group unit includes first and second neural networks. The first neural network operates according to the mean field approximation method to which the annealing is added, whereas the second neural network operates in accordance with the simulated annealing. Each of the first an second neural networks includes a plurality of neurons each connected via synapses to neurons so as to weighting outputs from the neurons based on synapse weights, thereby computing an output related to a total of weighted outputs from the neurons according to an output function. The parameter setting unit is responsive to a setting instruction to generate neuron parameters including synapse weights, threshold values, and output functions, which are set to the first neural network and which are selective set to the second neural network. The operation control unit responsive to an input of a problem analyzes the problem and then generates a setting instruction based on a result of the analysis to output the result to the parameter setting unit. After the neuron parameters are set thereto, in order for the first and second neural network to selectively or to iteratively operate, the operation control unit controls operations of computations in the network group unit in accordance with the analysis result and then presents results of the computations in the network group unit on the display unit.", "Neural network system for determining optimal solution "]
["An unsupervised back propagation method for training neural networks. For a set of inputs, target outputs are assigned l's and O's randomly or arbitrarily for a small number of outputs. The learning process is initiated and the convergence of outputs towards targets is monitored. At intervals, the learning is paused, and the values for those targets for the outputs which are converging at a less than average rate, are changed (e.g., 0\u21921, or 1\u21920), and the learning is then resumed with the new targets. The process is continuously iterated and the outputs converge on a stable classification, thereby providing unsupervised back propagation. In a further embodiment, samples classified with the trained network may serve as the training sets for additional subdivisions to grow additional layers of a hierarchical classification tree which converges to indivisible branch tips. After training is completed, such a tree may be used to classify new unlabelled samples with high efficiency. In yet another embodiment, the unsupervised back propagation method of the present invention may be adapted to classify fuzzy sets.", "Unsupervised neural network classification with back propagation "]
["An inner product computing unit computes inner products of an input pattern whose category is unknown, and orthogonalized dictionary sets of a plurality of reference patterns whose categories are known. A nonlinear converting unit nonlinearly converts the inner products in accordance with a positive-negative symmetrical nonlinear function. A neural network unit or a statistical discriminant function computing unit performs predetermined computations of the nonlinearly converted values on the basis of preset coefficients in units of categories using a neural network or a statistical discriminant function. A determining section compares values calculated in units of categories using the preset coefficients with each other to discriminate a category to which the input pattern belongs.", "Pattern recognition system and method using neural network "]
["Disclosed is a system and a method for combining the computational resources of numerous embedded devices to enable any of them to perform complex tasks like speech recognition or natural language understanding. A distinguished master device communicates with a network of embedded devices, and organizes them as the nodes of a neural network. To each node (embedded device) in the neural network, the master device sends the activation function for that node and the connectivity pattern for that node. The master device sends the inputs for the network to the distinguished input nodes of the network. During computation, each node computes the activation function of all of its inputs and sends its activation to all the nodes to which it needs to send output to. The outputs of the neural network are sent to the master device. Thus, the network of embedded devices can perform any computation (like speech recognition, natural language understanding, etc.) which can be mapped onto a neural network model.", "Method and apparatus for executing neural network applications on a network of embedded devices "]
["A waveform equalizer for equalizing a distorted signal, contains a sampling unit, a time series generating unit, and an equalization neural network unit. The sampling unit samples the level of a distorted signal at a predetermined rate. The time series generating unit serially receives the sampled level and outputs in parallel a predetermined number of the levels which have been last received. The equalization neural network unit receives the outputs of the time series generating unit, and generates an equalized signal of the distorted signal based on the outputs of the time series generating unit using a set of equalization network weights which are preset therein. The waveform equalizer may further contain a distortion characteristic detecting unit, an equalization network weight holding unit, and a selector unit. The distortion characteristic detecting unit detects a distortion characteristic of the distorted signal. The equalization network weight holding unit holds a plurality of sets of equalization network weights each for being set in the equalization neural network unit. The selector unit selects one of the plurality of sets of equalization network weights according to the distortion characteristic which is detected in the distortion characteristic detecting unit, and supplies the selected set in the equalization neural network unit to set the selected set therein.", "Waveform equalizer using a neural network "]
["A neural network learning system in which an input-output relationship is inferred. The system includes a probability density part for determining a probability density on a sum space of an input space and an output space from a set of given input and output samples by learning, the probability density on the sum space being defined to have a parameter, and an inference part for inferring a probability density function based on the probability density from the probability density part, so that an input-output relationship of the samples is inferred from the probability density function having a parameter value determined by learning, the learning of the parameter being repeated until the value of a predefined parameter differential function using a prescribed maximum likelihood method is smaller than a prescribed reference value.", "Neural network learning system inferring an input-output relationship from a set of given input and output samples "]
["The invention is directed to means, utilizing a neural network, for estimating helicopter airspeed at speeds below about 50 knots using only fixed system parameters as inputs to the neural network. The system includes: means for entering at least one initial parameter; means for measuring, in a nonrotating reference frame associated with the helicopter, a plurality of variable state parameters generated during flight of the helicopter; means for determining a plurality of input parameters based on the at least one initial parameter and the plurality of variable state parameters and for generating successive signals representing the input parameters; at least one equation representing a nonlinear input-output relationship between the input parameters and airspeed; memory means for storing the at least one equation and for successively receiving and storing signals from the determining means; and processing means responsive to signals received from the memory means for generating airspeed information based on the input parameters and the at least one equation.", "Neural network based helicopter low airspeed indicator "]
["A signal processing apparatus and concomitant method for learning and using fidelity metric as a control mechanism and to process large quantities of fidelity metrics from a visual discrimination measure (VDM) to a manageable subjective image quality ratings. The signal processing apparatus incorporates a VDM and a neural network. The VDM receives input image sequences and generates fidelity metrics, which are received by a neural network. The neural network is trained to learn and use the fidelity metrics as a control mechanism, e.g., to control a video encoder.", "Method and apparatus for training a neural network to learn and use fidelity metric as a control mechanism "]
["A novel associative network architecture is described in which a neural network is subdivided into a plurality of smaller blocks. Each block comprises an array of pattern matching cells which is used for calculating the relative match, or Hamming distance, between an input pattern and a stored weight pattern. The cells are arranged in columns along one or more local summing lines. The total current flowing along the local summing lines for a given block corresponds to the match for that block. Each of the blocks are coupled together using a plurality of global summing lines. The global summing lines sum the individual current contributions from the local summing lines of each associated block. Coupling between the local column lines and the global summing lines is achieved by using a specialized coupling device which permits control of the coupling ratio between the lines. By selectively turning on or off various blocks a measure of the match for individual blocks or for groups of blocks representing a subset of the network, may be calculated. Control over the coupling ratio within the blocks also prevents destructive levels of current from building up on the global summing lines.", "Neural network employing leveled summing scheme with blocked array "]
["A neural network based optical character recognition technique is presented for identifying characters in a moving web. Image acquisition means defines an imaging window through which the moving web passes such that the characters printed thereon can be imaged. Classification data is extracted and accumulated for each printed web character passing through the imaging window. A light source provides transmissive illumination of the web as it is being imaged. A neural network accelerator is coupled to the image acquisition means for intelligent processing of the accumulated classification data to produce therefrom printed character classification information indicative of each corresponding character imaged. A processor is coupled to the accelerator for converting the classification information into the appropriate ASCII character code. The technique is particularly useful for reading dot-matrix-type characters on a noisy, semi-transparent background at fast real-time rates. A neural network algorithm based recognition method is also described.", "Neural network optical character recognition system and method for classifying characters in a moving web "]
["A neural network comprises an input port connected to an output port by one or more paths, each of which comprises an alternating series of weights and neurons. The weights amplify passing signals by a strength factor. The network can be trained by finding a set of strength factor values for the weights such that the network produces the correct output pattern from a given input pattern. During training, a strength factor perturbating and refresh means applies perturbations to the strength factors of weights in the network, and updates the values of the strength factors depending on the difference between signals appearing at the output port, for a given pair of input and training patterns, when the weight is perturbed, and when it is not.", "Neural network with training by perturbation "]
["A plurality of neural circuits are connected in a neural network layer for generating their respective digital axonal responses to the same plurality of synapse input signals. Each neural circuit includes digital circuitry for approximating a sigmoidal response connected after respective circuitry for performing a weighted summation of the synapse input signals to generate a weighted summation result in digital form. In this digital circuitry the absolute value of the digital weighted summation result is first determined. Then, a window comparator determines into which of a plurality of amplitude ranges the absolute value of the weighted summation result falls. A digital intercept value and a digital slope value are selected in accordance with the range into which the absolute value of the weighted summation result falls. The absolute value of the digital weighted summation result is multiplied by the selected digital slope value to generate a digital product; and the digital intercept value is added to the digital product to generate an absolute value representation of a digital axonal response. The polarity of the weighted summation result is determined, and the same polarity is assigned to the absolute value representation of the digital axonal response, thereby to generate the digital axonal response.", "Digital circuitry for approximating sigmoidal response in a neural network layer "]
["A system for controlling the output of a rolling mill. An intelligent control system is part of a control loop between the mill and a PID controller. The control loop does not rely on the output of an exit gauge sensor in normal operation. The intelligent control system can be an artificial neural network or a parallel cascade network, and has an output node for generating an output signal that is predictive of the exit gauge at a future time. A comparator coupled to the artificial neural network output signal and to a reference signal derives an error signal which is fed to the PID controller for modulating the metal thickness.", "Predictive control of rolling mills using neural network gauge estimation "]
["A neural network includes a plurality of input nodes for receiving the respective elements of the input vector. A copy of all of the elements of the input vector is sent to the next level of nodes in the neural network denoted as intermediate nodes. The intermediate nodes each encode a separate template pattern. They compare the actual input pattern with the template and generate a signal indicative of the difference between the input pattern and the template pattern. Each of the templates encoded in the intermediate nodes has a class associated with it. The difference calculated by the intermediate nodes is passed to an output node for each of the intermediate nodes at a given class. The output node then selects the minimum difference amongst the values sent from the intermediate nodes. This lowest difference for the class represented by the output node is then forwarded to a selector. The selector receives such values from each of the output nodes of all of the classes and then selects that to output value which is a minimum difference. The selector in turn, generates a signal indicative of the class of the intermediate node that sent the smallest difference value.", "Self-organizing neural network for pattern classification "]
["An extension directed integrated circuit device having a learning function on a Boltzmann model, includes a plurality of synapse representing units arrayed in a matrix to form a rectangle including a first and second triangles on a semiconductor chip, a plurality of neuron representing units and a plurality of educator signal control circuits which are arranged along first and second sides of the rectangle, and a plurality of buffer circuits arranged along third and fourth sides of the rectangle. The first side is opposite to the third side, and the second side is opposite to the fourth side. Axon signal transfer lines and dendrite signal lines are so arranged that the neuron representing units are full-connected in each of the first right triangle the second right triangle. Alternatively, axon signal lines and dendrite signal ines are arranged in parallel with rows and columns of the synapse representing unit matrix, so that the neuron representing units are full-connected in the rectangle. Each synapse representing unit is connected to a pair of axon signal transfer lines and a pair of dendrite signal transfer lines.", "Neural network integrated circuit device having self-organizing function "]
["Design of a neural network for automatic detection of incidents on a freeway is described. A neural network is trained using a combination of both back-propagation and genetic algorithm-based methods for optimizing the design of the neural network. The back-propagation and genetic algorithm work together in a collaborative manner in the neural network design. The training starts with incremental learning based on the instantaneous error and the global total error is accumulated for batch updating at the end of the training data being presented to the neural network. The genetic algorithm directly evaluates the performance of multiple sets of neural networks in parallel and then use the analyzed results to breed new neural networks that tend to be better suited to the problems at hand.", "Automatic freeway incident detection system and method using artificial neural network and genetic algorithms "]
["A signal processing apparatus for controlling an object includes an input unit, a neural network, an output unit, a teaching unit, and an error signal generator for generating a teaching signal that makes the neural network learn in real time. An error signal generator generates an error signal from the teaching signal and information contained in the network output signal. The error signal controls the neural network so that the control output signal has correct control information with respect to the output signal from the controlled object.", "Signal processing apparatus having at least one neural network "]
["A system for building an artificial neural network is provided which precisely defines the network's structure of artificial neurons, and non-iteratively determines the synapse-weights and hard limiter threshold of each artificial neuron of the network. The system includes a computer for analyzing input data, which represents patterns of different classes of signals, to generate one or more data points in two or three dimensions representative of the signals in each of the different classes. A distribution of the data points is visualized on a map on an output device coupled to the computer. The data points are clustered on the map into clusters in accordance with the classes associated with the data points, and the map is then partitioned into regions by defining linear boundaries between clusters. The artificial neural network is configured in accordance with the data points, clusters, boundaries, and regions, such that each boundary represents a different artificial neuron of the artificial neural network, and the geometric relationship of the regions on the map to the classes defines the logic connectivity of the artificial neurons. The synaptic weights and threshold of each artificial neuron in the network are graphically determined based on the data points of the map.", "System for building an artificial neural network "]
["A method for rapid and sensitive protein family identification is disclosed. The new designs include an n-gram term weighting algorithm for extracting local motif patterns, an enhanced n-gram method for extracting residues of long-range correlation, and integrated neural networks for combining global and motif sequence information.", "Neural network system with N-gram term weighting method for molecular sequence classification and motif identification "]
["A system and method for recognizing an utterance of a speech in which each reference pattern stored in a dictionary is constituted by a series of phonemes of a word to be recognized, each phoneme having a predetermined length of continued time and having a series of frames and a lattice point (i, j) of an i-th number phoneme at an j-th number frame having a discriminating score derived from Neural Networks for the corresponding phoneme. When the series of phonemes recognized by a phoneme recognition block is compared with each reference pattern, one i of the input series of phonemes recognized by the phoneme recognition block being calculated as a matching score as gk(i, j); ##EQU1## wherein ak(i, j) denotes an output score value of the Neural Networks of the j-th number phoneme at the j-th number frame of the reference pattern and p denoted a penalty constant to avoid an extreme shrinkage of the phonemes, a total matching score is calculated as gk (I, J), I denoting the number of frames of the input series of phonemes and J denoting the number of phonemes of the reference pattern k, and one of the reference patterns which gives a maximum matching score is output as the word recognition.", "Speaker independent speech recognition system and method using neural network and/or DP matching technique "]
["A random access memory (RAM) circuit is provided wherein an input signal matrix forming an identifiable original pattern is learned and stored such that a distorted facsimile thereof may be applied to generate an output signal matrix forming a replication of the original pattern having improved recognizable features over the distorted facsimile. The input signal matrix is logically divided into a plurality of predetermined subsets comprising a unique element of the input signal matrix and the elements in the neighborhood thereof. Each predetermined subset is quantized into a first digital address and applied at the address inputs of a memory circuit for retrieving data stored in the addressed memory location, while one signal of the predetermined subset is digitized and weighted and combined with the data retrieved from the addressed memory location for storage in the same addressed memory location. Next, a plurality of second digital addresses is generated including predetermined combinations of the first digital address perturbed at least one bit and sequentially applied at the address inputs of the memory circuit whereby the steps of digitizing and weighting one signal of the predetermined subset of the input signal matrix, combining the digitized and weighted signal with the data retrieved from the addressed memory location, and storing the combination back into the addressed memory location are repeated for the second digital addresses.", "Neuram: neural network with ram "]
["A system for distinguishing between a target and clutter analyzes frequency components of returned wave energy by one or more networks each having inputs receiving successive samples of the returned energy and having outputs individually connected to the inputs through multiplier elements providing selectable factors. The multipliers corresponding to each output are connected to the output through a summing element and a selectable and generally sigmoidal activation function. The factors may be bandpass filter coefficients or discrete Fourier transform coefficients so as to generate frequency components of the energy. Predetermined frequency characteristics of the returned energy may be detected by providing the outputs of a network to a network in which the factors are selected as correlation or convolution coefficients, are selected to integrate fed back outputs, or are selected to sum several outputs within a predetermined range. The activation functions may be selected for thresholding, linearity, limiting, or generation of logarithms.", "Radar target discrimination systems using artificial neural network topology "]
["A neural network for comparing a known input to an unknown input comprises a first layer for receiving a first known input tensor and a first unknown input tensor. A second layer receives the first known and unknown input tensors. The second layer has at least one first trainable weight tensor associated with the first known input tensor and at least one second trainable weight tensor associated with the first unknown input tensor. The second layer includes at least one first processing element for transforming the first known input tensor on the first trainable weight tensor to produce a first known output and at least one second processing element for transforming the first unknown input tensor on the second trainable weight tensor to produce a first unknown output. The first known output comprises a first known output tensor of at least rank zero and has a third trainable weight tensor associated therewith. The first unknown output comprises a first unknown output tensor of at least rank zero and has a fourth trainable weight tensor associated therewith. The first known output tensor and the first unknown tensor are combined to form a second input tensor. A third layer receives the second input tensor. The third layer has at least one fifth trainable weight tensor associated with the second input tensor. The third layer includes at least one third processing element for transforming the second input tensor on the fifth trainable weight tensor, thereby comparing the first known output with the first unknown output and producing a resultant output. The resultant output is indicative of the degree of similarity between the first known input tensor and the first unknown input tensor.", "Object recognition system employing a sparse comparison neural network "]
["Methods and systems for modifying at least one synapse of a physicallelectromechanical neural network. A physical/electromechanical neural network implemented as an adaptive neural network can be provided, which includes one or more neurons and one or more synapses thereof, wherein the neurons and synapses are formed from a plurality of nanoparticles disposed within a dielectric solution in association with one or more pre-synaptic electrodes and one or more post-synaptic electrodes and an applied electric field. At least one pulse can be generated from one or more of the neurons to one or more of the pre-synaptic electrodes of a succeeding neuron and one or more post-synaptic electrodes of one or more of the neurons of the physical/electromechanical neural network, thereby strengthening at least one nanoparticle of a plurality of nanoparticles disposed within the dielectric solution and at least one synapse thereof.", "Adaptive neural network utilizing nanotechnology-based components "]
["A neural network processor for solving first-order competitive assignment problems consists of a matrix of N\u00d7M processing units, each of which corresponds to the pairing of a first number of elements of {Ri } with a second number of elements {Cj }, wherein limits of the first number are programmed in row control superneurons, and limits of the second number are programmed in column superneurons as MIN and MAX values. The cost (weight) Wij of the pairings is programmed separately into each PU. For each row and column of PUs, a dedicated constraint superneuron insures that the number of active neurons within the associated row or column fall within a specified range. Annealing is provided by gradually increasing the PU gain for each row and column or increasing positive feedback to each PU, the latter being effective to increase hysteresis of each PU or by combining both of these techniques.", "Neural-network dedicated processor for solving assignment problems "]
["A new method to analyze and predict the binding energy for enzyme-transition state inhibitor interactions is presented. Computational neural networks are employed to discovery quantum mechanical features of transition states and putative inhibitors necessary for binding. The method is able to generate its own relationship between the quantum mechanical structure of the inhibitor and the strength of binding. Feed-forward neural networks with back propagation of error can be trained to recognize the quantum mechanical electrostatic potential at the entire van der Waals surface, rather than a collapsed representation, of a group of training inhibitors and to predict the strength of interactions between the enzyme and a group of novel inhibitors. The experimental results show that the neural networks can predict with quantitative accuracy the binding strength of new inhibitors. The method is in fact able to predict the large binding free energy of the transition state, when trained with less tightly bound inhibitors. The present method is also applicable to prediction of the binding free energy of a ligand to a receptor. The application of this approach to the study of transition state inhibitors and ligands would permit evaluation of chemical libraries of potential inhibitory, agonistic, or antagonistic agents. The method is amenable to incorporation in a computer-readable medium accessible by general-purpose computers.", "Neural network methods to predict enzyme inhibitor or receptor ligand potency "]
["State-dependent supervised learning framework in artificial neuron networks may be implemented. A framework may be used to describe plasticity updates of neuron connections based on a connection state term and a neuron state term. Connection states may be updated based on inputs and outputs to and/or from neurons. The input connections of a neuron may be updated using input traces comprising a time-history of inputs provided via the connection. Weight of the connection may be updated and connection state may be time varying. The updated weights may be determined using a rate of change of the input trace and a term comprising a product of a per-neuron contribution and a per-connection contribution configured to account for the state time-dependency. Using event-dependent connection change components, connection updates may be executed on a per neuron basis, as opposed to a per-connection basis.", "Apparatus and methods for state-dependent learning in spiking neuron networks "]
["Methods and systems for backlash compensation. Restrictive assumptions on the backlash nonlinearity (e.g. the same slopes of the lines, etc.) are not required. The compensator scheme has dynamic inversion structure, with a neural network in the feedforward path that approximates the backlash inversion error plus filter dynamics needed for backstepping design. The neural network controller does not require preliminary off-line training. Neural network tuning is based on a modified Hebbian tuning law, which requires less computation than backpropagation. The backstepping controller uses a practical filtered derivative, unlike the usual differentiation required by earlier backstepping routines. Rigorous stability proofs are given using Lyapunov theory. Simulation results show that the proposed compensation scheme is an efficient way of improving the tracking performance of a vast array of nonlinear systems with backlash.", "Backlash compensation using neural network "]
["An artificial neural network detects points in feature space outside of a boundary determined by a set of sample data. The network is trained using pseudo data which compensates for the lack of original data representing \"abnormal\" or novel combinations of features. The training process is done iteratively using a net bias parameter to close the boundary around the sample data. When the neural net stabilizes, the training process is complete. Pseudo data is chosen using several disclosed methods.", "Method for using a feed forward neural network to perform classification with highly biased data "]
["Apparatus, and an accompanying method, for use in, e.g., a neural network-based optical character recognition (OCR) system (5) for accurately classifying each individual character extracted from a string of characters, and specifically for generating a highly reliable confidence measure that would be used in deciding whether to accept or reject each classified character. Specifically, a confidence measure, associated with each output of, e.g., a neural classifier (165), is generated through use of all the neural activation output values. Each individual neural activation output provides information for a corresponding atomic hypothesis of an evidence function. This hypothesis is that a pattern belongs to a particular class. Each neural output is transformed (1650) through a pre-defined monotonic function into a degree of support in its associated evidence function. These degrees of support are then combined (1680, 1690) through an orthogonal sum to yield a single confidence measure associated with the specific classification then being produced by the neural classifier.", "Evidential confidence measure and rejection technique for use in a neural network based optical character recognition system "]
["The sequencer (14) is part of a computational system (10) which includes a computational circuit component, or processor node array (16); the sequencer, or controller component (14); and a boundary interface (34) between the computational circuit component (16) and the controller component (14). The controller component (14) provides three main functions in the system: (one) it sequences computations in a computational component (16), which includes an array of processor nodes (74, 76, 78, 80, 82, 84); (two) it provides I/O processing (20) from several disparate sources between the processor node array (16) and a host processor (12); and (three) it synchronizes data flow from a substantially asynchronous portion of the system (12) with a substantially synchronous data flow in the processor node array portion of the system (16).", "Neural network sequencer and interface apparatus "]
["A method for and system for training at least one connection network located between neuron layers within a multi-layer physical neural network. A multi-layer physical neural network can be formed having a plurality of inputs and a plurality outputs thereof, wherein the multi-layer physical neural network comprises a plurality of layers therein, such that each layer thereof comprises at least one connection network and at least one associated neuron. Thereafter, a training wave, as further described herein, can be initiated across one or more connection networks associated with an initial layer of the multi-layer physical neural network which propagates thereafter through succeeding connection networks of succeeding layers of the multi-layer physical neural network by successively closing and opening at least one switch associated with each layer of the multi-layer physical neural network. At least one feedback signal thereof can be automatically provided to each preceding connection network associated with each preceding layer thereof to strengthen or weaken nanoconnections associated with each connection network of the multi-layer physical neural network.", "Multi-layer training in a physical neural network formed utilizing nanotechnology "]
["A method of accelerating the training of an artificial neural network uses a computer configured as an artificial neural network with a network input and a network output, and having a plurality of interconnected units arranged in layers including an input layer and an output layer. Each unit has a multiplicity of unit inputs and a set of variables for operating upon a unit inputs to provide a unit output. The computer is programmed with a back propagation algorithm. A plurality of examples are serially provided to the network input and the network output is observed. The examples are iterated and proposed changes to each set of variables are calculated in response to feedback representing differences betwen the network output for each example and the desired output. The proposed changes are accumulated for a predetermined number of iterations, whereupon the accumulated proposed changes are added to the set of variables.", "Adjusting neural networks "]
["A method controls a welding apparatus by using a neural network to recognize an acceptable weld signature. The neural network recognizes a pattern presented by the instantaneous weld signature, and modifies the instantaneous weld signature when the pattern is not acceptable. The method measures a welding voltage, current, and wire feed speed (WFS), and trains the neural network using the instantaneous weld signature when the instantaneous weld signature is different from each of the different training weld signatures. A welding apparatus for controlling a welding process includes a welding gun, a power supply for supplying a welding voltage and current, and a sensor for detecting values of a plurality of different welding process variables. A controller of the apparatus has a neural network for receiving the welding process variables and for recognizing a pattern in the weld signature. The controller modifies the weld signature when the pattern is not recognized.", "Welding power supply with neural network controls "]
["An event-driven neural network includes a plurality of interconnected core circuits is provided. Each core circuit includes an electronic synapse array has multiple digital synapses interconnecting a plurality of digital electronic neurons. A synapse interconnects an axon of a pre-synaptic neuron with a dendrite of a post-synaptic neuron. A neuron integrates input spikes and generates a spike event in response to the integrated input spikes exceeding a threshold. Each core circuit also has a scheduler that receives a spike event and delivers the spike event to a selected axon in the synapse array based on a schedule for deterministic event delivery.", "Neuromorphic event-driven neural computing architecture in a scalable neural network "]
["A speech-recognition system for recognizing isolated words includes pre-processing circuitry for performing analog-to-digital conversion and cepstral analysis, and a plurality of neural networks which compute discriminant functions based on polynomial expansions. The system may be implemented using either hardware or software or a combination thereof. The speech wave-form of a spoken word is analyzed and converted into a sequence of data frames. The sequence of frames is partitioned into data blocks, and the data blocks are then broadcast to a plurality of neural networks. Using the data blocks, the neural networks compute polynomial expansions. The output of the neural networks is used to determine the identity of the spoken word. The neural networks utilize a matrix-inversion or alternatively a least-squares estimation training algorithm which does not require repetitive training and which yields a global minimum to each given set of training examples.", "Method of training neural networks used for speech recognition "]
["A color corrector for changing pixels in an image where the color corrector includes a neural fuzzy classifier to generate a membership value which defines a degree of membership of each pixel in a group of pixels to be transformed. A pixel color changer is also provided to transform the pixel according to its membership in the group of pixels to be changed. The color corrector can also include a pixel group classifier for identifying groups of pixels in the image to train the neural fuzzy classifier to generate the membership value.", "Non-linear color corrector having a neural network and using fuzzy membership values to correct color and a method thereof "]
["Apparatus and methods for universal node design implementing a universal learning rule in a mixed signal spiking neural network. In one implementation, at one instance, the node apparatus, operable according to the parameterized universal learning model, receives a mixture of analog and spiking inputs, and generates a spiking output based on the model parameter for that node that is selected by the parameterized model for that specific mix of inputs. At another instance, the same node receives a different mix of inputs, that also may comprise only analog or only spiking inputs and generates an analog output based on a different value of the node parameter that is selected by the model for the second mix of inputs. In another implementation, the node apparatus may change its output from analog to spiking responsive to a training input for the same inputs.", "Neural network apparatus and methods for signal conversion "]
["There is disclosed a pattern identifying neural network comprising at least an input and an output layer, the output layer having a plurality of principal nodes, each principal node trained to recognize a different class of patterns, and at least one fuzzy node trained to recognize all classes of patterns recognized by the principal nodes but with outputs set out at levels lower than the corresponding outputs of the principal nodes.", "Fuzzy neural networks "]
["The network comprises several memory elements made of ferroelectric polymer, arranged in a matrix organization at the intersections of row and column electrodes. Each memory element (Mij) memorizes a synaptic coefficient aij of the network which may be restored by pyroelectric effect on the corresponding column of the network. Amplifier circuits, respectively connected to the columns, give a voltage which is equal to the sum, to which a sign is assigned, of the products of the synaptic coefficients by the voltage components applied to each of the lines of the network.", "Programmable ferroelectric polymer neural network "]
["Methods and systems for modifying at least one synapse of a physical neural network. A physical neural network implemented as an adaptive neural network can be provided, which includes one or more neurons and one or more synapses thereof, wherein the neurons and synapses are formed from a plurality of nanoparticles disposed within a dielectric solution in association with one or more pre-synaptic electrodes and one or more post-synaptic electrodes and an applied electric field. At least one pulse can be generated from one or more of the neurons to one or more of the pre-synaptic electrodes of a succeeding neuron and one or more post-synaptic electrodes of one or more of the neurons of the physical neural network, thereby strengthening at least one nanoparticle of a plurality of nanoparticles disposed within the dielectric solution and at least one synapse thereof.", "Adaptive neural network utilizing nanotechnology-based components "]
["A reconfigurable neural network circuit is provided. The reconfigurable neural network circuit comprises an electronic synapse array including multiple synapses interconnecting a plurality of digital electronic neurons. Each neuron comprises an integrator that integrates input spikes and generates a signal when the integrated inputs exceed a threshold. The circuit further comprises a control module for reconfiguring the synapse array. The control module comprises a global final state machine that controls timing for operation of the circuit, and a priority encoder that allows spiking neurons to sequentially access the synapse array.", "Reconfigurable and customizable general-purpose circuits for neural networks "]
["Discriminative pretraining technique embodiments are presented that pretrain the hidden layers of a Deep Neural Network (DNN). In general, a one-hidden-layer neural network is trained first using labels discriminatively with error back-propagation (BP). Then, after discarding an output layer in the previous one-hidden-layer neural network, another randomly initialized hidden layer is added on top of the previously trained hidden layer along with a new output layer that represents the targets for classification or recognition. The resulting multiple-hidden-layer DNN is then discriminatively trained using the same strategy, and so on until the desired number of hidden layers is reached. This produces a pretrained DNN. The discriminative pretraining technique embodiments have the advantage of bringing the DNN layer weights close to a good local optimum, while still leaving them in a range with a high gradient so that they can be fine-tuned effectively.", "Discriminative pretraining of deep neural networks "]
["A new image reconstruction technique for imaging two- and three-phase flows using electrical capacitance tomography (ECT) has been developed based on multi-criteria optimization using an analog neural network, hereafter referred to as Neural Network Multi-criteria Optimization Image Reconstruction (NN-MOIRT)). The reconstruction technique is a combination between multi-criteria optimization image reconstruction technique for linear tomography, and the so-called linear back projection (LBP) technique commonly used for capacitance tomography. The multi-criteria optimization image reconstruction problem is solved using Hopfield model dynamic neural-network computing. For three-component imaging, the single-step sigmoid function in the Hopfield networks is replaced by a double-step sigmoid function, allowing the neural computation to converge to three-distinct stable regions in the output space corresponding to the three components, enabling the differentiation among the single phases.", "Neural network based multi-criteria optimization image reconstruction technique for imaging two- and three-phase flow systems using electrical capacitance tomography "]
["A method for training a probabilistic neural network to map seismic attributes or similar quantities.", "Method for mapping seismic attributes using neural networks "]
["A weld controller utilizes a neural network to compute power factor of a secondary circuit of a welding transformer supply power to a workpiece through a pair of welding electrodes. Phase controlled switches supply power to the transformer and the neural network uses the phase angle at the time of energization of the switches and the length of time that the switches conduct to compute the power factor. The computed power factor is compared with previous computations of power factor online and any changes are interpreted as changes in resistance of the secondary circuit. This provides a measure of contact wear and the weld controller can compensate for these changes by increasing weld power. The neural network is adaptable for use with other types of control systems.", "Apparatus using a neural network for power factor calculation "]
["A means and method for training an electronic network of analog cells. A generic universal programmer interface (GUPI) is provided to enable connection of an adaptor to a host computer via a personal computer personal programmer (PCPP) for training an ETANN chip. An opening in the top of the GUPI provides for installation of the adaptor. The adaptor of the present invention plugs into the GUPI module to provide for connecting either an ETANN chip or a Confidence Test Module (CTM) for access by the user via the host computer. A target socket located on the top surface of the adaptor allows the user to physically connect the ETANN chip or the CTM to adapter of the present invention. The adaptor provides the resources needed for training an ETANN in several modes including: digital to analog conversion for driving the ETANN's analog input pins; a single analog to digital converter with an 80 channel analog multiplexer for measuring the ETANN's analog outputs; neuron perturbation circuitry; synapse current measurement circuitry; neuron sum measurement circuitry; VPP management circuitry; a binary searching synapse weight measurement circuit; isolation switches and the logic necessary to connect all these resources to the ETANN and the GUPI base. The present invention also includes host computer software necessary to control the operation of the adapter.", "Training system for neural networks and the like "]
["Apparatus and methods for feedback in a spiking neural network. In one approach, spiking neurons receive sensory stimulus and context signal that correspond to the same context. When the stimulus provides sufficient excitation, neurons generate response. Context connections are adjusted according to inverse spike-timing dependent plasticity. When the context signal precedes the post synaptic spike, context synaptic connections are depressed. Conversely, whenever the context signal follows the post synaptic spike, the connections are potentiated. The inverse STDP connection adjustment ensures precise control of feedback-induced firing, eliminates runaway positive feedback loops, enables self-stabilizing network operation. In another aspect of the invention, the connection adjustment methodology facilitates robust context switching when processing visual information. When a context (such an object) becomes intermittently absent, prior context connection potentiation enables firing for a period of time. If the object remains absent, the connection becomes depressed thereby preventing further firing.", "Spiking neural network feedback apparatus and methods "]
["A two-level hierarchical approach for process fault diagnosis is an operating system employs a function-oriented approach at a first level and a component characteristic-oriented approach at a second level, where the decision-making procedure is structured in order of decreasing intelligence with increasing precision. At the first level, the diagnostic method is general and has knowledge of the overall process including a wide variety of plant transients and the functional behavior of the process components. An expert system classifies malfunctions by function to narrow the diagnostic focus to a particular set of possible faulty components that could be responsible for the detected functional misbehavior of the operating system. At the second level, the diagnostic method limits its scope to component malfunctions, using more detailed knowledge of component characteristics. Trained artificial neural networks are used to further narrow the diagnosis and to uniquely identify the faulty component by classifying the abnormal condition data as a failure of one of the hypothesized components through component characteristics. Once an anomaly is detected, the hierarchical structure is used to successively narrow the diagnostic focus from a function misbehavior, i.e., a function oriented approach, until the fault can be determined, i.e., a component characteristic-oriented approach.", "Combined expert system/neural networks method for process fault diagnosis "]
["A system for converting an analog signal into a digital data stream includes a recurrent network with a plurality of converter circuits that individually receive the same analog signal as input. The circuits then generate a plurality of spike outputs that exhibit characteristics of the analog signal. Interconnecting feedback loops from each circuit output to the input of neighboring circuits queues the plurality of spike outputs to thereby self-organize the network. A digital clock is then used to establish predetermined time intervals for counting the spike outputs to create the digital data stream.", "Analog to digital conversion using recurrent neural networks "]
["The use of a pipelined algorithm that performs parallelized computations to train deep neural networks (DNNs) for performing data analysis may reduce training time. The DNNs may be one of context-independent DNNs or context-dependent DNNs. The training may include partitioning training data into sample batches of a specific batch size. The partitioning may be performed based on rates of data transfers between processors that execute the pipelined algorithm, considerations of accuracy and convergence, and the execution speed of each processor. Other techniques for training may include grouping layers of the DNNs for processing on a single processor, distributing a layer of the DNNs to multiple processors for processing, or modifying an execution order of steps in the pipelined algorithm.", "Deep neural networks training for speech and pattern recognition "]
["Neural networks are constructed (programmed), and trained on historical data relating the (i) alleles, to the (ii) clinical responses, of a large number of patients. The trained neural networks show which alleles are, in combination, of practical pertinence to a wide range of biological, social and clinical variables. The trained neural networks may be exercised to predict (i) the responses of populations to different therapies, and (ii) the occurrences of adverse reactions. The trained neural networks are exercised in consideration of the genomic data of an individual patient to predict the response(s) of the individual patient to, most particularly usefully, any of (1) optimal drug dosage, (2) drug dosage sensitivity, (3) expected therapeutic outcome(s), and/or (4) adverse side effects may can be predicted in consideration of the alleles of the patient. Both the human and the economic costs of both optimal and sub-optimal drug therapies may be extrapolated from the exercise of various optimized and trained neural networks. The preferred neural network mapping is on (i) inputs that have underdone \u201chouseholding\u201d, meaning that multiple genes are treated as a single unit, by (ii) use of a Genetic Algorithm (GA) that is \u201crolled\u201d, meaning that mapping transpires in neural networks organized hierarchically in stages so as to relate a typically vast amount genomic data as neural networks inputs to but very little clinical data as the outputs of a final, root node, neural network.", "Neural -network-based identification, and application, of genomic information practically relevant to diverse biological and sociological problems, including susceptibility to disease "]
["Software for controlling processes in a heterogeneous semiconductor manufacturing environment may include a wafer-centric database, a real-time scheduler using a neural network, and a graphical user interface displaying simulated operation of the system. These features may be employed alone or in combination to offer improved usability and computational efficiency for real time control and monitoring of a semiconductor manufacturing process. More generally, these techniques may be usefully employed in a variety of real time control systems, particularly systems requiring complex scheduling decisions or heterogeneous systems constructed of hardware from numerous independent vendors.", "Scheduling with neural networks and state machines "]
["A neural network includes an input layer comprising a plurality of input units (24) interconnected to a hidden layer with a plurality of hidden units (26) disposed therein through an interconnection matrix (28). Each of the hidden units (26) is a single output that is connected to output units (32) in an output layer through an interconnection matrix (30). Each of the interconnections between one of the hidden units (26) to one of the output units (32) has a weight associated therewith. Each of the hidden units (26) has an activation in the i'th dimension and extending across all the other dimensions in a non-localized manner in accordance with the following equation: ##EQU1## that the network learns by the Back Propagation method to vary the output weights and the parameters of the activation function \u03bchi and \u03c3hi.", "Neural network with semi-localized non-linear mapping of the input space "]
["Apparatus and methods for feedback in a spiking neural network. In one approach, spiking neurons receive sensory stimulus and context signal that correspond to the same context. When the stimulus provides sufficient excitation, neurons generate response. Context connections are adjusted according to inverse spike-timing dependent plasticity. When the context signal precedes the post synaptic spike, context synaptic connections are depressed. Conversely, whenever the context signal follows the post synaptic spike, the connections are potentiated. The inverse STDP connection adjustment ensures precise control of feedback-induced firing, eliminates runaway positive feedback loops, enables self-stabilizing network operation. In another aspect of the invention, the connection adjustment methodology facilitates robust context switching when processing visual information. When a context (such an object) becomes intermittently absent, prior context connection potentiation enables firing for a period of time. If the object remains absent, the connection becomes depressed thereby preventing further firing.", "Sensory input processing apparatus in a spiking neural network "]
["A system and method for characterizing a pattern, in which a spiking neural network having at least one layer of neurons is provided. The spiking neural network has a plurality of connected neurons for transmitting signals between the connected neurons. A model for inducing spiking in the neurons is specified. Each neuron is connected to a global regulating unit for transmitting signals between the neuron and the global regulating unit. Each neuron is connected to at least one other neuron for transmitting signals from this neuron to the at least one other neuron, this neuron and the at least one other neuron being on the same layer. Spiking of each neuron is synchronized according to a number of active neurons connected to the neuron. At least one pattern is submitted to the spiking neural network for generating sequences of spikes in the spiking neural network, the sequences of spikes (i) being modulated over time by the synchronization of the spiking and (ii) being regulated by the global regulating unit. The at least one pattern is characterized according to the sequences of spikes generated in the spiking neural network.", "Spatio-temporal pattern recognition using a spiking neural network and processing thereof on a portable and/or distributed computer "]
["A coprocessor and method for processing convolutional neural networks includes a configurable input switch coupled to an input. A plurality of convolver elements are enabled in accordance with the input switch. An output switch is configured to receive outputs from the set of convolver elements to provide data to output branches. A controller is configured to provide control signals to the input switch and the output switch such that the set of convolver elements are rendered active and a number of output branches are selected for a given cycle in accordance with the control signals.", "Dynamically configurable, multi-ported co-processor for convolutional neural networks "]
["A speech signal is subjected imperfect to vocal tract analysis model and the output therefrom is analyzed by a neural network. The output from the neural network is compared with the parameters stored in the network definition function, to derive measurement of the quality of the speech signal supplied to the source. The network definition function is determined by applying to the trainable processing apparatus a distortion perception measure indicative of the extent to which a distortion would be perceptible to a human listener.", "Trained artificial neural networks using an imperfect vocal tract model for assessment of speech signal quality "]
["A method is disclosed for for computing clusters, relationships amongst clusters, and association rules from data at various levels of significance. First the clusters are found via a dual-approximation method followed by Boolean minimization. Then a customized multiplicative neural network which uses a special kind of fuzzy logic is constructed from the association rules. This particular fuzzy-logic shows how make arithmetic equal to fuzzy-logic. Other types of fuzzy logics appropriate for this datamining tool are described. This particular method of clustering is multiplicative, resembling \u201cdimensional analysis\u201d of physics and engineering in contrast to the linear methods such as principal component analysis (PCA). The complete set of association rules is constructed from the data automatically. Then 2-dimensional and 3-dimensional visualization and visual-datamining tools are constructed.", "Scalable, parallelizable, fuzzy logic, boolean algebra, and multiplicative neural network based classifier, datamining, association rule finder and visualization software tool "]
["A method for configuring nanoscale neural network circuits using molecular-junction-nanowire crossbars, and nanoscale neural networks produced by this method. Summing of weighted inputs within a neural-network node is implemented using variable-resistance resistors selectively configured at molecular-junction-nanowire-crossbar junctions. Thresholding functions for neural network nodes are implemented using pFET and nFET components selectively configured at molecular-junction-nanowire-crossbar junctions to provide an inverter. The output of one level of neural network nodes is directed, through selectively configured connections, to the resistor elements of a second level of neural network nodes via circuits created in the molecular-junction-nanowire crossbar. An arbitrary number of inputs, outputs, neural network node levels, nodes, weighting functions, and thresholding functions for any desired neural network are readily obtained by the methods of the present invention.", "Molecular-junction-nanowire-crossbar-based neural network "]
["A neural network signal processor (NSP) (20) that can accept, as input, unprocessed signals (32), such as those directly from a sensor. Consecutive portions of the input waveform are directed simultaneously to input processing units, or \"neurons\" (22). Each portion of the input waveform (32) advances through the input neurons (22) until each neuron receives the entire waveform (32). During a training procedure, the NSP 20 receives a training waveform (30) and connective weights, or \"synapses\" (28) between the neurons are adjusted until a desired output is produced. The NSP (20) is trained to produce a single response while each portion of the input waveform is received by the input neurons (22). Once trained, when an unknown waveform (32) is received by the NSP (20), it will respond with the desired output when the unknown waveform (32) contains some form of the training waveform (30).", "Neural network signal processor "]
["The present invention is a method of diagnosing a cardiopulmonary condition in an individual by comparing data from a progressive multi-stage test for the individual to a non-linear multi-variate model, preferably a recurrent artificial neural network having sensor fusion. The present invention relies on a cardiovascular model developed from physiological measurements of an individual. Any differences between the modeled parameters and the parameters of an individual at a given time are used for diagnosis.", "Artificial neural network cardiopulmonary modeling and diagnosis "]
["A neural network based control system includes a nominal control system augmented by adaptive control such as a neuro-controller which generates additional compensating control signals based on differences between a model and actual system output. The nominal control system provides basic stability and performance, while the adaptive controller provides performance enhancement. The adaptive control can rely on any neural network that can encode a-priori knowledge and employs a high resolution pattern discrimination capability suitable for real-time changes. To prevent unbounded adaptation, the output of the adaptive controller is constrained by a limiter, thus ensuring safety of the overall system.", "Stabilized adaptive neural network based control system "]
["Apparatus and methods for learning and training in neural network-based devices. In one implementation, the devices each comprise multiple spiking neurons, configured to process sensory input. In one approach, alternate heterosynaptic plasticity mechanisms are used to enhance learning and field diversity within the devices. The selection of alternate plasticity rules is based on recent post-synaptic activity of neighboring neurons. Apparatus and methods for simplifying training of the devices are also disclosed, including a computer-based application. A data representation of the neural network may be imaged and transferred to another computational environment, effectively copying the brain. Techniques and architectures for achieve this training, storing, and distributing these data representations are also disclosed.", "Neural network learning and collaboration apparatus and methods "]
["A system and method for generating a neural network ensemble. Conventional algorithms are used to train a number of neural networks having error diversity, for example by having a different number of hidden nodes in each network. A genetic algorithm having a multi-objective fitness function is used to select one or more ensembles. The fitness function includes a negative error correlation objective to insure diversity among the ensemble members. A genetic algorithm may be used to select weighting factors for the multi-objective function. In one application, a trained model may be used to produce synthetic open hole logs in response to inputs of cased hole log data.", "Genetic algorithm based selection of neural network ensemble for processing well logging data "]
["Discrete-time neural networks are implemented using switched capacitors, switches and inverters and advantage is taken of the inherent saturation of the inverters to implement the neuron non-linearity without additional elements.", "Switched neural networks "]
["A neural-simulating system for an image processing system includes a plurality of networks arranged in a plurality of layers, the output signals of ones of the layers provide input signals to the others of the layers. Each of the plurality of layers include a plurality of neurons operating in parallel on the input signals to the layers. The plurality of neurons within a layer are arranged in groups. Each of the neurons within a group operates in parallel on the input signals. Each neuron within a group of neurons operates to extract a specific feature of an area of the image being processed. Each of the neurons derives output signals from the input signals representing the relative weight of the input signal applied thereto based upon a continuously differential transfer function for each function.", "Neural network image processing system "]
["Apparatus and methods for feedback in a spiking neural network. In one approach, spiking neurons receive sensory stimulus and context signal that correspond to the same context. When the stimulus provides sufficient excitation, neurons generate response. Context connections are adjusted according to inverse spike-timing dependent plasticity. When the context signal precedes the post synaptic spike, context synaptic connections are depressed. Conversely, whenever the context signal follows the post synaptic spike, the connections are potentiated. The inverse STDP connection adjustment ensures precise control of feedback-induced firing, eliminates runaway positive feedback loops, enables self-stabilizing network operation. In another aspect of the invention, the connection adjustment methodology facilitates robust context switching when processing visual information. When a context (such an object) becomes intermittently absent, prior context connection potentiation enables firing for a period of time. If the object remains absent, the connection becomes depressed thereby preventing further firing.", "Spiking neural network object recognition apparatus and methods "]
["A method for linearization of feedback in neural networks, and a neural network incorporating the feedback linearization method are presented. Control action is used to achieve tracking performance for a state-feedback linearizable, but unknown nonlinear control system. The control signal comprises a feedback linearization portion provided by neural networks, plus a robustifying portion that keep the control magnitude bounded. Proofs are provided to show that all of the signals in the closed-loop system are semi-globally uniformly ultimately bounded. This eliminates an off-line learning phase, and simplifies the initialization of neural network weights.", "Method for feedback linearization of neural networks and neural network incorporating same "]
["A control system for a countercurrent pulp washing process in which the pulp is formed as a pulp mat on at least one moving filter surface and the mat is supplied with rinse water to replace water in the pulp mat thereby reducing the soda loss in the mat before it is removed from the filter surface. The process is characterized by at least one predictable process variable including dissolved solids retained in the pulp mat. The system comprises a trainable neural network having a plurality of input neurons having input values applied thereto and output neurons for providing output values and means for training the neural network to provide predicted values for the predictable process variables.", "Control system for controlling a pulp washing system using a neural network controller "]
["A time delay neural network is defined having feature detection layers which are constrained for extracting features and subsampling a sequence of feature vectors input to the particular feature detection layer. Output from the network for both digit and uppercase letters is provided by an output classification layer which is fully connected to the final feature detection layer. Each feature vector relates to coordinate information about the original character preserved in a temporal order together with additional information related to the original character at the particular coordinate point. Such additional information may include local geometric information, local pen information, and phantom stroke coordinate information relating to connecting segments between the end point of one stroke and the beginning point of another stroke.", "Time delay neural network for printed and cursive handwritten character recognition "]
["An apparatus and method for controlling a process using a neural network which operates as part of a closed loop control system. The state of the control system is defined by one or more process condition signals and monitored for a predetermined set of controller parameters. The output of the control system is one or more device control signals, used by a control device to alter a process being controlled. The neural network uses normalized values of process condition signals in combination with a predetermined set of controller parameters to produce correction control signals. The correction control signals are then used to the create device control signals. Proper normalization of at least one of the process condition signals using the throttling range set by the controller parameters is necessary. The remaining input signals must be normalized as well, but the method of normalization is not as critical, except to create a common range for all process condition signals input to the neural network.", "Neural network including input normalization for use in a closed loop control system "]
["An apparatus for in-depth three dimensional tumor mapping including (A) a light source; (B) a multi-fiber bundle including at least one illumination fiber and at least two receiving fibers, the at least one illumination fiber being connected to the light source; (C) a spectrometer connected to the at least two receiving fibers; and (D) a hybrid neural network connected to the spectrometer, said hybrid neural network including a principle component analysis processor and a neural network classifier.", "Hybrid neural network and multiple fiber probe for in-depth 3-D mapping "]
["A neural network shell has a defined interface to an application program. By interfacing with the neural network shell, any application program becomes a neural network application program. The neural network shell contains a set of utility programs that transfers data into and out of a neural network data structure. This set of utility programs allows an application program to define a new neural network model, create a neural network data structure, train a neural network, and run a neural network. Once trained, the neural network data structure can be transported to other computer systems or to application programs written in different computing languages running on similar or different computer systems.", "Neural network shell for application programs "]
["Generalized state-dependent learning framework in artificial neuron networks may be implemented. A framework may be used to describe plasticity updates of neuron connections based on connection state term and neuron state term. The state connections within the network may be updated based on inputs and outputs to/from neurons. The input connections of a neuron may be updated using connection traces comprising a time-history of inputs provided via the connections. Weights of the connections may be updated and connection state may be time varying. The updated weights may be determined using a rate of change of the trace and a term comprising a product of a per-neuron contribution and a per-connection contribution configured to account for the state time-dependency. Using event-dependent connection change components, connection updates may be executed on per neuron basis, as opposed to per-connection basis.", "Apparatus and methods for generalized state-dependent learning in spiking neuron networks "]
["A neural-like network system that adaptively controls a visually guided, two-jointed robot arm to reach spot targets in three dimensions. The system learns and maintains visual-motor calibrations by itself, starting with only loosely defined relationships. The geometry of the system is composed of distributed, interleaved combinations of actuator inputs. It is fault tolerant and uses analog processing. Learning is achieved by modifying the distributions of input weights in the system after each arm positioning. Modifications of the weights are made incrementally according to errors of consistency between the actuator signals used to orient the cameras and those used to move the arm.", "Neural network system for adaptive sensory-motor coordination of multijoint robots for single postures "]
["An analyzing system analyzes object signals, particularly voice signals, by estimating a generation likelihood of an observation vector sequence being a time series of feature vectors with use of a Markov model having a plurality of states and given transition probabilities from state to state. A state designation section determines a state i at a time t stochastically using the Markov model. Plural predictors, each of which is composed of a neural network and is provided per each state of the Markov model, are provided for generating a predictional vector of the feature vector xt in the state i at the time t based on values of the feature vectors other than the feature vector xt. A first calculation section calculates an error vector of the predictional vector to the feature vector xT. A second calculation section calculates a generation likelihood of the error vector using a predetermined probability distribution of the error vector according to which the error vector is generated.", "Voice analyzing system using hidden Markov model and having plural neural network predictors "]
["A neural network system capable of performing integrated processing of a plurality of information includes a feature extractor group for extracting a plurality of learning feature data from learning data in a learning mode and a plurality of object feature data from object data to be processed in an execution mode, and an information processing unit for learning features of the learning data, based on the plurality of learning feature data from the feature extractor group and corresponding teacher data in the learning mode, and determining final learning result data from the plurality of object feature data from the feature extractor group in accordance with the learning result, including a logic representing relation between the plurality of object feature data in the execution mode.", "Neural network with learning function "]
["A method of accelerating the training of an artificial neural network uses a computer configured as an artificial neural network with a network input and a network output, and having a plurality of interconnected units arranged in layers including an input layer and an output layer. Each unit has a multiplicity of unit inputs and a set of variables for operating upon the unit inputs to provide a unit output. A plurality of examples are serially provided to the network input and the network output is observed. The computer is programmed with a back propagation algorithm for adjusting each set of variables in response to feedback representing differences between the network output for each example and the desired output. The examples are iterated while those values which change are identified. The examples are reiterated and the algorithm is applied to only those values which changed in a previous iteration.", "Accelerating learning in neural networks "]
["A sequence generator employing a neural network having its output coupled to at least one plurality of delay elements. The delayed outputs are fed back to an input interconnection network, wherein they contribute to the next state transition through an appropriate combination of interconnections.", "Temporal sequences with neural networks "]
["A control system having four major components: a target optimizer, a path optimizer, a neural network adaptation controller and a neural network. In the target optimizer, the controlled variables are optimized to provide the most economically desirable outputs, subject to operating constraints. Various manipulated variable and disturbance values are provided for modeling purposes. The neural network receives as inputs a plurality of settings for each manipulated and disturbance variable. For target optimization all the neural network input values are set equal to produce a steady state controlled variable value. The entire process is repeated with differing manipulated variable values until an optimal solution develops. The resulting target controlled and manipulated variable values are provided to the path optimizer to allow the manipulated variables to be adjusted to obtain the target output. Various manipulated variable values are developed to model moves from current to desired values. In this case trend indicating values of the manipulated and disturbance variables are provided to produce time varying values of the controlled variables. The process is repeated until an optimal path is obtained, at which time the manipulated variable values are applied to the actual process. On a periodic basis all of the disturbance, manipulated and controlled variables are sampled to find areas where the training of the neural network is sparse or where high dynamic conditions are indicated. These values are added to the set of values used to train the neural network.", "Control system using an adaptive neural network for target and path optimization for a multivariable, nonlinear process "]
["This is a recurrent or feedforward analog neural network processor having a multi-level neuron array and a synaptic matrix for storing weighted analog values of synaptic connection strengths which is characterized by temporarily changing one connection strength at a time to determine its effect on system output relative to the desired target. That connection strength is then adjusted based on the effect, whereby the processor is taught the correct response to training examples connection by connection.", "Analog hardware for learning neural networks "]
["A speech recognition apparatus in which the speech signal is digitalized and subjected to special analysis, word end detection is effected by energy analysis of the speech signal and the recognition system utilizes a Markov model in combination with a neural network learning by specific training steps.", "Speaker independent isolated word recognition system using neural networks "]
["Herein disclosed is a data processing system having a memory packaged therein for realizing a large-scale and high-speed parallel distributed processing and, especially, a data processing system for the neural network processing. The neural network processing system according to the present invention comprises: a memory circuit for storing neuron output values, connection weights, the desired values of outputs, and data necessary for learning; and input/output circuit for writing or reading data in or out of said memory circuit; a processing circuit for performing a processing for determining the neuron outputs such as the product, sum and nonlinear conversion of the data stored in said memory circuit, a comparison of the output value and its desired value, and a processing necessary for learning; and a control circuit for controlling the operations of said memory circuit, said input/output circuit and said processing circuit. The processing circuit is constructed to include at least one of an adder, a multiplier, a nonlinear transfer function circuit and a comparator so that at least a portion of the processing necessary for determining the neutron output values such as the product or sum may be accomplished in parallel. Moreover, these circuits are shared among a plurality of neutrons and are operated in a time sharing manner to determine the plural neuron output values. Still moreover, the aforementioned comparator compares the neuron output value determined and the desired value of the otuput in parallel.", "Neural network processing system using semiconductor memories "]
["Neural networks learn expert system rules, for either business or real-time applications, to improve the robustness and speed of execution of the expert system. One or more neural networks are constructed which incorporate the production rules of one or more expert systems. Each neural network is constructed of neurons or neuron circuits each having only one significant processing element in the form of a multiplier. Each neural network utilizes a training algorithm which does not require repetitive training and which yields a global minimum to each given set of input vectors.", "Method for structuring an expert system utilizing one or more neural networks "]
["Disclosed herein is a programming tool stored on a computer-readable medium and adapted for implementation by a computer for designing an artificial neural network. The programming tool includes a network configuration module to provide a first display interface to support configuration of the artificial neural network, and a pattern data module to provide a second display interface to support establishment and modification of first and second pattern data sets for training and testing the artificial neural network, respectively.", "Artificial neural network design and evaluation tool "]
["A parallel convolutional neural network is provided. The CNN is implemented by a plurality of convolutional neural networks each on a respective processing node. Each CNN has a plurality of layers. A subset of the layers are interconnected between processing nodes such that activations are fed forward across nodes. The remaining subset is not so interconnected.", "System and method for parallelizing convolutional neural networks "]
["Systems, methodologies, media, and other embodiments associated with feature weighting in neural networks are described. One exemplary method embodiment includes using a set of weights to scale input feature values. Then the scaled data are used to train a neural net model of the relationship to be learned. The learned model is used to produce a new set of feature weights. The procedure continues iteratively until stopping criteria is met.", "Iterative feature weighting with neural networks "]
["A method and system for design optimization that incorporates the advantages of both traditional response surface methodology (RSM) and neural networks is disclosed. The present invention employs a unique strategy called parameter-based partitioning of the given design space. In the design procedure, a sequence of composite response surfaces based on both neural networks and polynomial fits is used to traverse the design space to identify an optimal solution. The composite response surface has both the power of neural networks and the economy of low-order polynomials (in terms of the number of simulations needed and the network training requirements). The present invention handles design problems with many more parameters than would be possible using neural networks alone and permits a designer to rapidly perform a variety of trade-off studies before arriving at the final design.", "Method for constructing composite response surfaces by combining neural networks with other interpolation or estimation techniques "]
["Characteristics of the plasma in a plasma-based manufacturing process step are monitored directly and in real time by observing the spectrum which it produces. An artificial neural network analyzes the plasma spectrum and generates control signals to control one or more of the process input parameters in response to any deviation of the spectrum beyond a narrow range. In an embodiment, a plasma reaction chamber forms a plasma in response to input parameters such as gas flow, pressure and power. The chamber includes a window through which the electromagnetic spectrum produced by a plasma in the chamber, just above the subject surface, may be viewed. The spectrum is conducted to an optical spectrometer which measures the intensity of the incoming optical spectrum at different wavelengths. The output of optical spectrometer is provided to an analyzer which produces a plurality of error signals, each indicating whether a respective one of the input parameters to the chamber is to be increased or decreased. The microcontroller provides signals to control respective controls, but these lines are intercepted and first added to the error signals, before being provided to the controls for the chamber. The analyzer can include a neural network and an optional spectrum preprocessor to reduce background noise, as well as a comparator which compares the parameter values predicted by the neural network with a set of desired values provided by the microcontroller.", "Closed loop adaptive control of spectrum-producing step using neural networks "]
["A method of training an artificial neural network uses a computer configured as a plurality of interconnected neural units arranged in a layered network including an input layer having a network input, and an output layer having a network output. A neural unit has a first subunit and a second subunit. The first subunit having one or more first inputs, and a corresponding first set of variables for operating upon the first inputs to provide a first output. The first set of variables can change in response to feedback representing differences between desired network outputs for selected network inputs and actual network outputs. The second subunit has a plurality of second inputs, and a corresponding second set of variables for operating upon said second inputs to provide a second output. The second set of variables can change in response to differences between desired network outputs for selected network inputs and actual network outputs. The computer provides an activating variable representing the difference between current second output and previous second outputs. A series of examples of data is provided as network input to said network. The activating variable is added to the feedback to accelerate the change of said first set of variables. The actual resulting network outputs are compared to desired outputs corresponding to the examples. The examples are iterated until the network outputs converge to a solution.", "Training neural networks "]
["In order for neural network technology to make useful determinations of the identity of letters and numbers that are processed in real time at a postal service sorting center, it is necessary for the neural network to \"learn\" to recognize accurately the many shapes and sizes in which each letter or number are formed on the address surface of the envelope by postal service users. It has been realized that accuracy in the recognition of many letters and numbers is not appreciably sacrificed if the neural network is instructed to identify those characteristics of each letter or number which are in the category \"invariant.\" Then, rather than requiring the neural network to recognize all gradations of shape, location, size, etc. of the identified invariant characteristic, a generalized and bounded description of the invariant segments is used which requires far less inputting of sample data and less processing of information relating to an unknown letter or number.", "Training system for neural networks "]
["A speech recognition system utilizes both matrix and vector quantizers as front ends to a second stage speech classifier such as hidden Markov models (HMMs) and utilizes neural network postprocessing to, for example, improve speech recognition performance. Matrix quantization exploits the \u201cevolution\u201d of the speech short-term spectral envelopes as well as frequency domain information, and vector quantization (VQ) primarily operates on frequency domain information. Time domain information may be substantially limited which may introduce error into the matrix quantization, and the VQ may provide error compensation. The matrix and vector quantizers may split spectral subbands to target selected frequencies for enhanced processing and may use fuzzy associations to develop fuzzy observation sequence data. A mixer provides a variety of input data to the neural network for classification determination. The neural network's ability to analyze the input data generally enhances recognition accuracy. Fuzzy operators may be utilized to reduce quantization error. Multiple codebooks may also be combined to form single respective codebooks for split matrix and split vector quantization to reduce processing resources demand.", "Matrix quantization with vector quantization error compensation and neural network postprocessing for robust speech recognition "]
["A method for predicting impact by using neural networks comprising steps of supplying a predetermined crash curve to a first neural network having an intermediate layer to train said first neural network by means of learning calculation and supplying a predetermined air bag deployment limit curve to a second neural network to train said second neural network, supplying data indicative of crash curve obtained by an acceleration sensing device on collision to said first and second neural networks, predicting in said first neural network a time instance at which a threshold displacement is going to reach based on the basis of the training result in said first neural network, comparing in said second neural network data indicative of crash curve on said collision and said air bag deployment limit curve to produce a decision signal of deploying the air bag according to the comparison result, calculating AND of said decision signal and said time instance in accordance with said AND, to deploy the air bag depending on the impact, and supplying an operation command signal to the air bag deployment operation means.", "A method for predicting impact by using neural networks "]
["A method for interpolating an omitted scan line between two neighboring scan lines of an interlaced image includes detecting an edge direction of the image at a selected point on the omitted scan line, selecting a neural network based upon the detected edge direction, and using the neural network to provide an interpolated value for the selected point.", "Method and apparatus for image deinterlacing using neural networks "]
["A neural processing element for use in a modular neural network is provided. One embodiment provides a neural network comprising an array of autonomous modules (300). The modules (300) can be arranged in a variety of configurations to form neural networks with various topologies, for example, with a hierarchical modular structure. Each module (300) contains sufficient neurons (100) to enable it to do useful work as a stand alone system, with the advantage that many modules (300) can be connected together to create a wide variety of configurations and network sizes. This modular approach results in a scaleable system that meets increased workload with an increase in parallelism and thereby avoids the usually extensive increases in training times associated with unitary implementations.", "Neural processing element for use in a neural network "]
["The invention provides a method for automated training of a plurality of artificial neural networks for phoneme recognition using training data, wherein the training data comprises speech signals subdivided into frames, each frame associated with a phoneme label, wherein the phoneme label indicates a phoneme associated with the frame. A sequence of frames from the training data are provided, wherein the number of frames in the sequence of frames is at least equal to the number of artificial neural networks. Each of the artificial neural networks is assigned a different subsequence of the provided sequence, wherein each subsequence comprises a predetermined number of frames. A common phoneme label for the sequence of frames is determined based on the phoneme labels of one or more frames of one or more subsequences of the provided sequence. Each artificial neural network using the common phoneme label.", "Method for Automated Training of a Plurality of Artificial Neural Networks "]
["Systems and methods for selecting an appropriate caching algorithm to be used when temporarily storing data accessed by an executing application using a neural network may dynamically and/or iteratively replace an initial caching algorithm being used for the application. An input layer of the neural network may gather values of performance related parameters, such as cache hit rates, data throughput rates, or memory access request response times. The neural network may detect a pattern or change in a pattern of accesses, or a change in a workload, a hardware component, or an operating system parameter. Dependent on these and/or other inputs, the neural network may select and apply a caching algorithm likely to improve performance of the application. Other inputs to the neural network may include values of hardware configuration parameters and/or operating system parameters. The neural network may perform a training exercise or may be self-training, e.g., using reinforcement learning.", "System and Method for Effective Caching Using Neural Networks "]
["A routing system in a multimedia integrated network formed of nodes connected by links is provided for transmitting various media such as voice, image and data in a packet format. The respective nodes forming the integrated network output the packets in an optimum output direction so that conditions required by various media and reliability of communication are satisfied. Each node includes an interconnection type neural network for determining the packet output direction. An external stimulus input unit outputs an external stimulus to the neurons in the neural network in response to a present state of the integrated network, such as a packet delay time and a packet loss ratio for respective links, and a condition required by the media such as an allowable packet loss ratio. Therefore, the packet is output in an optimum direction which is adaptive to the present state of the integrated network and which satisfies a condition required by the media.", "Routing system using a neural network "]
["A method tests a subscriber line. The method includes determining values of electrical line features from electrical measurements on the subscriber line and processing a portion of the values of the electrical features with a neural network. The neural network predicts whether the line qualifies to support one or more preselected data services from the portion of the values.", "Line qualification with neural networks "]
["An apparatus for controlling a quantization step size for use in an encoder which divides one image frame into first blocks and encodes the divided first blocks and transmits the encoded data at a constant transmission rate. The apparatus includes a forward analyzer for detecting image complexity with respect to each first block to be quantized, a luminance analyzer for outputting a luminance value representative of each first block, a picture quality estimator for restoring quantized data by using a quantization step size corresponding to the quantized data and generating a judgement reference value corresponding to a minimum blocking effect which cannot be visually recognized on the basis of the restored data of every second block composed of first blocks, a buffer for storing the quantized data and outputting buffer occupancy of the stored data, a neural network having stored weight values which are updated according to the judgement reference value of the picture quality estimator with respect to the quantized previous second block and the stored update rule, generating a quantization step size for the present second block on the basis of the motion vectors, and/or the image complexity and/or the luminance values with respect to the present second block, the buffer occupancy and the updated weight values, and supplying the quantization step size to the picture quality estimator, and a quantizer for quantizing data of the second blocks encoded by the encoder according to a corresponding quantization step size supplied from the neural network.", "Quantization step size control apparatus using neural networks "]
["To avoid the problem of category assignment in artificial neural networks (ANNs) based upon a mapping of the input space (like ROI and KNN algorithms), the present method uses \u201cprobabilities\u201d. Now patterns memorized as prototypes do not represent categories any longer but the \u201cprobabilities\u201d to belong to categories. Thus, after having memorized the most representative patterns in a first step of the learning phase, the second step consists of an evaluation of these probabilities. To that end, several counters are associated with each prototype and are used to evaluate the response frequency and accuracy for each neuron of the ANN. These counters are dynamically incremented during this second step using distances evaluation (between the input vectors and the prototypes) and error criteria (for example the differences between the desired responses and the response given by the ANN). At the end of the learning phase, a function of the contents of these counters allows an evaluation of these probabilities for each neuron to belong to predetermined categories. During the recognition phase, the probabilities associated with the neurons selected by the algorithm permit the characterization of new input vectors and more generally any kind of input (images, signals, sets of data) to detect and classify anomalies. The method allows a significant reduction in the number of neurons that are required in the ANN while improving its overall response accuracy.", "Method for detecting and classifying anomalies using artificial neural networks "]
["An apparatus for transforming a voice signal of a talker into a voice signal having characteristics of a different person provides apparatus for separating the talker's voice signal into a plurality of voice parameters including frequency components, a neural network for transforming at least some of the separated frequency components into those characteristic of the different person, and apparatus for combining the voice parameters for reconstituting the talker's voice signal having characteristics of the different person.", "Apparatus for transforming voice using neural networks "]
["A method of automating the calibration of lookup tables containing correction values to be used in an on-board vehicle system is disclosed. The method includes training a neural network to model engine behavior by outputting cylinder specific crankshaft acceleration correction values in response to any engine speed and load input conditions. The correction values generated are stored in a memory device. The training takes place off-board the vehicle, using a data set previously obtained from operating a representative engine under normal operating conditions.", "Method of generation correction tables for misfire detection using neural networks "]
["A system is described herein which uses a neural network having an input layer that accepts an input vector and a feature vector. The input vector represents at least part of input information, such as, but not limited to, a word or phrase in a sequence of input words. The feature vector provides supplemental information pertaining to the input information. The neural network produces an output vector based on the input vector and the feature vector. In one implementation, the neural network is a recurrent neural network. Also described herein are various applications of the system, including a machine translation application.", "Feature-Augmented Neural Networks and Applications of Same "]
["In a speech recognition system, deep neural networks (DNNs) are employed in phoneme recognition. While DNNs typically provide better phoneme recognition performance than other techniques, such as Gaussian mixture models (GMM), adapting a DNN to a particular speaker is a real challenge. According to at least one example embodiment, speech data and corresponding speaker data are both applied as input to a DNN. In response, the DNN generates a prediction of a phoneme based on the input speech data and the corresponding speaker data. The speaker data may be generated from the corresponding speech data.", "Method and Apparatus for Speech Recognition Using Neural Networks with Speaker Adaptation "]
["A neural network includes an electronic synapse array of multiple digital synapses interconnecting a plurality of digital electronic neurons. Each synapse interconnects an axon of a pre-synaptic neuron with a dendrite of a post-synaptic neuron. Each neuron integrates input spikes and generates a spike event in response to the integrated input spikes exceeding a threshold. A decoder receives spike events sequentially and transmits the spike events to selected axons in the synapse array. An encoder transmits spike events corresponding to spiking neurons. A controller coordinates events from the synapse array to the neurons, and signals when neurons may compute their spike events within each time step, ensuring one-to-one correspondence with an equivalent software model. The synapse array includes an interconnecting crossbar that sequentially receives spike events from axons, wherein one axon at a time drives the crossbar, and the crossbar transmits synaptic events in parallel to multiple neurons.", "Low-power event-driven neural computing architecture in neural networks "]
["A cortronic neural network defines connections between neurons in a number of regions using target lists, which identify the output connections of each neuron and the connection strength. Neurons are preferably sparsely interconnected between regions. Training of connection weights employs a three stage process, which involves computation of the contribution to the input intensity of each neuron by every currently active neuron, a competition process that determines the next set of active neurons based on their current input intensity, and a weight adjustment process that updates and normalizes the connection weights based on which neurons won the competition process, and their connectivity with other winning neurons.", "Cortronic neural networks with distributed processing "]
["Event-based updates in artificial neuron networks may be implemented. An internal event may be defined in order to update incoming connections of a neuron. The internal event may be triggered by an external signal and/or internally by the neuron. A reinforcement signal may be used to trigger an internal event of a neuron in order to perform synaptic updates without necessitating post-synaptic response. An external event may be defined in order to deliver response of the neuron to desired targets. The external and internal events may be combined into a composite event configured to effectuate connection update and spike delivery to post-synaptic target. The scope of the internal event may comprise the respective neuron and does not extend to other neurons of the network. Conversely, the scope of the external event may extend to other neurons of the network via, for example, post-synaptic spike delivery.", "Apparatus and methods for implementing event-based updates in spiking neuron networks "]
["Methods are provided for developing medical diagnostic tests using decision-support systems, such as neural networks. Patient data or information, typically patient history or clinical data, are analyzed by the decision-support systems to identify important or relevant variables and decision-support systems are trained on the patient data. Patient data are augmented by biochemical test data, or results, where available, to refine performance. The resulting decision-support systems are employed to evaluate specific observation values and test results, to guide the development of biochemical or other diagnostic tests, too assess a course of treatment, to identify new diagnostic tests and disease markers, to identify useful therapies, and to provide the decision-support functionality for the test. Methods for identification of important input variables for a medical diagnostic tests for use in training the decision-support systems to guide the development of the tests, for improving the sensitivity and specificity of such tests, and for selecting diagnostic tests that improve overall diagnosis of, or potential for, a disease state and that permit the effectiveness of a selected therapeutic protocol to be assessed are provided. The methods for identification can be applied in any field in which statistics are used to determine outcomes. A method for evaluating the effectiveness of any given diagnostic test is also provided.", "Method for selecting medical and biochemical diagnostic tests using neural network-related applications "]
["The invention relates to a method for computer-assisted learning of one or more neural networks based on a time series of data comprising first values at subsequent time steps. Furthermore, the invention relates to a method for computer-assisted prediction of values of a time series based on one or more neural networks. According to the invention, a rate of change of an initial time series of data is calculated, thus resulting in derivative values. Those derivative values are subjected to Empirical Mode Decomposition which is a well-known technique. The modes extracted by this Empirical Mode Decomposition as well as some time-lagged values of the time series are used as inputs for the neural networks whereas the output of those networks are future values of the time series which are to be predicted. Those networks are trained and used for prediction of future values of the time series. The invention provides good prediction results because derivative values are used as inputs and also past values are considered as inputs. The invention may be used for predicting values of any time series and is particularly for predicting the dynamics of a technical system, e.g. for predicting vibrations occurring in a technical system.", "A method for computer-assisted learning of one or more neural networks "]
["Systems and methods are disclosed to recognize human action from one or more video frames by performing 3D convolutions to capture motion information encoded in multiple adjacent frames and extracting features from spatial and temporal dimensions therefrom; generating multiple channels of information from the video frames, combining information from all channels to obtain a feature representation for a 3D CNN model; and applying the 3D CNN model to recognize human actions.", "3d convolutional neural networks for automatic human action recognition "]
["The present invention provides a system and method for supervised training of a neural network. A neural network architecture and training method is disclosed that is a modification of an ARTMAP architecture. The modified ARTMAP network is an efficient and robust paradigm which has the unique property of incremental supervised learning. Furthermore, the modified ARTMAP network has the capability of removing undesired knowledge that has previously been learned by the network.", "Supervised training of a neural network "]
["A method and system for designing a neural network classifier and such a neural network for an automated insurance underwriting system and/or its quality assurance is described. While the design method is demonstrated for quality assurance of automated insurance underwriting, it is broadly applicable to diverse decision-making applications in business, commercial, and manufacturing processes. Specifically, multi-class classification problems are solved by decomposing a multi-class classifier into multiple binary-classifiers, which reduces the complexity of the neural network structure, thus reducing the training time and improving the classification performance. Furthermore, the invention also describes a method to incorporate the domain knowledge into the neural network classifier. Both methods work to improve the performance of the classifier.", "System and process for a neural network classification for insurance underwriting suitable for use by an automated system "]
["A system and a method for recognizing patterns comprises a first stage forxtracting features from inputted patterns and for providing topological representations of the characteristics of the inputted patterns and a second stage for classifying and recognizing the inputted patterns. The first stage comprises two one-layer neural networks and the second stage comprises a feedforward two-layer neural network. Supplying signals representative of a set of inputted patterns to the input layers of the first and second neural networks, training the first and second neural networks using a competitive learning algorithm, and generating topological representations of the input patterns using the first and second neural networks The method further comprises providing a third neural network for classifying and recognizing the inputted patterns and training the third neural network with a back-propagation algorithm so that the third neural network recognizes at least one interested pattern.", "Hybrid neural network for pattern recognition "]
["An method and apparatus for extracting an interpretable, meaningful, and concise rule set from neural networks is presented. The method involves adjustment of gain parameter, \u03bb and the threshold, Tj for the sigmoid activation function of the interactive-or operator used in the extraction/development of a rule set from an artificial neural network. A multi-stage procedure involving coarse and fine adjustment is used in order to constrain the range of the antecedents of the extracted rules to the range of values of the inputs to the artificial neural network. Furthermore, the consequents of the extracted rules are provided based on degree of membership such that they are easily understandable by human beings. The method disclosed may be applied to any pattern recognition task, and is particularly useful in applications such as vehicle occupant sensing and recognition, object recognition, gesture recognition, and facial pattern recognition, among others.", "Fuzzy expert system for interpretable rule extraction from neural networks "]
["Deep recurrent neural networks applied to speech recognition. The deep recurrent neural networks (RNNs) are preferably implemented by stacked long short-term memory bidirectional RNNs. The RNNs are trained using end-to-end training with suitable regularization.", "System and method for speech recognition using deep recurrent neural networks "]
["A neural network content-addressable error-correcting memory system is disclosed including a plurality of hidden and visible processing units interconnected via a linear interconnection matrix. The network is symmetric and all self-connections are not present. All connections between processing units are present, except those connecting hidden units to other hidden units. Each visible unit is connected to each other visible unit and to each hidden unit. A mean field theory learning and retrieval algorithm is also provided. Bit patterns or code words are stored in the network via the learning algorithm. The retrieval algorithm retrieves error-corrected bit patterns in response to noisy or error-containing input bit patterns.", "Neural-network content-addressable memory "]
["A method of training an artificial neural network uses a first computer configured as a plurality of interconnected neural units arranged in a network. A neural unit has a first subunit and a second subunit. The first subunit has first inputs and a corresponding first set of variables for operating upon the first inputs to provide a first output during a forward pass. The first set of variables can change in response to feedback representing differences between desired network outputs and actual network outputs. The second subunit has a plurality of second inputs, and a corresponding second set of variables for operating upon the second inputs to provide a second output. The second set of variables can change in response to differences between desired network outputs for selected network inputs and actual network outputs. The computer provides an activating variable representing the difference between current second output and previous second outputs. The activating variable is added to the feedback to accelerate the change of said first set of variables. A second computer is configured as a plurality of interconnected neural units arranged in a network. The network is functionally equivalent to the network of the first computer in a forward pass when provided with sets of values corresponding to each converged first set of variables of the first computer.", "Neural network training tool "]
["A neural network is trained using a training neural network having the same topology as the original network but having a differential network output and accepting also differential network inputs. This new training method enables deeper neural networks to be successfully trained by avoiding a problem occuring in conventional training methods in which errors vanish as they are propagated in the reverse direction through deep networks. An acceleration in convergence rate is achieved by adjusting the error used in training to compensate for the linkage between multiple training data points.", "Training a neural network using differential input "]
["A dynamically stable associative learning neural network system includes, in its basic architectural unit, at least one each of a conditioned signal input, an unconditioned signal input and an output. Interposed between input and output elements are \"patches,\" or storage areas of dynamic interaction between conditioned and unconditioned signals which process information to achieve associative learning locally under rules designed for application-related goals of the system. Patches may be fixed or variable in size. Adjustments to a patch radius may be by \"pruning\" or \"budding.\" The neural network is taught by successive application of training sets of input signals to the input terminals until a dynamic equilibrium is reached. Enhancements and expansions of the basic unit result in multilayered (multi-subnetworked) systems having increased capabilities for complex pattern classification and feature recognition.", "Dynamically stable associative learning neural network system "]
["An adaptive filtering neural network classifier for classifying input signals, includes a neural network and one or more adaptive filters for receiving input analog signals to be classified and generates inputs for the classifier. Each adaptive filter is characterized as having a predetermined number of operating parameters. An analog to digital converter converts each input signal into a digital signal before input to the neural network. The neural network processes each digital signal to generate therefrom a plurality of weighted output signals in accordance with the type of network implemented. One of the weighted output signals represents a class for the input signal, and an error signal representing a difference between the weighted output signals and a predetermined desired output is also generated by the network. A control device responsive to the error signal generates a further set of operating filter parameters for input to each of the adaptive filters to change the operating response thereof and minimize the error signal.", "Adaptive filtering neural network classifier "]
[null, "Neural networks arrangement for the determination of a substance dosage to administer to a patient "]
["Feed forward neural network models for associative content addressable memory utilize a first level matrix of resistor connections to store words and compare addressing cues with the stored words represented by connections of unit resistive value, and a winner-take-all circuit for producing a unary output signal corresponding to the word most closely matched in the first matrix. The unary output signal is converted to a binary output code, such as by a suitable matrix. Cues are coded for the address input as binary 1=+V, binary 0=-V, and unknown =0V. Two input amplifiers are employed with two input conductors for each input bit position, one noninverting and the other inverting, so that the winner-take-all circuit at the output of the first matrix may be organized to select the highest number of matches with stored words as the unary output signal. By inverting the cues at the input to the first matrix, and inverting the output of the first level matrix, the effect of resistor value imprecision in the first matrix is virtually obviated. By space coding, the first and second matrices may be expanded into multiple sets of matrices, each with its own winner-take-all circuit for producing unary output signals applied from the first set to the second set of matrices. The output conductors of the second set of matrices are grouped to provide a sparse output code that is then converted to a binary code corresponding to the word recalled.", "Feed forward neural network for unary associative memory "]
["A plant (72) is operable to receive control inputs c(t) and provide an output y(t). The plant (72) has associated therewith state variables s(t) that are not variable. A control network (74) is provided that accurately models the plant (72). The output of the control network (74) provides a predicted output which is combined with a desired output to generate an error. This error is back propagated through an inverse control network (76), which is the inverse of the control network (74) to generate a control error signal that is input to a distributed control system (73) to vary the control inputs to the plant (72) in order to change the output y(t) to meet the desired output. The control network (74) is comprised of a first network NET 1 that is operable to store a representation of the dependency of the control variables on the state variables. The predicted result is subtracted from the actual state variable input and stored as a residual in a residual layer (102). The output of the residual layer (102) is input to a hidden layer (108) which also receives the control inputs to generate a predicted output in an output layer (106). During back propagation of error, the residual values in the residual layer (102) are latched and only the control inputs allowed to vary.", "Residual activation neural network "]
["A speech recognition method according to the present invention uses distances calculated through a variance weighting process using covariance matrixes as the local distances (prediction residuals) between the feature vectors of input syllables/sound elements and predicted vectors formed by different statuses of reference neural prediction models (NPM's) using finite status transition networks. The category to minimize the accumulated value of these local distances along the status transitions of all the prediction models is figured out by dynamic programming, and used as the recognition output. Learning of the reference prediction models used in this recognition method is accomplished by repeating said distance calculating process and the process to correct the parameters of the different statuses and the covariance matrixes of said prediction models in the direction of reducing the distance between the learning patterns whose category is known and the prediction models of the same category as this known category, and what have satisfied prescribed conditions of convergence through these calculating and correcting processes are determined as reference pattern models.", "Speech recognition by neural network adapted to reference pattern learning "]
["An adaptive distance calculating neural network classifier chip accepts high dimensionality input pattern vectors with up to 256 5-bit elements per vector and compares the input vector with up to 1024 prototype vectors stored on-chip by calculating the distance between the input vector and each of the prototype vectors. The classifier further provides for identifying up to 64 classes to which the prototype vectors belong. If the distance between input and prototype vector is less than a programmable threshold distance, the prototype fires and the class to which it belongs is identified. If prototype vectors belonging to more than one class fire, a probabilistic model based on Parzen windows may be invoked to resolve the classification by providing the relative probabilities of various class membership. The classifier chip is trainable by supplying appropriate training vectors and associated class membership. Distance and probability parameters are generated during training and are stored for use in the classification mode. Incremental training is also provided so that additional prototypes may be added to an existing base. In order to extend the classifier capacity, multichip operation is provided under the supervision of a system administrator controller/CPU.", "Distance calculating neural network classifier chip and system "]
["A voice recognition apparatus capable of recognizing any word utterance by using a neural network, the apparatus includes a unit for inputting an input utterance and for outputting compressed feature variables of the input utterance a unit using a neural network and connected to the input unit for receiving the compressed feature variables output from the input unit and for outputting a value corresponding to a similarity between the input utterance and words to be recognized. The neural network unit has a first unit for outputting a value which corresponds to a similarity in partial phoneme series of a specific word among vocabularies to be recognized with respect to the input utterance. The neural network also has a second unit connected to the first unit for receiving all of the values output from the first unit and for outputting a value corresponding to a similarity in the specific word with respect to the input utterance. And the neural network also has a third unit connected to the second unit for receiving all of the values output from the second unit and for outputting a value corresponding to a classification of voice recognition in which the input utterance belongs.", "Speech recognition system with neural network "]
["Input parameters which correspond to operational flight data of an aircraft within a predetermined flight domain, are defined through measured variable state parameters generated during aircraft flight, utilizing a neural network trained by exemplars corresponding to such variable state parameters and reference information on the aircraft for data processing of real time values of the variable state parameter measurement to calculate values of the input parameters and provide a corresponding output as a reliable estimate of aircraft flight data such as airspeed, sideslip and angle of attack.", "Neural network system for estimation of aircraft flight data "]
["A cardiac device system for implementing a cardiac device having adaptive treatment therapies utilizing a neural network based learning engine includes an implantable cardiac device module and an external data processing system for specifying the operating characteristics of the cardiac device module. Both the cardiac device module and the external processing system possess an artificial neural network to specify the operation of the cardiac device module as it provides adaptive treatment therapies. The external data processing system includes a complete neural network module that trains and validates the operation of the neural network to match the optimal treatment options with a received set of collected patient data. In contrast, a runtime neural network module that only provides real time operation of the neural network using collected patient data is located within the cardiac device module. The cardiac device module and the external processing module communicate with each other to pass collected patient data from the cardiac device module to the external processing system when the operation of the neural network is to be updated. The cardiac device module and the external processing module also communicate with each other to pass operating coefficients for the neural network back from the external processing system to the cardiac device module once these coefficients are updated.", "Neural network based learning engine to adapt therapies "]
["A gas turbine control system includes a controller that is coupled to actuator systems that govern operation of the gas turbine. The controller includes a processor for generating respective actuator control signals in correspondence with a plurality of turbine operating condition signals; the controller includes at least one neural network estimator that is trained to generate an estimated turbine operating condition signal. The neural network estimator typically has one or more hidden neuron layers that are coupled together in a feedforward structure, a recurrent neural network architecture. The estimated turbine operating condition signal generated by the neural network estimator typically, but not necessarily, represents a turbine internal cycle operating parameter for which the turbine has no corresponding operating parameter sensor.", "Controller with neural network for estimating gas turbine internal cycle parameters "]
["A neural network is used to recognize characters from a character set. Based upon the character recognized, a smaller neural network is used for verification of the character recognized. The smaller neural network is trained to recognize only a single character of the set and provides a \"yes\" or \"no\" type verification of correct identification of the character.", "Neural network for character recognition and verification "]
["A mapping circuit includes a linear circuit for outputting a signal which is linearly changed with respect to its input, a non-linear circuit for outputting a signal which is non-linearly changed with respect to its input, and an adder for summing the output signals of the linear and non-linear circuits and an external input signal. A chaotic neuron circuit using the mapping circuit has a simple structure and more precise chaos characteristics. A chaotic neural network can thus be formed by the serial and/or parallel interconnection of a plurality of chaotic neuron circuits, wherein the weight of each neuron is controlled.", "Chaotic neural circuit and chaotic neural network using the same "]
["A system that couples distributed power generators together as a collective unit for the purposes of selling or purchasing energy from the electrical power grid. The apparatus includes a charge/discharge controller and an adaptive controller. The charge/discharge controller transfers energy generated by the plurality of distributed power generators to the power grid. The adaptive controller directs when the charge/discharge controller transfers energy generated by at least one of the plurality of distributed power generators to the electrical grid.", "Distributed energy neural network integration system "]
["This application discloses a system that optimizes a neural network by generating all of the discrete weights for a given neural node by creating a normalized weight vector for each possible weight combination. The normalized vectors for each node define the weight space for that node. This complete set of weight vectors for each node is searched using a direct search method during the learning phase to optimize the network. The search evaluates a node cost function to determine a base point from which a pattern more within the weight space is made. Around the pattern mode point exploratory moves are made which are cost function evaluated. The pattern move is performed by eliminating from the search vectors with lower commonality.", "Digital neural network with discrete point rule space "]
["High-speed, analog, fully-parallel and asynchronous building blocks are cascaded for larger sizes and enhanced resolution. A hardware-compatible algorithm permits hardware-in-the-loop learning despite limited weight resolution. A computation-intensive feature classification application has been demonstrated with this flexible hardware and new algorithm at high speed. This result indicates that these building block chips can be embedded as application-specific-coprocessors for solving real-world problems at extremely high data rates.", "Cascaded VLSI neural network architecture for on-line learning "]
["A method of automatic labeling using an optimum-partitioned classified neural network includes searching for neural networks having minimum errors with respect to a number of L phoneme combinations from a number of K neural network combinations generated at an initial stage or updated, updating weights during learning of the K neural networks by K phoneme combination groups searched with the same neural networks, and composing an optimum-partitioned classified neural network combination using the K neural networks of which a total error sum has converged; and tuning a phoneme boundary of a first label file by using the phoneme combination group classification result and the optimum-partitioned classified neural network combination, and generating a final label file reflecting the tuning result.", "Method of setting optimum-partitioned classified neural network and method and apparatus for automatic labeling using optimum-partitioned classified neural network "]
["Apparatus for realizing a neural network of a complex structure, such as the Neocognitron, in a neural network processor comprises processing elements corresponding to the neurons of a multilayer feed-forward neural network. Each of the processing elements comprises an MOS analog circuit that receives input voltage signals and provides output voltage signals. The MOS analog circuits are arranged in a systolic array.", "Neural network, processor, and pattern recognition apparatus "]
["A self healing ad hoc communications network and method of training for and healing the network. The network includes wireless devices or nodes that include a neural network element and the ad hoc network operates as a neural network. Some of the nodes are designated as healing nodes that are identified during network training and are strategically located in the network coverage area. Whenever one group of nodes loses connection with another a healing node may reposition itself to reconnect the two groups. Thus, the network can maintain connectivity without constraining node movement.", "Neural network-based mobility management for healing mobile ad hoc radio networks "]
["An image processing system, which operates on an input image data stream consisting of successive multi-level values, effecting a plurality of respectively different types of image data processing in accordance with a plurality of different categories of the input image data, utilizes a neural network to assign each datum to a specific category, with resultant output signals from the neural network being used to select the appropriate type of image data processing for that datum.", "Image processing system utilizing neural network for discrimination between text data and other image data "]
["A method for forming a module (hydrodynamic or thermodynamic for example) intended for real-time simulation of the flow mode, at any point of a pipe, of a multiphase fluid stream comprising at least a liquid phase and at least a gas phase. The method comprises using a modelling system based on non-linear neural networks each having inputs for structure parameters and physical quantities, outputs where quantities necessary for estimation of the flow mode are available, and at least one intermediate layer. The neural networks are determined iteratively to adjust to the values of a learning base with predetermined tables connecting various values obtained for the output data to the corresponding values of the input data. A learning base suited to the imposed operating conditions is used and optimized neural networks best adjusted to the imposed operating conditions are generated.", "Method for forming an optimized neural network module intended to simulate the flow mode of a multiphase fluid stream "]
["A neural network is trained with a general set of data to function as a general model of a machine or process with local condition inputs set equal to zero. The network is then retrained or receives additional training on an extentd data set containing the general set of data, characterized by zero values for the local condition inputs, and data on specific local conditions, characterized by non-zero values for the local condition inputs. The result is a trained neural network which functions as a general model when the inputs for the local conditions inputs are set equal to zero, and which functions as a model of some specific local condition when the local condition inputs match the encoding of the some local data set contained within the training data. The neural network has an architecture and a number of neurons such that its functioning as the local model is partially dependent upon its functioning as the general model. This trained neural network is combined with sensors, actuators, a control and communications computer and with a user interface to function as combine control system.", "Control system with neural network trained as general and local models "]
["Optimization of a FNN (FNN)-based controller is described. The optimization includes selecting which input signals will be used by the FNN to compute a desired control output. Output parameters are identified and computed by fuzzy reasoning using a neural network. Adjustment of fuzzy rules and/or membership functions for the FNN is provided by a learning process. The learning process includes selecting candidate input data signals (e.g. selecting candidate sensor signals) as inputs for the FNN. The input data is categorized and coded into a chromosome structure for use by a genetic algorithm. The genetic algorithm is used to select an optimum chromosome (individual). The optimum chromosome specifies the number(s) and type(s) of input data signals for the FNN so as to optimize the operation of the FNN-based control system. The optimized FNN-based control system can be used in many control environments, including control of an internal combustion engine.", "Method for optimization of a fuzzy neural network "]
["A synaptic array according to the present invention comprises a plurality of electrically-adaptable elements. Electrons may be placed onto and removed from a floating node in each electrically adaptable element associated with at least one MOS insulated gate field effect transistor, usually the gate of the transistor, in an analog manner, by application of first and second electrical control signals generated in response to an adapt signal. The inputs to all synaptic elements in a row are connected to a common row input line. Adapt inputs to all synaptic elements in a column are connected together to a common column adapt line. The current supplied to all amplifiers in a column is commonly provided by a sense line. In order to adapt the synaptic elements in the M row by N column matrix of the present invention, the voltages to which a given column n of the matrix is to be adapted are placed onto the input voltage lines, and the synaptic elements in column n are then simultaneously adapted by assertion of an adapt signal on the adapt line for column n. The vectors of input voltages for adapting successive columns may be placed sequentially onto the row input voltage lines and used to adapt the columns of synaptic elements by assertion of the adapt signals on the appropriate column adapt lines until the entire array is electrically adapted. After each synaptic element has been adapted, the current flowing through it will be maximized when the voltage at the input of the synaptic element equals the voltage to which the synaptic element has been adapted. An electrically adaptable winner-take-all circuit has its inputs connected to the column-sense lines of the array.", "Electrically adaptable neural network with post-processing circuitry "]
["A virtual-zero architecture is intended for use in a single instruction stream, multiple data stream (SIMD) processor which includes an input bus, an input unit, manipulation units, an output unit and an output bus. The virtual-zero architecture includes a memory unit (40) for storing data, an arithmetic unit (42) for mathematically operating on the data, a memory address generation unit (32) and an adder for computing a next memory address. The memory address generation unit (32) includes an address register (34) in the memory unit for identifying the address of a particular data block, a counter (38) for counting the number of memory addresses in a particular data block, and a rotation register (36) for providing a data-void address in the memory unit if and only if all of the entries in the data block are zero. The memory (40) and the address (32) units provide zero-value data blocks to the arithmetic unit (44) to simulate the data block having the data-void address during processing. The architecture may also be used to selectively handle input to a system.", "Neural network using virtual-zero "]
["A preprocessing system for preprocessing input data to a neural network includes a training system for training a model (20) on data from a data file (10). The data is first preprocessed in a preprocessor (12) to fill in bad or missing data and merge all the time values on a common time scale. The preprocess operation utilizes preprocessing algorithms and time merging algorithms which are stored in a storage area (14). The output of the preprocessor (12) is then delayed in a delay block (16) in accordance with delay settings in storage area (18). These delayed outputs are then utilized to train the model (20), the model parameter is then stored in a storage area (22) during run time, a distributed control system (24) outputs the data to a preprocess block (34) and then preprocesses data in accordance with the algorithms in storage area (14). These outputs are then delayed in accordance with a delay block (36) with the delay settings (18). The output of the delay block (36) comprises inputs to a run time system model (26) which is built to provide a representation of the system in accordance with the model parameters in the storage area (22). A predicted control output or predicted control inputs are then generated. The control input is input back to the DCS (24).", "Method and apparatus for preprocessing input data to a neural network "]
["An apparatus for printing signal correction and printer operation control, for use in applications such as color copiers, utilizes a neural network to convert input image signals, derived for example by scanning and analyzing an original image, into printing density signals which are supplied to a printer. In addition, a detection signal expressing at least one internal environmental condition of the printer, such as temperature, is inputted to the neural network, so that the output printing density signals are automatically compensated for changes in internal environment of the printer.", "Printing signal correction and printer operation control apparatus utilizing neural network "]
["Each processing element has a number of weights for each input connection. These weights are coefficients of a polynomial equation. The use of quadratic nodes permits discrimination between body pixel and edge pixels, in which an intermediate value is present, using a grey scale image. In the training method of the present invention, the middle layer is initially one leaf node which is connected to each output node. The contribution of each leaf node to the total output error is determined and the weights of the inputs to the leaf nodes are adjusted to minimize the error. The leaf node that has the best chance of improving the total output error is then \"converted\" into a branch node with two leaves. A branch node selected from a pool of trial branch nodes is used to replace the chosen leaf node. The trial branch nodes are then trained by gradient training to optimize the branch error function. From the set of trial branch nodes, the best performing node is selected and is substituted for the previously-selected leaf node. Two new leaf nodes are then created from the newly-substituted best-performing-branch node. A leaf node is accepted or rejected based upon the number of times it was activated related to the correctness of the classification. Once a leaf node is rejected, it is eliminated from any further operation, thereby minimizing the size of the network. Integer mathematics can be generated within the network so that a separate floating point coprocessor is not required.", "Neural network for improved classification of patterns which adds a best performing trial branch node to the network "]
["A component machine testing technique is provided that performs diagnostic analysis on a vibration signal of the component machine that has been separated from power and load machine background noise in a first neural network. The diagnostic analysis, with operator direction through an interactive interface, uses a second neural network in performing a series of diagnostic operations followed by archival of any experience acquired in the testing operation being performed.", "Component machine testing using neural network processed vibration data analysis "]
["The learning of an associative memory suitable for the connectionist model which can deal with the patterns having the non-random frequencies of the appearances or the non-random correlations. In this invention, the learning of the associative memory in a form of a neural network, in which a plurality of nodes having activation values are connected by a plurality of links having link weight values, is achieved by entering a plurality of learning patterns sequentially, where each learning pattern has a plurality of elements in correspondence with the nodes, calculating an energy E of the entered learning pattern, determining a learning amount \u03b4 for the entered learning pattern according to a difference between the calculated energy E and a predetermined reference energy level Eth, and updating the link weight values of the links according to the entered learning pattern and the determined learning amount \u03b4.", "Learning of associative memory in form of neural network suitable for connectionist model "]
["This invention is an adaptive neuron for use in neural network processors. The adaptive neuron participates in the supervised learning phase of operation on a coequal basis with the synapse matrix elements by adaptively changing its gain in a similar manner to the change of weights in the synapse io elements. In this manner, training time is decreased by as much as three orders of magnitude.", "Neural network with dynamically adaptable neurons "]
["An artificial neural network (ANN) decoding system decodes a convolutionally-encoded data stream at high speed and with high efficiency. The ANN decoding system implements the Viterbi algorithm and is significantly faster than comparable digital-only designs due to its fully parallel architecture. Several modifications to the fully analog system are described, including an analog/digital hybrid design that results in an extremely fast and efficient Viterbi decoding system. A complexity and analysis shows that the modified ANN decoding system is much simpler and easier to implement than its fully digital counterpart. The structure of the ANN decoding system of the invention provides a natural fit for VLSI implementation. Simulation results show that the performance of the ANN decoding system exactly matches that of an ideal Viterbi decoding system.", "Artificial neural network viterbi decoding system and method "]
["A pattern searching method using neural networks and correlation. This method combines the quickness and adaptiveness of neural networks with the accuracy of the mathematical correlation approach. Images are divided into small sub-images which are presented to the trained neural network. Sub-images that may contain the pattern or partial pattern are selected by the neural network. The neural network also provides the approximate location of the pattern, therefore the selected sub-images can be adjusted to contain the complete pattern. Desired patterns can be located by measuring the new sub-images' correlation values against the reference models in a small area. Experiments show that this superior method is able to find the desired patterns. Moreover, this method is much faster than traditional pattern searching methods which use only correlation.", "Pattern searching method using neural networks and correlation "]
["A temperature control system utilizes detected occupancy and activity levels to automatically condition a response of an HVAC system to a difference between a setpoint temperature and an actual temperature within a zone. The illustrated example includes an infrared sensor that provides at least one signal indicating the activity and occupancy levels in the zone. A neural network processes the sensor signal to provide an indication of the occupancy and activity levels to a controller. The controller automatically adjusts at least one control parameter of the HVAC system to compensate for changes in the occupancy or activity levels that would affect the temperature comfort setting in the zone.", "Temperature control strategy utilizing neural network processing of occupancy and activity level sensing "]
["An architecture, systems and methods for a scalable artificial neural network, wherein the architecture includes: an input layer; at least one hidden layer; an output layer; and a parallelization subsystem configured to provide a variable degree of parallelization to the input layer, at least one hidden layer, and output layer. In a particular case, the architecture includes a back-propagation subsystem that is configured to adjust weights in the scalable artificial neural network in accordance with the variable degree of parallelization. Systems and methods are also provided for selecting an appropriate degree of parallelization based on factors such as hardware resources and performance requirements.", "Architecture, system and method for artificial neural network implementation "]
["A method and system for natural language processing including using a trained neural network having a plurality of baseline nodes. A connection weight between any selected pair of baseline nodes is described by text strings within the selected pair of nodes. A plurality of text messages are received from a preselected source. For each received message, a non-baseline node associated with a selected one of the baseline nodes is created. A connection weight between any non-baseline node and the associated baseline node is described by the text string within the baseline node and the received text message.", "Self-organizing neural network for plain text categorization "]
["A synthetic neural network having a plurality of neuronal elements arranged in an input layer, an output layer, and a hidden layer between the input layer and the output layer. The network has a first plurality of synaptic weighting elements interconnecting the neuronal elements of the input layer with the neuronal elements of the hidden layer, and a second plurality of synaptic weighting elements interconnecting the neuronal elements of the hidden layer with the neuronal elements of the output layer. The improvement involves the synaptic weighting elements in the synthetic neural network being in the form of a silicon dioxide film derived from a hydrogen silsesquioxane resin. Such a silicon dioxide film is characterized by a jV curve which includes both linear and non-linear regions.", "Neural networks containing variable resistors as synapses "]
["The present invention relates to a technique in the form of an exemplary computer vision system for detecting the orientation of text or features on an object of manufacture. In the present system, an image of the features or text is used to extract lines using horizontal bitmap sums, and then individual symbols using vertical bitmap sums, using thresholds with each of the sums. The separated symbols are then appropriately trimmed and sealed to provide individual normalized symbols. A Decision Module comprising a Feed-Forward Neural network and a sequential decision arrangement determines the \"up\", \"down\" or \"indeterminate\" orientation of the text after a variable number of symbols have been processed. The system can then compare the determined orientation with a database to further determine if the object is in the \"right-side up\" \"upside down\" or \"indeterminate\" orientation.", "Technique for object orientation detection using a feed-forward neural network "]
["A data communication apparatus comprises: means for dividing data to be transmitted into a plurality of blocks and extracting the data from each block; a first multi-layered neural network of three or more layers which has weighting coefficients to output the same data as the input data for the data extracted from each block and which can output data from an intermediate layer; the transmission data extracted from each block being inputted to the first neural network and outputted from the intermediate layer; means for encoding the transmission data which is outputted from the intermediate layer of the first neural network and, thereafter, transmitting; means for receiving and decoding the transmitted data; a second multi-layered neural network of three or more layers which has the same weight coefficients as those of the first neural network and can input data from an intermediate layer; the decoded data of each block being inputted to the second neural network and outputted from an output layer; and means for restoring the data on the basis of the output data from the output layer of the second neural network.", "Data communication method and apparatus using neural-networks "]
["In a parallel multi-value neural network having a main neural network 16 and a sub neural network 18 coupled with the main neural network 16 in parallel for an input signal, the main neural network 16 is trained with a training input signal by using a main multi-value teacher signal, and the sub neural network is successively trained with the training input signal by using multi-value errors between a multi-value output signal of the main neural network 16 derived through a multi-value threshold means 17 and the main multi-value teacher signal, so as to compensate the multi-value errors involved in the multi-value output signal of the main neural network 16 by the multi-value output signal of the sub neural network 18 derived through a multi-value threshold means 19. A desired multi-value output signal of the parallel multi-value neural network 15 is obtained by adding in modulo the multi-value output signals of both the neural networks through a multi-value modulo adder 20.", "Parallel multi-value neural networks "]
["A network architecture for the programmable emulation of large artificial neural networks ANN having digital operation employs a plurality L of neuron units of identical structure, each equipped with m neurons, the inputs (E) thereof being connected to network inputs (EN) multiplied or branching via individual input registers (REGE). The outputs (A) of the neuron units are connectable to network outputs (AN) at different points in time via individual multiplexers (MUX) and individual output registers (REGA) and the neuron units have individual auxiliary inputs via which signals can be supplied to them that represent weighting values (W) for weighting the appertaining neural connections and represent thresholds (0) for weighting input signals.", "Network architecture for the programmable emulation of artificial neural networks having digital operation "]
["Deep convolutional neural networks receive local and global representations of images as inputs and learn the best representation for a particular feature through multiple convolutional and fully connected layers. A double-column neural network structure receives each of the local and global representations as two heterogeneous parallel inputs to the two columns. After some layers of transformations, the two columns are merged to form the final classifier. Additionally, features may be learned in one of the fully connected layers. The features of the images may be leveraged to boost classification accuracy of other features by learning a regularized double-column neural network.", "Image assessment using deep convolutional neural networks "]
["Disclosed are a system and method for determining the pose (translation, rotation, and scale), or position and orientation, of a model object that best matches a target object located in image data. Through an iterative process small adjustments are made to the original position and orientation of the model object until it converges to a state that best matches the target object contained in the image data. Edge data representative of edges of the target object and edge data representative of the model object are processed for each data point in the model object relative to each point in the target object to produce a set of minimum distance vectors between the model object and the target object. A neural network estimates translation, rotation, and scaling adjustments that are to be made to the model object. Pose of the model object is adjusted relative to the target object based upon the estimated translation, rotation, and scaling adjustments provided by the neural network. Iterative calculation of the minimum distance vectors, estimation of the translation, rotation, and scaling adjustments, and adjustment of the position and orientation of the model object is adapted to reposition the model object until it substantially overlays the target object. Final position of the model object provides an estimate of the position and orientation of the target object in the digitized image.", "Position and orientation estimation neural network system and method "]
["Methods are provided for downhole sensing and flow control utilizing neural networks. In a described embodiment, a temporary sensor is positioned downhole with a permanent sensor. Outputs of the temporary and permanent sensors are recorded as training data sets. A neural network is trained using the training data sets. When the temporary sensor is no longer present or no longer operational in the well, the neural network is capable of determining the temporary sensor's output in response to the input to the neural network of the permanent sensor's output.", "Downhole sensing and flow control utilizing neural networks "]
["A neuron unit processes a plurality of input signals and outputs an output signal which is indicative of a result of the processing. The neuron unit includes input lines for receiving the input signals, a forward process part including a supplying part for supplying weight functions and an operation part for carrying out an operation on each of the input signals using one of the weight functions and for outputting the output signal, and a self-learning part including a generating part for generating new weight functions based on errors between the output signal of the forward process part and teaching signals and a varying part for varying the weight functions supplied by the supplying part of the forward process part to the new weight functions generated by the generating part.", "Neuron unit, neural network and signal processing method "]
["A temporal learning neural network includes a plurality of temporal learning neural processing elements and an input/output control section. Each element includes a calculation device and a learning device. The calculation device includes an input memory section and a response calculation circuit. The learning device includes a learning processing circuit and a history evaluation circuit. The calculation circuit calculates a sum of a total summation value of a product of input values and connection efficacies, and an internal potential, compares the sum with a predetermined threshold value, outputs a 1 or 0 signal depending on the comparison and substitutes internal potential of a next time for the sum. The processing circuit receives an input history evaluation value when the calculation circuit has produced an output 1 signal which strengthens, weakens or leaves unchanged the connection efficacies depending on the comparison. The evaluation circuit obtains an input history value, compares the obtained input history value with the learning threshold value, generates an evaluation signal and distributes the evaluation signal to the input memory section. The input/output control section is provided with input terminals and output terminals, sends signals input from the calculation circuit and evaluation circuit to the input memory section, receives signals output from the calculation circuit and evaluation circuit, and effects communication with each of the processing elements. This process is an input temporal associative learning process.", "Temporal learning neural network "]
["An artificial neural network having analog circuits for simultaneous parallel processing using individually variable synaptic input weights. The processing is implemented with a circuit adapted to vary the weight, which may be stored in a metal oxide field effect transistor, for teaching the network by addressing from outside the network or for Hebbian or delta rule learning by the network itself.", "Artificial neural network implementation "]
["A method and system for labeling a selected word of a sentence using a deep neural network includes, in one exemplary embodiment, determining an index term corresponding to each feature of the word, transforming the index term or terms of the word into a vector, and predicting a label for the word using the vector. The method and system, in another exemplary embodiment, includes determining, for each word in the sentence, an index term corresponding to each feature of the word, transforming the index term or terms of each word in the sentence into a vector, applying a convolution operation to the vector of the selected word and at least one of the vectors of the other words in the sentence, to transform the vectors into a matrix of vectors, each of the vectors in the matrix including a plurality of row values, constructing a single vector from the vectors in the matrix, and predicting a label for the selected word using the single vector.", "Deep Neural Networks and Methods for Using Same "]
["A neural network has been developed that uses first-arrival energy to predict the characteristics of impending earthquake seismograph signals. The propagation of ground motion energy through the earth is a highly nonlinear function. This is due to different forms of ground motion as well as to changes in the elastic properties of the media throughout the propagation path. The neural network is trained using seismogram data from earthquakes. Presented with a previously unseen earthquake, the neural network produces a profile of the complete earthquake signal using data from the first seconds of the signal. This offers a significant advance in the real-time monitoring, warning, and subsequent hazard minimization of catastrophic ground motion.", "Real-time neural network earthquake profile predictor "]
["A nonlinear oscillator (10) includes a neural network (12) having at least one output (12a) for outputting a one dimensional vector. The neural network includes a plurality of layers, including an input layer, an output layer, and at least one hidden layer. Each of the layers includes at least one processing element (PE) that is interconnected to processing elements of adjacent layers. The input layer has an input coupled to the at least one output and includes an analog delay line (14) having a plurality of taps each of which outputs a time-delayed sample of the one dimensional output vector. Each of the taps is connected to each one of the processing elements of the at least one hidden layer for providing a time-delayed sample of the one dimensional output vector thereto. The nonlinear oscillator further includes a feedback network (16) that is interposed between the output of the neural network and the input of the input layer for modifying a magnitude and/or a polarity of the one dimensional output vector prior to the sample of the one dimensional output vector being applied to the input of the analog delay line. The analog delay line is capable of being shifted in either a first or a second direction. Connection weights of the neural network are trained on a deterministic sequence of data from a chaotic source or may be a representation of a stochastic process, wherein each of the weights is randomly selected.", "Nonlinear neural network oscillator "]
["A method of accelerating the training of an artificial neural network uses a computer configured as an artificial neural network with a network input and a network output, and having a plurality of interconnected units arranged in layers including an input layer and an output layer. Each unit has a multiplicity of unit inputs and a set of variables for operating upon a unit inputs to provide a unit output in the range positive 1 and negative 1. A plurality of examples are serially provided to the network input and the network output is observed. The computer is programmed with a back propagation algorithm for calculating changes to the sets of variables in response to feedback representing differences between the network output for each example and the desired output. The absolute magnitude of the product of an input and the corresponding output of a unit is calculated. The feedback to that unit is adjusted in response to absolute magnitude so that said feedback is larger with a larger absolute magnitude than with a smaller absolute magnitude.", "Neural networks learning method "]
["A system, software module, and computer program product for performing neural network based data mining that improved performance in model building, good integration with the various databases throughout the enterprise, flexible specification and adjustment of the models being built, and flexible model arrangement and export capability. The software module for performing neural network based data mining in an electronic data processing system comprises: a model setup block operable to receive client input including information specifying a setup of a neural network data mining models, generate the model setup, generate parameters for the model setup based on the received information, a modeling algorithms block operable to select and initialize a neural network modeling algorithm based on the generated model setup, a model building block operable to receive training data and build a neural network model using the training data and the selected neural network modeling algorithm and a model scoring block operable to receive scoring data and generate predictions and/or recommendations using the scoring data and the neural network model.", "Neural network module for data mining "]
["A semiconductor cell for producing an output current that is related to the match between an input vector pattern and a weighting pattern is described. The cell is particularly useful as a synapse cell within a neural network to perform pattern recognition tasks. The cell includes a pair of input lines for receiving a differential input vector element value and a pair of output lines for providing a difference current to a current summing neural amplifier. A plurality of floating gate devices each having a floating gate member are employed in the synapse cell to store charge in accordance with a predetermined weight pattern. Each of the floating gate devices is uniquely coupled to a combination of an output current line and an input voltage line such that the difference current provided to the neural amplifier is related to the match between the input vector and the stored weight.", "EXCLUSIVE-OR cell for neural network and the like "]
["An electronic circuit is disclosed having a sample/hold amplifier connected to an adaptive amplifier. A plurality of such electronic circuits may be configured in an array of rows and columns. An input voltage vector may be compared with an analog voltage vector stored in a row or column of the array and the stored vector closest to the applied input vector may be identified and further processed. The stored analog value may be read out of the synapse by applying a voltage to a read line. An array of the readable synapses may be provided and used in conjunction with a dummy synapse to compensate for an error offset introduced by the operating characteristics of the synapses.", "Dynamic synapse for neural network "]
["A solder paste brick inspection and physical quality scoring system 10 employs a neural network 70 trained with a fuzzified output vector. An image of solder paste bricks 64 on a printed circuit board 12 is acquired by a CCD camera 30. Values of a predetermined set of brick metrics are extracted from the image by a computer 28 and used as a crisp input vector to trained neural network 70. A defuzzifier 76 converts a fuzzy output vector from neural network 70 into a crisp quality score output which can be used for monitoring and process control.", "Neural network solder paste inspection system "]
["A speech recognition system can recognize a plurality of voice data having different patterns. The speech recognition system has a voice recognizing and processing device including a plurality of speech recognition neural networks that have previously learned different voice patterns to recognize given voice data. Each of the speech recognition neutral networks is adapted to judge whether or not input voice data coincides with one of the voice data to be recognized. Each neural network then outputs adaptation judgment data representing the adaptation in speech recognition. A selector responsive to the adaptation judgment data from each of the speech recognition neural networks selects one of the neural networks that has the highest adaptation in speech recognition. An output control device outputs the result of speech recognition from the speech recognition neural network selected by the selector.", "Speech recognition system using neural networks "]
["A television signal processing apparatus includes at least one neural network for processing a signal representing an image. The neural network includes a plurality of perceptrons each of which includes circuitry for weighting a plurality of delayed representations of said signal, circuitry for providing sums of weighted signals provided by said weighting circuitry, and circuitry for processing said sums with a sigmoidal transfer function. The neural network also includes circuitry for combining output signals provided by ones of said perceptrons for providing a processed signal.", "Neural networks as for video signal processing "]
["Highly accurate, reliable optical character recognition is afforded by a layered network having several layers of constrained feature detection wherein each layer of constrained feature detection includes a plurality of constrained feature maps and a corresponding plurality of feature reduction maps. Each feature reduction map is connected to only one constrained feature map in the same layer for undersampling that constrained feature map. Units in each constrained feature map of the first constrained feature detection layer respond as a function of a corresponding kernel and of different portions of the pixel image of the character captured in a receptive field associated with the unit. Units in each feature map of the second constrained feature detection layer respond as a function of a corresponding kernel and of different portions of an individual feature reduction map or a combination of several feature reduction maps in the first constrained feature detection layer as captured in a receptive field of the unit. The feature reduction maps of the second constrained feature detection layer are fully connected to each unit in the final character classification layer. Kernels are automatically learned by constrained back propagation during network initialization or training.", "Hierarchical constrained automatic learning neural network for character recognition "]
["Disclosed is a an integrated circuit method and system for generating a compiler to map a code set to object code capable of being executed on an operating system platform. The integrated circuit is encoded with logic including at least one neural network. The at least one neural network in the integrated circuit is trained to convert the code set to object code. The at least one trained neural network is then used to convert the code set to object code.", "Method and system for converting code to executable code using neural networks implemented in a very large scale integration (VLSI) integrated circuit "]
["The present invention provides an aircraft engine vibration system that provides information about engine health. Embodiments of the present invention monitor for excessive vibration, monitor for bird strike, monitor for ice build up on the fan section, and monitor general engine health. An embodiment of the present invention utilizes neural network architecture for the detection of excessive vibration and ice detection build-up on the fan section of a turbo-fan engine and to monitor engine health through the high-pressure turbine section of the engine.", "Vibration engine monitoring neural network object monitoring "]
["A multiphase flow meter used in conjunction with an electrical submersible pump system in a well bore includes sensors to determine and transmit well bore pressure measurements, including tubing and down hole pressure measurements. The multiphase flow meter also includes at least one artificial neural network device to be used for outputting flow characteristics of the well bore. The artificial neural network device is trained to output tubing and downhole flow characteristics responsive to multiphase-flow pressure gradient calculations and pump and reservoir models, combined with standard down-hole pressure, tubing surface pressure readings, and the frequency applied to the electrical submersible pump motor.", "Multiphase flow meter for electrical submersible pumps using artificial neural networks "]
["Methods of creating and using robust neural network ensembles are disclosed. Some embodiments take the form of computer-based methods that comprise receiving a set of available inputs; receiving training data; training at least one neural network for each of at least two different subsets of the set of available inputs; and providing at least two trained neural networks having different subsets of the available inputs as components of a neural network ensemble configured to transform the available inputs into at least one output. The neural network ensemble may be applied as a log synthesis method that comprises: receiving a set of downhole logs; applying a first subset of downhole logs to a first neural network to obtain an estimated log; applying a second, different subset of the downhole logs to a second neural network to obtain an estimated log; and combining the estimated logs to obtain a synthetic log.", "Ensembles of neural networks with different input sets "]
["The operation of neural networks begins with the initialization of the system with the information to be processed. Presently, this initialization is performed by pinning the system with rather large analog or digital signals representing this information. The problems associated with the high power required for such initialization are eliminated and accuracy is maintained by utilizing a specific set of input points and appropriately positioned switches. In particular, a switch corresponding to each amplifier is introduced, and the initializing data is introduced between the amplifier and this switch.", "Neural networks "]
["An intelligent control system based on an explicit model of cognitive development (Table 1) performs high-level functions. It comprises up to O hierarchically stacked neural networks, Nm, . . . , Nm+(O\u22121), where m denotes the stage/order tasks performed in the first neural network, Nm, and O denotes the highest stage/order tasks performed in the highest-level neural network. The type of processing actions performed in a network, Nm, corresponds to the complexity for stage/order m. Thus N1 performs tasks at the level corresponding to stage/order 1. N5 processes information at the level corresponding to stage/order 5. Stacked neural networks begin and end at any stage/order, but information must be processed by each stage in ascending order sequence. Stages/orders cannot be skipped. Each neural network in a stack may use different architectures, interconnections, algorithms, and training methods, depending on the stage/order of the neural network and the type of intelligent control system implemented.", "Intelligent control with hierarchical stacked neural networks "]
["Speech recognition techniques are employed in a variety of applications and services serving large numbers of users. As such, there is an increasing demand for speech recognition systems with enhanced performance. Specifically, enhanced performance in large vocabulary continuous speech recognition (LVCSR) systems is a market demand. Herein, convolutional neural networks are explored as an alternative speech recognition approach and different CNN architectures are tested. According to at least one example embodiment, a method and corresponding apparatus for performing speech recognition comprise employing a CNN with at least two convolutional layers and at least two fully-connected layers in speech recognition. Using the CNN a textual representation of input audio data may be provided based on output data by the CNN.", "Method and Apparatus for Using Convolutional Neural Networks in Speech Recognition "]
["A systolic array of processing elements (9) is connected to receive weight inputs (21) and multiplexed data inputs (20) for operation in feedforward, partially - or fully-connected neural network mode or in cooperative, competitive neural network mode. Feature vector or two-dimensional image data is retrieved from external data memory and is transformed via input look-up table to input data for the systolic array that performs a convolution with kernal values as weight inputs (21).\nThe convoluted image or neuron outputs from the systolic array are scaled and transformed via output look-up table for storage in the external data memory.", "Two-dimensional systolic array for neural networks, and method "]
["A numeric encoding method and apparatus for neural networks, encodes numeric input data into a form applicable to an input of a neural network by partitioning a binary input into N-bit input segments, each of which is replaced with a code having M adjacent logic ones and 2N -1 logic zeros, the bit position of the least significant of the M logic ones corresponding to the binary value of the input segment it replaces. The codes are concatenated to form an encoded input. A decoding method decodes an output from the neural network into a binary form by partitioning the output into output segments having 2N +M-1 bits each, each of which is replaced with an N-bit binary segment being a bracketed weighted average of the significances of logic ones present in the output segment. The binary segments are concatenated to form a decoded output.", "Numeric encoding method and apparatus for neural networks "]
["Deep Neural Network (DNN) training technique embodiments are presented that train a DNN while exploiting the sparseness of non-zero hidden layer interconnection weight values. Generally, a fully connected DNN is initially trained by sweeping through a full training set a number of times. Then, for the most part, only the interconnections whose weight magnitudes exceed a minimum weight threshold are considered in further training. This minimum weight threshold can be established as a value that results in only a prescribed maximum number of interconnections being considered when setting interconnection weight values via an error back-propagation procedure during the training. It is noted that the continued DNN training tends to converge much faster than the initial training.", "Exploiting sparseness in training deep neural networks "]
["An adaptative source rate control method and apparatus utilizing a neural network are presented for controlling a data transmission of a media object over a communication network. A back propagation method transmitting control parameters related to the operation of a communications network is used for to dynamically adjust the performance of the network, in view of network parameters being sourced to a point of transmission. The bit rate and quantization level of the data stream are dynamically adjusted and shaped by the neural network in response to control parameters.", "Dynamic rate adaptation using neural networks for transmitting video data "]
["A method of training a neural network to perform decoding of a time-varying signal comprising a sequence of input symbols, which is coded by a coder such that each coded output symbol depends on more than one input symbol, characterised by repetitively: providing a plurality of successive input symbols to the neural network and to the coder, comparing the network outputs with the input signals; and adapting the network parameters to reduce the differences therebetween.", "Neural networks decoder "]
["In a neural network, input neuron units of an input layer are grouped into first through J-th input layer frames, where J represents a predetermined natural number. Intermediate neuron units of an intermediate layer are grouped into first through J-th intermediate layer frames. An output layer comprises an output neuron unit. Each intermediate neuron unit of a j-th intermediate layer frame is connected to the input neuron units of j'-th input layer frames, where j is variable between 1 and j and j' represents at least two consecutive integers, one of which is equal to j and at least one other of which is less than j. Each output neuron unit is connected to the intermediate neuron units of the intermediate layer. For recognition of an input pattern represented by a time sequence of feature vectors, each consisting of K vector components, where K represents a predetermined positive integer, each input layer frame consists of K input neuron units. Each intermediate layer frame consists of M intermediate neuron units, where M represents a positive integer which is less than K. The vector components of each feature vector are supplied to the respective input neuron units of one of the input layer frames that is preferably selected from three consecutively numbered input layer frames. The neural network is readily trained to make a predetermined one of the output neuron units produce an output signal indicative of the input pattern and can be implemented by a microprocessor.", "Multi-layer neural network to which dynamic programming techniques are applicable "]
["The architectures for a scalable neural processor (SNAP) and a Triangular Scalable Neural Array Processor (T-SNAP) are expanded to handle network simulations where the number of neurons to be modeled exceeds the number of physical neurons implemented. This virtual neural processing is described for three general virtual architectural approaches for handling the virtual neurons, one for SNAP and one for TSNAP, and a third approach applied to both SNAP and TSNAP.", "Virtual neurocomputer architectures for neural networks "]
["Systems and methods for training networks are provided. A method for training networks comprises receiving an input from each of a plurality of neural networks differing from each other in at least one of architecture, input modality, and feature type, connecting the plurality of neural networks through a common output layer, or through one or more common hidden layers and a common output layer to result in a joint network, and training the joint network.", "Method and system for joint training of hybrid neural networks for acoustic modeling in automatic speech recognition "]
["A neural network system and method for analyzing data sets, especially microarray gene expression data. The neural network is trained to generate time-dependent outcome predictions based on input features and outcome functions for a number of subjects. The features may be highly dimensional relative to the number of subjects, and feature selection is applied to the input feature data for training the neural network. A trained neural network processes input features from a subject to generate an outcome function that reflects the probability of the occurrence of an event at a given time point for that subject.", "Time-dependent outcome prediction using neural networks "]
["A control method of controlling a controlled system according to the invention comprises the first step of inputting a current and future target controlled variable to a first neural network model which performs learning using a past target controlled variable for the controlled system as an input signal and a past manipulated variable as a teacher signal, thereby obtaining a current virtual manipulated variable, the second step of causing a second neural network model, which have learnt to predict a behavior of the controlled system, to receive the virtual manipulated variable obtained in the first step and a controlled variable obtained from the controlled system at a current time, thereby obtaining a predicted controlled variable, the third step of obtaining an error of the predicted controlled variable obtained in the second step with respect to the target controlled variable, the fourth step of obtaining a correction amount for the virtual manipulated variable in accordance with a back propagation calculation of the second neural network model, using the error obtained in the third step, thereby correcting the virtual manipulated variable with the correction amount, and the fifth step of outputting the virtual manipulated variable corrected in the fourth step to the controlled system.", "Control method and apparatus using two neural networks "]
["A biometric identification apparatus and method using bio signals and an artificial neural network, are provided. The biometric identification apparatus includes: a periodic signal extraction unit which extracts one or more periodic signals from an input bio signal; a template calculation unit which calculates a template value using the extracted periodic signals; a template storage unit which stores a plurality of template values corresponding to a plurality of living bodies; and a reading unit which reads the template value that is most approximate to the template value calculated by the template calculation unit from the template storage unit. Accordingly, it is possible to identify a living body by taking into consideration all of the characteristics of bio signals detected from the living body.", "Biometric identification apparatus and method using bio signals and artificial neural network "]
["A neural network processing element uses primarily optical components to model a biological neuron having both spatial and temporal dependence. The neural network processing element includes a switch-controlled laser source, a multiple holographic lens, a spatial/temporal light modulator, and a photodetector array. Laser beam control may be optical, electrical or acoustical, or a combination of these.", "Optical neural network processing element with multiple holographic element interconnects "]
["Any deterministic finite-state automata (DFA) can be implemented in a sparse recurrent neural network (RNN) with second-order weights and sigmoidal discriminant functions. Construction algorithms can be extended to fault-tolerant DFA implementations such that faults in an analog implementation of neurons or weights do not affect the desired network performance. The weights are replicated k times for k-1 fault tolerance. Alternatively, the independent network is replicated 2k+1 times and the majority of the outputs is used for a k fault tolerance. In a further alternative solution, a single network with k\u03b7 neurons uses a \"n choose k\"encoding algorithm for k fault tolerance.", "Fault-tolerant implementation of finite-state automata in recurrent neural networks "]
["This is a fully parallel analog backpropagation learning processor which comprises a plurality of programmable resistive memory elements serving as synapse connections whose values can be weighted during learning with buffer amplifiers, summing circuits, and sample-and-hold circuits arranged in a plurality of neuron layers in accordance with delta-backpropagation algorithms modified so as to control weight changes due to circuit drift.", "Analog hardware for delta-backpropagation neural networks "]
["An optical vector multiplier can perform linear algebra calculations by using electro-optical modulators of sandwich-type construction or in liquid crystal fields such as occur, inter alia, in the case of neural networks. It can calculate linear algebra operations as rapidly as possible, although the matrix is represented by slow liquid crystal fields. In addition, it can also use the transposed matrix, can calculate the vector product of two vectors and store the resultant matrix of the vector product directly in the region of the matrix modulator cells. To this end, two rapid electro-optical modulators of sandwich-type construction, representing a vector, are disposed offset by 90\u00b0 and a matrix-shaped optical modulator that represents the matrix follows this arrangement. The vector modulator arrangement that is offset by 90\u00b0 can form the vector product of two vectors, the result being determined by a detector matrix whose detectors are located on the matrix modulator of each modulator cell. Thus the information can be stored locally there and processed.", "Optical vector multiplier for neural networks "]
["A method of accelerating the training of an artificial neural network uses a computer configured as an artificial neural network with a network input and a network output, and having a plurality of interconnected units arranged in layers including an input layer and an output layer. Each unit has a multiplicity of unit inputs and a set of variables for operating upon a unit inputs to provide a unit output. A plurality of examples are serially provided to the network input and the network output is observed. The computer is programmed with a back propagation algorithm for adjusting each set of variables in response to feedback representing differences between the network output for each example and the desired output. The examples are iterated until the signs of the outputs of the units of the output layer converge. Then each set of variables is multiplied by a multiplier. The examples are reiterated until the magnitude of the outputs of the units of the output layer converge.", "Speeding learning in neural networks "]
["The improved neuron is connected to input buses which transport input data and control signals. It basically consists of a computation block, a register block, an evaluation block and a daisy chain block. All these blocks, except the computation block substantially have a symmetric construction. Registers are used to store data: the local norm and context, the distance, the AIF value and the category. The improved neuron further needs some R/W memory capacity which may be placed either in the neuron or outside. The evaluation circuit is connected to an output bus to generate global signals thereon. The daisy chain block allows to chain the improved neuron with others to form an artificial neural network (ANN). The improved neuron may work either as a single neuron (single mode) or as two independent neurons (dual mode). In the latter case, the computation block, which is common to the two dual neurons, must operate sequentially to service one neuron after the other. The selection between the two modes (single/dual) is made by the user which stores a specific logic value in a dedicated register of the control logic circuitry in each improved neuron.", "Neuron architecture having a dual structure and neural networks incorporating the same "]
["A system and process for readily determining, for a specified knowledge domain in a given field of endeavor, perturbations applicable to an artificial neural network embodying such a specified knowledge domain that will produce a desired output, comprising a first, previously trained, artificial neural network containing training in some problem domain, which neural network is responsive to the presentment of a set of data inputs at the input portion thereof to produce a set of data outputs at the output portion thereof, a monitoring portion which constantly monitors the outputs of the first neural network to identify the desired outputs, and a network perturbation portion for effecting the application of perturbations, either externally or internally, to the first neural network to thereby effect changes in the output thereof. The perturbations may be effected by any number of different means, including by, but not limited to, presentment of new, varied data inputs, alteration or fixed or previously applied data inputs, such as by the introduction of noise to the inputs, relaxation or degradation of the network, and so forth, either randomly or systematically, and may be accomplished autonomously or upon specific external authorization or control. Identification of a desired output establishes an input/perturbation/output mapping relationship from which data inputs (external perturbations) and/or knowledge domain alterations (internal perturbations) that produce the desired output can be determined. The system and process can be employed in some instances and in some embodiments as a target seeking system for use with various design or problem solving applications, and can, in some embodiments, comprise or be comprised of a system and process for autonomously producing and identifying desirable design concepts through utilization of such a target seeking system.", "Neural network-based target seeking system "]
["Systems and methods using a neural network based portable absorption spectrometer system for real-time automatic evaluation of tissue injury are described. An apparatus includes an electromagnetic signal generator; an optical fiber (520) connected to the electromagnetic signal generator; a fiber optic probe (530) connected to the optical fiber; a broad band spectrometer (90) connected to the fiber optic probe (530); and a hybrid neural network connected to the broad band spectrometer (90). The hybrid neural network includes a principal component analyzer (200) of broad band spectral data obtained from said broad band spectrometer (90). The systems and methods provide unexpected advantages in that the accuracy of tissue injury analysis is improved.", "Accurate tissue injury assessment using hybrid neural network analysis "]
["Disclosed herein are devices, systems, and methods for detecting the presence and orientation of traffic lane markings. Deep convolutional neural networks are used with convolutional layers and max-pooling layers to generate fully connected nodes. After the convolutional and max-pooling layers, two sublayers are applied, one to determine presence and one to determine geometry. The presence of a lane marking segment as detected by the first sublayer can serve as a gate for the second sublayer by regulating the credit assignment for training the network. Only when the first sublayer predicts actual presence will the geometric layout of the lane marking segment contribute to the training of the overall network. This achieves advantages with respect to accuracy and efficiency and contributes to efficient robust model selection.", "Multi-task deep convolutional neural networks for efficient and robust traffic lane detection "]
["The present invention is a method and apparatus which accurately predicts one or more\noperating characteristics of an earth boring drill bit operated under a set of known\noperating conditions. A range of operating conditions may be input so that the\noperating characteristic(s) of the drill bit may be predicted over, and perhaps beyond the\nrange the drill bit designer has anticipated. In this manner, a new drill bit design may be\nrefined and/or proven with a high level of confidence prior to manufacture. Only\nminimal field testing of the new design is required to verify its performance.", "Drill bit design using neural networks "]
["Neural network systems (100) with learning and recall are applied to clustered multiple-featured data (122, 124, 126) and analog data.", "Neural network and system "]
["A neural system comprises multiple neurons interconnected via synapse devices. Each neuron integrates input signals arriving on its dendrite, generates a spike in response to the integrated input signals exceeding a threshold, and sends the spike to the interconnected neurons via its axon. The system further includes multiple noruens, each noruen is interconnected via the interconnect network with those neurons that the noruen's corresponding neuron sends its axon to. Each noruen integrates input spikes from connected spiking neurons and generates a spike in response to the integrated input spikes exceeding a threshold. There can be one noruen for every corresponding neuron. For a first neuron connected via its axon via a synapse to dendrite of a second neuron, a noruen corresponding to the second neuron is connected via its axon through the same synapse to dendrite of the noruen corresponding to the first neuron.", "Structural plasticity in spiking neural networks with symmetric dual of an electronic neuron "]
["Preferred embodiments of the invention provide systems and methods to observe one or more network elements associated with a network, receive an indication of an event relating to one or more network element configurations associated with the network, observe a potential outcome associated with the network, store the potential outcome such that the potential outcome is associated with the event, determine a probable outcome based on the potential outcome and store the probable outcome such that the probable outcome is associated with the event.", "Neural networks within a network management system "]
["A method for and system for training a connection network located between neuron layers within a multi-layer physical neural network. A multi-layer physical neural network can be formed having a plurality of inputs and a plurality outputs thereof, wherein the multi-layer physical neural network comprises a plurality of layers, wherein each layer comprises one or more connection networks and associated neurons. Thereafter, a training wave can be initiated across the connection networks associated with an initial layer of the multi-layer physical neural network which propagates thereafter through succeeding connection networks of succeeding layers of the neural network by successively closing and opening switches associated with each layer. One or more feedback signals thereof can be automatically provided to strengthen or weaken nanoconnections associated with each connection network.", "Multilayer training in a physical neural network formed utilizing nanotechnology "]
["A neural network utilizing the threshold characteristics of a semiconductor device as the various memory elements of the network. Each memory element comprises a complementary pair of MOSFETs in which the threshold voltage is adjusted as a function of the input voltage to the element. The network is able to learn by example using a local learning algorithm. The network includes a series of output amplifiers in which the output is provided by the sum of the outputs of a series of learning elements coupled to the amplifier. The output of each learning element is the difference between the input signal to each learning element and an individual learning threshold at each input. The learning is accomplished by charge trapping in the insulator of each individual input MOSFET pair. The thresholds of each transistor automatically adjust to both the input and output voltages to learn the desired state. After input patterns have been learned by the network, the learning functions is set to zero so that the thresholds remain constant and the network will come to an equilibrium state under the influence of a test input pattern thereby providing, as an output, the learned pattern most closely resembling the test input pattern.", "Neural network having an associative memory that learns by example "]
["A method and system is provided for predicting loads within a power system through the training of on-line and an off-line neural networks. Load data and load increments are used with an on-line load prediction scheme to generate predicted load values to optimize power generation and minimize costs. This objective is achieved by employing a method and system which predicts short term load trends through the use of historical load data and short term load forecast data.", "Load prediction based on-line and off-line training of neural networks "]
["A neural network controller for a pulsed rocket motor tactical missile. The missile includes a fuselage or body, with a propulsion system. The pulsed propulsion system has a need for a logical control of the application of propulsion energy throughout the missile's flight. The controller is trained to provide optimal initiation of individual rocket motor thrust pulses based on tactical information available at various points/times in the missile's flight. The controller training is through use of training cases, in which the network learns to output a specific target value(s) when specific values are input. When trained with a large sample of training cases selected from the multidimensional population of interest, the neural network effectively learns the correlations between inputs and outputs and can predict input/output relationships not previously seen in any training case.", "Neural network controller for a pulsed rocket motor tactical missile system "]
["Neural network systems (100) with learning and recall are applied to clustered multiple-featured data (122,124,126) and analog data.", "Neural network and system "]
["A electronic engine control (EEC) module executes a neural network processing program to control the idle speed of an internal combustion engine by controlling the bypass air (throttle duty cycle) and the engine's ignition timing. The neural network is defined by a unitary data structure which defmes the network architecture, including the number of node layers, the number of nodes per layer, and the interconnections between nodes. To achieve idle speed control, the neural network processes input signals indicating the current operating state of the engine, including engine speed, the intake mass air flow rate, a desired engine speed, engine temperature, and other variables which influence engine speed, including loads imposed by power steering and air conditioning systems. The network definition data structure holds weight values which determine the manner in which network signals, including the input signals, are combined. The network definition data structures are created by a network training system which utilizes an external training processor which employ dynamic gradient methods to derive network weight values in accordance with a cost function which quantitatively defines system objectives and an identification network which is pretined to provide gradient signals representative of the behavior of the physical plant. The training processor executes training cycles asynchronously with the operation of the EEC module in a representative test vehicle.", "Trained neural network engine idle speed control system "]
["Learning processes for a single hidden layer neural network, including linear input units, nonlinear hidden units, and linear output units, calculate the lower-layer network parameter gradients by taking into consideration a solution for the upper-layer network parameters. The upper-layer network parameters are calculated by a closed form formula given the lower-layer network parameters. An accelerated gradient algorithm can be used to update the lower-layer network parameters. A weighted gradient also can be used. With the combination of these techniques, accelerated training with faster convergence, to a point with a lower error rate, can be obtained.", "Learning processes for single hidden layer neural networks with linear output units "]
["By way of example, the technology disclosed by this document receives image data; extracts a depth image and a color image from the image data; creates a mask image by segmenting the depth image; determines a first likelihood score from the depth image and the mask image using a layered classifier; determines a second likelihood score from the color image and the mask image using a deep convolutional neural network; and determines a class of at least a portion of the image data based on the first likelihood score and the second likelihood score. Further, the technology can pre-filter the mask image using the layered classifier and then use the pre-filtered mask image and the color image to calculate a second likelihood score using the deep convolutional neural network to speed up processing.", "Augmenting Layer-Based Object Detection With Deep Convolutional Neural Networks "]
["An artificial neural network incorporating difference type, non-MP (McCullough-Pitts) neuron cells and a method and apparatus for training this network. More specifically, the output of each neuron cell is a nonlinear mapping of a distance metric of a difference vector and an offset. The difference vector is the difference between an input and a reference vector; the offset is representative of the radius of a hyperspheroidal discriminant function.", "Neural network incorporating difference neurons "]
["An enhanced neural network shell for application programs is disclosed. The user is prompted to enter in non-technical information about the specific problem type that the user wants solved by a neural network. The user also is prompted to indicate the input data usage information to the neural network. Based on this information, the neural network shell creates a neural network data structure by automatically selecting an appropriate neural network model and automatically generating an appropriate number of inputs, outputs, and/or other model-specific parameters for the selected neural network model. The user is no longer required to have expertise in neural network technology to create a neural network data structure.", "Enhanced neural network shell for application programs "]
["Methods, computer-readable media, and systems are provided for machine learning in a simultaneous recurrent neural network. One embodiment of the invention provides a method including initializing one or more weight in the network, initializing parameters of an extended Kalman filter, setting a Jacobian matrix to an empty matrix, augmenting the Jacobian matrix for each of a plurality of training patterns, adjusting the one or more weights using the extended Kalman filter formulas, and calculating a network output for one or more testing patterns.", "Methods of improved learning in simultaneous recurrent neural networks "]
["A rule-based expert system is generated from a neural network. The neural network is trained in such a way as to avoid redundancy and to select input weights to the various processing elements in such a way as to nullify the input weights which have smaller absolute values. The neural network is translated into a set of rules by a heuristic search technique. Additionally, the translation distinguishes between positive and negative attributes for efficiency and can adequately explore rule size exponential with a given parameter. Both explicit and implicit knowledge of adapted neural networks are decoded and represented as if--then rules.", "Translation of a neural network into a rule-based expert system "]
["A self-organizing neural network and method for classifying a pattern signature having N-features is provided. The network provides a posteriori conditional class probability that the pattern signature belongs to a selected class from a plurality of classes with which the neural network was trained. In its training mode, a plurality of training vectors is processed to generate an N-feature, N-dimensional space defined by a set of non-overlapping trained clusters. Each training vector has N-feature coordinates and a class coordinate. Each trained cluster has a center and a radius defined by a vigilance parameter. The center of each trained cluster is a reference vector that represents a recursive mean of the N-feature coordinates from training vectors bounded by a corresponding trained cluster. Each reference vector defines a fractional probability associated with the selected class based upon a ratio of i) a count of training vectors from the selected class that are bounded by the corresponding trained cluster to ii) a total count of training vectors bounded by the corresponding trained cluster. In the exercise mode, an input vector defines the pattern signature to be classified. The input vector has N-feature coordinates associated with an unknown class. One of the reference vectors is selected so as to minimize differences with the N-feature coordinates of the input vector. The fractional probability of the selected one of the reference vectors is the a posteriori conditional class probability that the input vector belongs to the selected class.", "Self-organizing neural network for classifying pattern signatures with `a posteriori` conditional class probability "]
["The invention is directed to a method, utilizing a neural network, for estimating helicopter airspeed in the low airspeed flight range of below about 50 knots using only fixed system parameters as inputs to the neural network. The method includes the steps of: (a) defining input parameters derivable from variable state parameters generated during flight of the helicopter and measurable in a nonrotating reference frame associated with the helicopter; (b) determining the input parameters and a corresponding helicopter airspeed at a plurality of flight conditions representing a predetermined low airspeed flight domain of the helicopter; (c) establishing a learned relationship between the determined input parameters and the corresponding helicopter airspeed wherein the relationship is represented by at least one nonlinear equation; (d) storing the at least one nonlinear equation in a memory onboard the helicopter; (e) measuring real time values of the variable state parameters during low airspeed flight of the helicopter; (f) calculating real time values of the input parameters; (g) storing the real time values of the input parameters in the memory; (h) processing the real time values of the input parameters in accordance with the at least one nonlinear equation to determine real time airspeed; and (i) displaying the real time airspeed.", "Neural network based method for estimating helicopter low airspeed "]
["Software for controlling processes in a heterogeneous semiconductor manufacturing environment may include a wafer-centric database, a real-time scheduler using a neural network, and a graphical user interface displaying simulated operation of the system. These features may be employed alone or in combination to offer improved usability and computational efficiency for real time control and monitoring of a semiconductor manufacturing process. More generally, these techniques may be usefully employed in a variety of real time control systems, particularly systems requiring complex scheduling decisions or heterogeneous systems constructed of hardware from numerous independent vendors.", "Applications of neural networks "]
["A neural network for processing sensory information. The network comprise one or more layers including interconnecting cells having individual states. Each cell is connected to one or more neighboring cells. Sensory signals and signals from interconnected neighboring cells control a current or a conductance within a cell to influence the cell's state. In some embodiments, the current or conductance of a cell can be controlled by a signal arising externally of the layer. Each cell can comprise an electrical circuit which receives an input signal and causes a current corresponding to the signal to pass through a variable conductance. The conductance is a function of the states of the one or more interconnecting neighboring cells. Proper interconnection of the cells on a layer can produce a neural network which is sensitive to predetermined patterns or the passage of such patterns across a sensor array whose signals are input into the network. The layers in the network can be made sensitive to distinct sensory parameters, so that networks which are sensitive to different wavelengths or polarizations of light energy can be produced.", "Optoelectronic sensory neural network "]
["A system and method are disclosed for automatically generating music on the basis of an initial sequence of input notes, and in particular to such a system and method utilizing a recursive artificial neural network (RANN) architecture. The aforementioned system includes a score interpreter (2) interpreting an initial input sequence, a rhythm production RANN (4) for generating a subsequent note duration, a note generation RANN (6) for generating a subsequent note, and feedback means for feeding the pitch and duration of the subsequent note back to the rhythm generation (4) and note generation (6) RANNs, the subsequent note thereby becoming the current note for a following iteration.", "System and method for automatic music generation using a neural network architecture "]
["Recognition of speech with successive expansion of a reference vocabulary, can be used for automatic telephone dialing by voice input. Neural and conventional recognition methods are performed in parallel so that during training and configuration of the neural network, a conventional recognizer operating according to the dynamic programming principle has available newly added word patterns as references for immediate use in recognition. Upon completion of the training and configuration, the neural network takes over the recognition of the now expanded vocabulary.", "Speech recognition combining dynamic programming and neural network techniques "]
["A system and method for selecting a training data set from a set of multidimensional geophysical input data samples for training a model to predict target data. The input data may be data sets produced by a pulsed neutron logging tool at multiple depth points in a cases well. Target data may be responses of an open hole logging tool. The input data is divided into clusters. Actual target data from the training well is linked to the clusters. The linked clusters are analyzed for variance, etc. and fuzzy inference is used to select a portion of each cluster to include in a training set. The reduced set is used to train a model, such as an artificial neural network. The trained model may then be used to produce synthetic open hole logs in response to inputs of cased hole log data.", "Neural network training data selection using memory reduced cluster analysis for field model development "]
["A method of training neural systems and estimating regression coefficients of regression models with respect to an error criterion is disclosed. If the error criterion is a risk-averting error cri- terion, the invented method performs the training/estimation by starting with a small value of the risk-sensitivity index of the risk-averting error criterion and gradually increasing it to ensure numerical feasibility. If the error criterion is a risk-neutral error criterion such as a standard sum- of-squares error criterion, the invented method performs the training/estimation first with respect to a risk-averting error criterion associated with the risk-neutral error criterion. If the result is not satisfactory for the risk-neutral error criterion, further training/estimation is performed either by continuing risk-averting training/estimation with decreasing values of the associated risk-averting error criterion or by training/estimation with respect to the given risk-neutral error criterion or by both.", "Risk-averting method of training neural networks and estimating regression models "]
["An artificial neural network apparatus comprising an array of neural units, each comprising a router, at least one neuron device and at least one synapse unit. The routers of respective neural units communicate with one another using data packets. The synapse units receive and create analogue signals, the routers converting these signals from or into packet form for communication between neural units. The use of routers in this way simplifies the required interconnectivity between neural units in the array and so facilitates the creation of large artificial neural networks.", "Artificial neural network architecture "]
["An artificial neural network, ANN, and method of training the ANN for inversion of logging tool signals into well logs of formation parameters is disclosed. Properly selected synthetic models of earth formations are used to train the ANN. The models include Oklahoma and chirp type of formations. In each model parameter contrasts of from 10 to 1 to about 100 to 1 are included. Models including maximum and minimum parameter values spanning the operating range of the selected logging tool are included. Parameter contrasts at interfaces are limited to realistic values found in earth formations. The selected models are used to generate synthetic tool signals, which are then used as inputs to the ANN for training. When the ANN coefficients are properly adjusted to produce an output matching the original models, the ANN can be used for inversion of any real signals from the selected logging tool.", "Processing well logging data with neural network "]
["Use is made of a neural network in order to restore a binary image to an original multi-level image, by way of example. Using the neural network makes it possible to raise the accuracy of restoration and the speed of processing.", "Binary to multi-level image restoration using neural network "]
["A family of novel multi-layer discrete-time neural net controllers is presented for the control of an multi-input multi-output (MIMO) dynamical system. No learning phase is needed. The structure of the neural net (NN) controller is derived using a filtered error/passivity approach. For guaranteed stability, the upper bound on the constant learning rate parameter for the delta rule employed in standard back propagation is shown to decrease with the number of hidden-layer neurons so that learning must slow down. This major drawback is shown to be easily overcome by using a projection algorithm in each layer. The notion of persistency of excitation for multilayer NN is defined and explored. New on-line improved tuning algorithms for discrete-time systems are derived, which are similar to e-modification for the case of continuous-time systems, that include a modification to the learning rate parameter plus a correction term. These algorithms guarantee tracking as well as bounded NN weights. An extension of these novel weight tuning updates to NN with an arbitrary number of hidden layers is discussed. The notions of discrete-time passive NN, dissipative NN, and robust NN are introduced.", "Discrete-time tuning of neural network controllers for nonlinear dynamical systems "]
["Neural networks provide efficient, robust and precise filtering techniques for compensating linear and non-linear distortion of an audio transducer such as a speaker, amplified broadcast antenna or perhaps a microphone. These techniques include both a method of characterizing the audio transducer to compute the inverse transfer functions and a method of implementing those inverse transfer functions for reproduction. The inverse transfer functions are preferably extracted using time domain calculations such as provided by linear and non-linear neural networks, which more accurately represent the properties of audio signals and the audio transducer than conventional frequency domain or modeling based approaches. Although the preferred approach is to compensate for both linear and non-linear distortion, the neural network filtering techniques may be applied independently.", "Neural network filtering techniques for compensating linear and non-linear distortion of an audio transducer "]
["A system for using machine-learning to create a model for performing integrated circuit layout extraction is disclosed. The system of the present invention has two main phases: model creation and model application. The model creation phase comprises creating one or more extraction models using machine-learning techniques. First, a complex extraction problem is decomposed into smaller simpler extraction problems. Then, each smaller extraction problem is then analyzed to identify a set of physical parameters that fully define the smaller extraction problem. Next, models are created using machine learning techniques for all of the smaller simpler extraction problems. The machine learning is performed by first creating training data sets composed of the identified parameters from typical examples of the smaller extraction problem and the answers to those example extraction problems as solved using a highly accurate physics-based field solver. Next, the system trains a set of neural networks using the training sets. In one embodiment, Bayesian inference is used to train the neural networks that are used to model the extraction. After the creation the neural network based models for each of the smaller simpler extraction problems, the neural network based models may be used for extraction.", "Method and apparatus for performing extraction using a neural network "]
["An artificial neural network, which has a plurality of neurons each receiving a plurality of inputs whose effect is determined by adjust able weights at synapses individually connecting the inputs to the neuron to provide a sum signal to a sigmoidal function generator determining the output of the neuron, undergoes memory modification by a steepest-descent method in which individual variations in the outputs of the neurons are successively generated by small perturbations imposed on the sum signals. As each variation is generated on the output of a neuron, an overall error of all the neuron outputs in relation to their desired values is measured and compared to this error prior to the perturbation. The difference in these errors, with adjustments which may be changed as the neuron outputs converge toward their desired values, is used to modify each weight of the neuron presently subjected to the perturbation.", "Memory modification of artificial neural networks "]
["A method of and system for parallelizing an program, comprising the steps of inputting an algorithm, operating said algorithm on selected data inputs to generate representative outputs, inputting representative outputs into parallelizing algorithms, and outputting a parallel implementation of said algorithm. In particular, this provides a parallel framework for target classification and pattern recognition procedures.", "Hybrid neural network classifier, systems and methods "]
["Particle patterns formed on an inclined bottom surface of a reaction vessel are photoelectrically detected to produce a two-dimensional image signal. The signal is processed to judge or classify the particle patterns into an agglutinated pattern, a non-agglutinated pattern or an uncertain pattern with the aid of a neural network. An image signal representing a particle pattern is first extracted, then the image signal is decomposed into a series of light intensity areas due to different contours of the inclined bottom surface. The integrated light intensities of each area are presented to a neural network. The neural network operates in a training mode and a classification mode. In the training mode the neural network is presented with numerous samples of decomposed images as well as their respective classification. In the classification mode the neural network will judge a decomposed image based on a generalization made during the training mode.", "Method for judging particle agglutination patterns using neural networks "]
["A data compression and recovery apparatus compresses picture element data of a color image by expressing two primary color values of each picture element as a set of parameter values of a neural network in conjunction with reference color data values of a corresponding block of picture elements. Date recovery is achieved by inputting each block of reference color values to a neural network while establishing the corresponding set of parameter values in the network, to thereby obtain the original pair of encoded primary color values for each of successive picture elements. The third primary color can be used as the reference color.", "Color image data compression and recovery apparatus based on neural networks "]
["A neural network has inputs formed by square array of optical modulators Mij and outputs by optical detectors Dij coupled to threshold comparators. A holographic plate includes a spatial modulator whose elements are controlled by a controller to form an array of optical beams from a coherent optical source. Each optical beam optically interconnects a modulator Mij with a respective detector Dij. The weight values of the neural network are provided by the intensities of the optical beams. This obviates the need for an optical weighting mask between an array of light emitting diodes and a detector array allowing a higher density of lower power consumption components and reprogrammability of the network.", "Neural networks "]
["Systems and methods for sequence transcription with neural networks are provided. More particularly, a neural network can be implemented to map a plurality of training images received by the neural network into a probabilistic model of sequences comprising P(S|X) by maximizing log P(S|X) on the plurality of training images. X represents an input image and S represents an output sequence of characters for the input image. The trained neural network can process a received image containing characters associated with building numbers. The trained neural network can generate a predicted sequence of characters by processing the received image.", "Sequence transcription with deep neural networks "]
["Realization of a reconfigurable neuron for use in a neural network has been achieved using analog techniques. In the reconfigurable neuron, digital input data are multiplied by programmable digital weights in a novel connection structure whose output permits straightforward summation of the products thereby forming a sum signal. The sum signal is multiplied by a programmable scalar, in general, 1, when the input data and the digital weights are binary. When the digital input data and the digital weights are multilevel, the scalar in each reconfigurable neuron is programmed to be a fraction which corresponds to the bit position in the digital data representation, that is, a programmable scalar of 1/2, 1/4, 1/8, and so on. The signal formed by scalar multiplication is passed through a programmable build out circuit which permits neural network reconfiguration by interconnection of one neuron to one or more other neurons. Following the build out circuit, the output signal therefrom is supplied to one input of a differential comparator for the reconfigurable neuron. The differential comparator receives its other input from a supplied reference potential. In general, the comparator and reference potential level are designed to generate the nonlinearity for the neuron. One common nonlinearity is a hard limiter function. The present neuron offers the capability of synthesizing other nonlinear transfer functions by utilizing several reference potential levels connected through a controllable switching circuit.", "Reconfigurable neural network "]
["A method of reducing the amount of learning data required to execute a neural network learning procedure, whereby an original entire set of learning sample data are divided, using cluster analysis of the original entire learning sample data, into a plurality of sub-groups, with the sub-groups being respectively applied to a neural network as learning data and with respective values of recognition index obtained thereby for the neural network being judged, to select the smallest sub-group which will provide a value of recognition index that is at least equal to the recognition index obtainable by using the original entire learning data.", "Method of reducing amount of data required to achieve neural network learning "]
["An ultrasonic flowmeter using ultrasonic pulses transmitted within a pipe containing flowing fluid determines values, including flow rate or fluid velocity of the flowing fluid. The ultrasonic flowmeter transmits trains of ultrasonic pulses having velocity components either in the upstream or downstream direction. The ultrasonic flowmeter includes the transducer placed downstream external to the pipe and a transducer placed upstream external to the pipe. Time measurements are made to determine a time, TU, required for a pulse train to travel from the downstream transducer to the upstream transducer and the time, TD, required for a pulse train to travel from the upstream transducer to the downstream transducer. The ultrasonic flowmeter is configured to have consistent triggering of the pulses of the pulse train to reduce error. Also, the electronics are simplified to allow for lower timing resolution requirements. Once TU and TD are determined, a series of validation filters, parameter filters, coarse flow rate filters, and refined flow rate filters are used to determine flow measurement. Alternatively, fluid velocity filters are used. Also, diameter filters are used to determine the current interior diameter of the pipe. All measurements are made noninvasively. The system and method includes determining coefficients for the various filters to account for variability in dimensions, parameters and conditions involved.", "System and method using digital filters and neural networks to determine fluid flow "]
["A four quadrant, analog multiplier circuit useful for MOS implementation of feedback/feedforward neural networks. The multiplier circuit uses only one op-amp and one pair of input MOS FETs. It becomes a multiplier/summer by the addition of only one additional pair of input FETs for each additional product to be summed and achieves the vector scalar product of 2 n-tuple vector inputs using only 2(n+1) MOS transistors.", "Analog, continuous time vector scalar multiplier circuits and programmable feedback neural network using them "]
["An electrophotographic process control device capable of controlling the supply of a toner in such a manner as to stabilize an image against changes in the characteristics of a photoconductive element and in toner density. At the learning stage of a neural network, data from sensors are applied to the input layer of the network while a latent image gamma characteristic indicative of a relation between the amount of exposure and the potential of an image area is used as learning data to be given via the output layer of the network. At a control stage, the data from the sensors are applied to the input layer of the network, as at the learning stage, and the amount of exposure is so controlled as to set up a desired potential in an image area on the basis of a latent image gamma characteristic obtainable from the output layer of the network.", "Electrophotographic process control device using a neural network to control an amount of exposure "]
["A classification system is provided for combining multiple input representations by a single neural network architecture. In such a classification system having a single neural network architecture, classification channels corresponding to various input representations may be integrated through their own and shared hidden layers of the network to produce highly accurate classification. The classification system is particularly applicable to character classifying applications which use stroke and character image features as the main classification criteria, along with scalar features such as stroke count and aspect ratio features as secondary classification. The classification channels corresponding to the scalar features may be cross wired to the classification channels corresponding to the main input representations for further improving the accuracy of the classification output. Because a single neural network architecture is used, only one, standard training technique is needed for this classification system, special data handling is minimized, and the training time can be reduced, while highly accurate classification is achieved.", "Classifying system having a single neural network architecture for multiple input representations "]
["A neural network is implemented by discrete-time, continuous voltage state analog device in which neuron, synapse and synaptic strength signals are generated in highly parallel analog circuits in successive states from stored values of the interdependent signals calculated in a previous state. The neuron and synapse signals are refined in a relaxation loop while the synaptic strength signals are held constant. In learning modes, the synaptic strength signals are modified in successive states from stable values of the analog neuron signals. The analog signals are stored for as long as required in master/slaver sample and hold circuits as digitized signals which are periodically refreshed to maintain the stored voltage within a voltage window bracketing the original analog signal.", "State analog neural network and method of implementing same "]
["A method for guessing, in an electronic game, an object that a user is thinking of, from a set of target objects, after asking the user at least one question, the method utilizing a neural network structured in a target objects-by-questions matrix format, wherein each cell of the matrix defines an input-output connection weight, and the neural network can be utilized in a first mode, whereby answers to asked questions are input nodes and the target objects are output nodes, and in a second mode, whereby the target objects are input nodes and the questions are output nodes, the method comprising the steps of ranking the target objects by utilizing the neural network in the first mode; ranking the questions by utilizing the neural network in the second mode; and providing a guess in accordance with the ranking of the target objects.", "Artificial neural network guessing method and game "]
["Radar apparatus used for point-source location, where an adaptive feed forward artificial neural network is used to calculate a position vector from image information provided by radar receiving element outputs. Where an object is sensed within a field of view of a multiple output radar, then the radar receiving sensor element outputs are processed inputs as image vectors for use in input nodes of an input node layer of the artificial neural network. Typically the neural network has the same number of input nodes as the number of sensor element outputs an array within the radar receiver. Increased accuracy of point-source location can be achieved by increasing the number of hidden layers used, and/or increasing the number of nodes within each hidden layer. Training of the artificial neural network is described for (5\u00d71), (1\u00d75) and (4\u00d74) radar receiving arrays, and also for idealised and noisy data.", "Radar apparatus using neural network for azimuth and elevation detection "]
["A dynamically stable associative learning neural network system include a plurality of synapses and a non-linear function circuit and includes an adaptive weight circuit for adjusting the weight of each synapse based upon the present signal and the prior history of signals applied to the input of the particular synapse and the present signal and the prior history of signals applied to the input of a predetermined set of other collateral synapses. A flow-through neuron circuit embodiment includes a flow-through synapse having a predetermined fixed weight. A neural network is formed employing neuron circuits of both the above types. A set of flow-through neuron circuits are connected by flow-through synapses to form separate paths between each input terminal and a corresponding output terminal. Other neuron circuits having only adjustable weight synapses are included within the network. This neuron network is initialized by setting the adjustable synapses at some value near the minimum weight. The neural network is taught by successively application of sets of inputs signals to the input terminals until a dynamic equilibrium is reached.", "Neural network with weight adjustment based on prior history of input signals "]
["A self-contained chip set architecture for ANN systems, based on back-propagation model with full-connectivity topology, and on-chip learning and refreshing, based on analog chip set technology providing self-contained synapse and neuron modules with fault tolerant neural computing, capable of growing to any arbitrary size as a result of embedded electronic addressing. Direct analog and digital I/O ports allow real-time computation and interface communication with other systems including digital host of any bus bandwidth. Scalability is provided, allowing accommodation of all input/output data sizes and different host platform.", "Hybrid chip-set architecture for artificial neural network system "]
["A system and method of computer data analysis using neural networks. In one embodiment of the invention, the system and method includes generating a data representation using a data set, the data set including a plurality of attributes, wherein generating the data representation includes: modifying the data set using a training algorithm, wherein the training algorithm includes growing the data set; and performing convergence testing, wherein convergence testing checks for convergence of the training algorithm, and wherein the modifying of the data set is repeated until convergence of the training algorithm occurs; and displaying one or more subsets of the data set using the data representation. In one embodiment, the data representation is a knowledge filter that includes a representation of an input data set. The representation may be constructed during a training process. In one exemplary embodiment, the training process uses unsupervised neural networks to create the data representation. In general terms, the data representation may include a number of coupled, or connected, hexagons called nodes. Considering relevant attributes, two nodes that are closer together may be more similar than two nodes that are further apart.", "Method and system of data analysis using neural networks "]
["A neural network for predicting values in non-linear functional mappings having a single hidden layer function generator (12) and an output layer (40). The single hidden layer function generator (12) is operable to receive one or more mapping inputs (x1) and generate a plurality of terms (14) from each mapping input. The plurality of terms generated by the single hidden layer function generator (12) includes at least one trigonometric term selected from the group comprising sin(x1), sin(2x1), sin(3x1), cos(x1), cos(2xl), cos(3xl), cosec(xl), cotan(xl), and being free of Gaussian and Sigmoidal terms.", "Neural network for predicting values in non-linear functional mappings "]
["A system and method for fault detection is provided. The fault detection system provides the ability to detect symptoms of fault in turbine engines and other mechanical systems that have nonlinear relationships. The fault detection system uses a neural network to perform a data representation and feature extraction where the extracted features are analogous to principal components derived in a principal component analysis. This neural network data representation analysis can then be used to determine the likelihood of a fault in the system.", "Nonlinear neural network fault detection system and method "]
["A method for performance envelope boundary cueing for a vehicle control system comprises the steps of formulating a prediction system for a neural network and training the neural network to predict values of limited parameters as a function of current control positions and current vehicle operating conditions. The method further comprises the steps of applying the neural network to the control system of the vehicle, where the vehicle has capability for measuring current control positions and current vehicle operating conditions. The neural network generates a map of current control positions and vehicle operating conditions versus the limited parameters in a pre-determined vehicle operating condition. The method estimates critical control deflections from the current control positions required to drive the vehicle to a performance envelope boundary. Finally, the method comprises the steps of communicating the critical control deflection to the vehicle control system; and driving the vehicle control system to provide a tactile cue to an operator of the vehicle as the control positions approach the critical control deflections.", "Neural network based automatic limit prediction and avoidance system and method "]
["A neural network is used to process a set of ranking features in order to determine the relevancy ranking for a set of documents or other items. The neural network calculates a predicted relevancy score for each document and the documents can then be ordered by that score. Alternate embodiments apply a set of data transformations to the ranking features before they are input to the neural network. Training can be used to adapt both the neural network and certain of the data transformations to target environments.", "Enterprise relevancy ranking using a neural network "]
["Methods of training neural networks (100, 600) that include one or more inputs (102-108) and a sequence of processing nodes (110, 112, 114, 116) in which each processing node may be coupled to one or more processing nodes that are closer to an output node are provided. The methods include establishing an objective function that preferably includes a term related to differences between actual and expected output for training data, and a term related to the number of weights of significant magnitude. Training involves optimizing the objective function in terms of weights that characterize directed edges of the neural network. The objective function is optimized using algorithms that employ derivatives of the objective function. Algorithms for accurately and efficiently estimating derivatives of the summed input going into output processing nodes of the neural network with respect to the weights of the neural network are provided.", "Neural Network and Method of Training "]
["A remaining response time for an elevator car under consideration for assignment to a newly registered hall call is estimated by using a neural network. The neural network or any other downstream module may be standardized for use in any building by use of an upstream fixed length stop description that summarizes the state of the building at the time of the registration of the new hall call for one or more postulated paths of each and every car under consideration for answering the new hall call.", "Elevator control neural network "]
["A method and apparatus for calculating an expected access time associated with one of a plurality of disk drive commands employs one or more neural networks. A plurality of disk drive commands received from an external source are stored in a memory, typically in a queue. Using a neural network, an expected access time associated with each of the queued commands is determined. Determining the expected access time associated with each of the queued commands involves determining a time for performing a seek and settle operation for each of the queued commands and a latency time associated with each of the queued commands. The command indicated by the neural network as having a minimum expected access time relative to access times associated with other ones of the queued commands is identified for execution. A first neural network may be used to determine an expected access time associated with each read command stored in a read command queue and a second neural network may be used to determine an expected access time associated with each write command stored in a write command queue. A pair of read and write neural networks may be associated with each of a number of read/write transducers employed in a disk drive system. The neural network may be trained at the time of manufacture and on a periodic basis during the service life of the disk drive.", "Hard disk drive employing neural network for performing expected access time calculations "]
["Artificial Neural Networks (ANNs) are useful mathematical constructs for tasks such as prediction and classification. While methods are well-established for the actual training of individual neural networks, determining optimal ANN architectures and input spaces is often a very difficult task. An exhaustive search of all possible combinations of parameters is rarely possible, except for trivial problems. A novel method is presented which applies Genetic Algorithms (GAs) to the dual optimization tasks of ANN architecture and input selection. The method contained herein accomplishes this using a single genetic population, simultaneously performing both phases of optimization. This method allows for a very efficient ANN construction process with minimal user intervention.", "Method for simultaneously optimizing artificial neural network inputs and architectures using genetic algorithms "]
["A digital computer architecture specifically tailored for implementing a neural network. Several simultaneously operable processors (10) each have their own local memory (17) for storing weight and connectivity information corresponding to nodes of the neural network whose output values will be calculated by said processor (10). A global memory (55,56) is coupled to each of the processors (10) via a common data bus (30). Output values corresponding to a first layer of the neural network are broadcast from the global memory (55,56) into each of the processors (10). The processors (10) calculate output values for a set of nodes of the next higher-ordered layer of the neural network. Said newly-calculated output values are broadcast from each processor (10) to the global memory (55,56) and to all the other processors (10), which use the output values as a head start in calculating a new set of output values corresponding to the next layer of the neural network.", "Method of implementing a neural network on a digital computer "]
["A process for converting characters arranged circularly, as for example about the center hole of a compact disk, into a linear arrangement. Points are assigned to locations on the circular arrangement. These points are mapped to a linear arrangement. The number of points is selected to be greater than those of the original image to enhance the resolution of the resulting image. The location of the points is stored in an address array. The values of the pixels in the original image are then copied to a target array. The pixel values are then converted to binary values serving as input to a recognition neural network and a verification neural network.", "Neural network for character recognition of rotated characters "]
["A signal transformation method that transforms an input signal obtained from a subject under a first value of a parameter to an output signal obtainable from said subject under a second value of said parameter is disclosed. The method creates a plurality of neural networks and subjects them to learn the mapping transformation. Genetic programming is used to evolve said plurality of neural networks by applying genetic operators to alter the configurations of said plurality of neural networks. The process of neural learning and genetic altering repeats until a predetermined number of generations is reach. The neural network that performs the mapping transformation best can be selected as the optimal neural network. This optimal neural network can be used subsequently to transform a second input signal to a second output signal for a pre-defined value of the parameter. The method of deriving the mapping transformation and the method of using the optimal neural network can be implemented as software applications that run on a data processing system.", "System and method of using genetic programming and neural network technologies to enhance spectral data "]
["A signal discrimination device using a neural network for discriminating input signals such as radar reception signals includes an adaptive code generator means for generating codes for representing the discrimination categories. The distances between the codes for closely related categories are smaller than the distances between the codes for remotely related categories. During the learning stage, the neural network is trained to output the codes for respective inputs. The discrimination result judgment means determines the categories by comparing the outputs of the neural network and the codes for the respective categories.", "Signal discrimination device using neural network "]
["Several embodiments of neural processors implemented on a VLSI circuit chip are disclosed, all of which are capable of entering a matrix T into an array of photosensitive devices which may be charge coupled or charge injection devices (CCD or CID). Using CCD's to receive and store the synapses of the matrix T from a spatial light modulator, or other optical means of projecting an array of pixels, semiparallel synchronous operation is achieved. Using CID's, full parallel synchronous operation is achieved. And using phototransistors to receive the array of pixels, full parallel and asynchronous operation is achieved. In the latter case, the source of the pixel matrix must provide the memory necessary for the matrix T. In the other cases, the source of the pixel matrix may be turned off after the matrix T has been entered and stored by the CCD's or CID's.", "Parallel optoelectronic neural network processors "]
["A method and apparatus are disclosed that modify [ies] and generalize [s] the use in artificial neural networks of the error backpropagation algorithm. Each neuron unit first divides a plurality of weighted inputs into more than one group, then sums up weighted inputs in each group to provide each group's intermediate outputs, and finally processes the intermediate outputs to produce an output of the neuron unit. Since the method uses, when modifying each weight, a partial differential coefficient generated by partially-differentiating the output of the neuron unit by each weighted input, the weight can be properly modified even if the output of a neuron unit as a function of intermediate outputs has a plurality of variables corresponding to the number of groups. Since the conventional method uses only one differential coefficient, that is, the differential coefficient of the output of a neuron unit differentiated by the sum of all weighted inputs in a neuron unit, for all weights in a neuron unit, it may be said that the method according to the present invention generalizes the conventional method. The present invention is especially useful for pulse density neural networks which express data as an ON-bit density of a bit string.", "Neural network and method for training the neural network "]
["A method and apparatus for color matching are provided, in which paint recipe neural networks are utilized. The color of a standard is expressed as color values. The neural network includes an input layer having nodes for receiving input data related to paint bases. Weighted connections connect to the nodes of the input layer and have coefficients for weighting the input data. An output layer having nodes are either directly or indirectly connected to the weighted connections and generates output data related to color values. The data to the input layer and the data from the output layer are interrelated through the neural network's nonlinear relationship. The paint color matching neural network can be used for, but not limited to, color formula correction, matching from scratch, effect pigment identification, selection of targets for color tools, searching existing formulas for the closest match, identification of formula mistakes, development of color tolerances and enhancing conversion routines.", "Computer-implemented neural network color matching formulation system "]
["An adaptive probabilistic neural network (APNN) includes a cluster processor circuit which generates a signal which represents a probability density function estimation value which is used to sort input pulse parameter data signals based upon a probability of obtaining a correct match with a group of input pulse parameter data signals that have already been sorted. In the APNN system, a pulse buffer memory circuit is contained within the cluster processor circuit and temporarily stores the assigned input pulse parameter data signals. The pulse buffer memory circuit is initially empty. As the input pulse parameter data signals are presented to the APNN, the system sorts the incoming data signals based on the probability density function estimation value signal generated by each currently operating cluster processor circuit. The current input pulse parameter data signal is sorted and stored in the pulse buffer memory circuit of the cluster processor circuit. A small probability density function estimation value signal indicates the current unassigned input pulse parameter data signal is not recognized by the APNN system. A large probability density function estimation value signal indicates a match and the current input pulse parameter data signal will be included within a particular cluster processor circuit.", "Real time adaptive probabilistic neural network system and method for data sorting "]
["A phase diversity wavefront correction system for use in a multiple aperture optical imaging system forms an in-focus image as a composite, focused image from the multiple apertures of the system and also forms an additional image which is deliberately made out of focus to a known extent. Taken together, the two images are processed to create one or more metrics, such as the power metric and sharpness metric. Neural networks are provided, each having an output corresponding to a parameter of an aperture of the imaging system, such as a piston position (axial displacement) or tip/tilt (angular displacement) of one telescope with respect to the others in the system. The neural networks each correspond to one parameter of a telescope or a combinations of parameters and are trained to identify a subset of elements within the metrics that, when input into the network, produce the best estimate of the piston or tip/tilt position relative to a reference telescope or an estimate of a combination of parameters, such as the average of a subset of telescopes. During active use of the system, metrics generated from the in-focus and out-of-focus images of the object scene and the trained neural networks are used to provide estimates of piston and/or tip/tilt positions which are in turn used to drive the pistons and/or tip/tilt controllers to correct for aberrant movement and keep the telescopes phased.", "Fast phase diversity wavefront correction using a neural network "]
["A hierarchical neural network for monitoring network functions and that functions as a true anomaly detector is disclosed. Detection of an anomaly is achieved by monitoring selected areas of network behavior, such as protocols, that are predictable in advance. Combining outputs of neural networks within the hierarchical network yields satisfactory anomaly detection.", "Hierarchial neural network intrusion detector "]
["N neural networks having different set-values are provided, where N is an integer greater than 2. Each neural network has plurality of artificial neurons and processes information. An optimal output detecting circuit receives outputs of the N neural networks and determines the optimal one of the neural networks based on the outputs of the N neural networks. An output circuit receives and outputs the output of the neural network detected by the optimal output detecting circuit.", "Neural network system having minimum energy function value "]
["An input is classified into one of a plurality of possible outputs. A top-level classifier generates an approximate identification for the input as one of the possible outputs and selects two or more neural networks corresponding to the approximate identification. The selected neural networks generate two or more identifications for the input as one or more of the possible outputs. A postprocessor classifies the input as one of the possible outputs in accordance with the two or more identifications of the selected neural networks. According to an alternative embodiment, a top-level classifier selects a subset of neurons of a neural network and the subset of neurons identifies the input as one of the possible outputs.", "Method and apparatus for hierarchical input classification using a neural network "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for recognizing speech using neural networks. One of the methods includes receiving an audio input; processing the audio input using an acoustic model to generate a respective phoneme score for each of a plurality of phoneme labels; processing one or more of the phoneme scores using an inverse pronunciation model to generate a respective grapheme score for each of a plurality of grapheme labels; and processing one or more of the grapheme scores using a language model to generate a respective text label score for each of a plurality of text labels.", "Recognizing speech using neural networks "]
["A neural network system is provided that models the system in a system model (12) with the output thereof providing a predicted output. This predicted output is modified or controlled by an output control (14). Input data is processed in a data preprocess step (10) to reconcile the data for input to the system model (12). Additionally, the error resulted from the reconciliation is input to an uncertainty model to predict the uncertainty in the predicted output. This is input to a decision processor (20) which is utilized to control the output control (14). The output control (14) is controlled to either vary the predicted output or to inhibit the predicted output whenever the output of the uncertainty model (18) exceeds a predetermined decision threshold, input by a decision threshold block (22). Additionally, a validity model (16) is also provided which represents the reliability or validity of the output as a function of the number of data points in a given data region during training of the system model (12). This predicts the confidence in the predicted output which is also input to the decision processor (20). The decision processor (20) therefore bases its decision on the predicted confidence and the predicted uncertainty. Additionally, the uncertainty output by the data preprocess block (10) can be utilized to train the system model (12).", "Method for training and/or testing a neural network with missing and/or incomplete data "]
["A method for predicting respiratory disturbances, and a method for developing such an artificial neural network. The inputs to the method and to the artificial neural network of the present invention are the answers given by a person to a series of questions. The output of the artificial neural network is a predicted respiratory disturbance index.", "Artificial neural network for predicting respiratory disturbances and method for developing the same "]
["A method for the supervised teaching of a recurrent neutral network (RNN) is disclosed. A typical embodiment of the method utilizes a large (50 units or more), randomly initialized RNN with a globally stable dynamics. During the training period, the output units of this RNN are teacher-forced to follow the desired output signal. During this period, activations from all hidden units are recorded. At the end of the teaching period, these recorded data are used as input for a method which computes new weights of those connections that feed into the output units. The method is distinguished from existing training methods for RNNs through the following characteristics: (1) Only the weights of connections to output units are changed by learning\u2014existing methods for teaching recurrent networks adjust all network weights. (2) The internal dynamics of large networks are used as a \u201creservoir\u201d of dynamical components which are not changed, but only newly combined by the learning procedure\u2014existing methods use small networks, whose internal dynamics are themselves completely re-shaped through learning.", "Method for supervised teaching of a recurrent artificial neural network "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for detecting objects in images. One of the methods includes receiving an input image. A full object mask is generated by providing the input image to a first deep neural network object detector that produces a full object mask for an object of a particular object type depicted in the input image. A partial object mask is generated by providing the input image to a second deep neural network object detector that produces a partial object mask for a portion of the object of the particular object type depicted in the input image. A bounding box is determined for the object in the image using the full object mask and the partial object mask.", "Object detection using deep neural networks "]
["A neural network system comprising input, intermediate and output layers interconnected through synapses, respectively is disclosed.", "Spatial light modulator and neural network "]
["A voltage-mode pulse width modulation (PWM) VLSI implementation of neural networks, comprising: a voltage-pulse converter for converting an input voltage into a neuron-state pulse; a synapse multiplier, including a multiplier cell for multiplying the neuron-state pulse by an input weight voltage and an integral and summation cell for integrating and summing up the multiplied output and producing a first output voltage; and a sigmoid circuit for converting the first output voltage into a second output voltage with the non-linear activation function of neuron.", "Voltage-mode pulse width modulation VLSI implementation of neural networks "]
["Face hallucination using a bi-channel deep convolutional neural network (BCNN), which can adaptively fuse two channels of information. In one example, the BCNN is implemented to extract high level features from an input image. The extracted high level features are combined with low level details in the input image to produce the higher resolution image. Preferably, a proper coefficient is obtained to adaptively combine the high level features and the low level details.", "Face Hallucination Using Convolutional Neural Networks "]
["In a neural network which includes one input layer, one or more intermediate layers and one output layer, neural elements in the input layer and neural elements in the intermediate layer are divided into groups. Arithmetic operations representing the coupling between the neural elements of the input layer and the neural elements of the intermediate layer are put into table form.", "Data processing using neural networks having conversion tables in an intermediate layer "]
["Various neural-network based surrogate model construction methods are disclosed herein, along with various applications of such models. Designed for use when only a sparse amount of data is available (a \u201csparse data condition\u201d), some embodiments of the disclosed systems and methods: create a pool of neural networks trained on a first portion of a sparse data set; generate for each of various multi-objective functions a set of neural network ensembles that minimize the multi-objective function; select a local ensemble from each set of ensembles based on data not included in said first portion of said sparse data set; and combine a subset of the local ensembles to form a global ensemble. This approach enables usage of larger candidate pools, multi-stage validation, and a comprehensive performance measure that provides more robust predictions in the voids of parameter space.", "Neural-Network Based Surrogate Model Construction Methods and Applications Thereof "]
["A method and system for predicting the resistance of a disease to a therapeutic agent is provided. Further provided is a method and system for designing a therapeutic treatment agent for a patient afflicted with a disease. Specifically, the methods use a trained neural network to interpret genotypic information obtained from the disease. The trained neural network is trained using a database of known or determined genotypic mutations that are correlated with phenotypic therapeutic agent resistance. The present invention also provides methods and systems for predicting the probability of a patient developing a genetic disease. A trained neural network for making such predictions is also provided. Also provided is a method and system for determining the genetic basis of therapeutic agent resistance.", "Method for predicting therapeutic agent resistance using neural networks "]
["Embodiments of the invention can include methods and systems for controlling clearances in a turbine. In one embodiment, a method can include applying at least one operating parameter as an input to at least one neural network model, modeling via the neural network model a thermal expansion of at least one turbine component, and taking a control action based at least in part on the modeled thermal expansion of the one or more turbine components. An example system can include a controller operable to determine and apply the operating parameters as inputs to the neural network model, model thermal expansion via the neural network model, and generate a control action based at least in part on the modeled thermal expansion.", "Methods and Systems for Neural Network Modeling of Turbine Components "]
["This control method is suitable to the control of voltage reactive-power of a power system. The method comprises a step of connecting a neuro controller in parallel to a control unit and connecting a neural network control-object simulator for simulating the object in parallel to the object, a step of letting the neural networks of the neuro controller and of the simulator go through a pre-learning phase so as to make input-output relations of the controller match those of the control unit, and input-output relations of the simulator match those of the object, a step of connecting the output of the controller to the input of the simulator, a step of letting the neural network of the controller go through a learning phase so as to make the output of the simulator match the input of the controller, and a step of letting the controller control the object after completing the learning phase.", "A control method using neural networks and a voltage/reactive power controller for a power system "]
["A semiconductor charge transfer synapse cell has a capacitor coupled between an input line and an intermediate node. A voltage pulse applied to the input line causes charge transfer from one summing line to another through a pair of series connected field-effect devices. Each of the devices has an associated gate potential which controls its resistance. In response to the low-to-high voltage transition of the input pulse current flows through the devices from the intermediate node to the summing lines. A high-to-low transition causes current to flow in the opposite direction. Because the relative conductances of the devices are different depending on the direction of current flow, a net charge is transferred from one summing line to the other. The amount of charge transferred is a function of the amplitude of the pulsed input, the gate potentials, and the capacitance value.", "Charge domain differential conductance synapse cell for neural networks "]
["The subject invention relates to a method and system for diagnosis of a medical condition. A specific embodiment of the subject invention utilizes a plurality of neural networks at a corresponding plurality of clinical sites to assist physicians in diagnosing a medical condition of a patient. Each of the plurality of neural networks can communicate with a server associated with a central neural network via, for example, the Internet. The server can receive patient data which can include, for example, images, patient information, parameters, biopsy information, and physician diagnoses. The central neural network can be trained on a large volume of medical cases, which come from the plurality of clinical sites. Even for relatively rare medical conditions, the neural network can be provided with diagnosed cases at an adequate rate for effective training of the central neural network. At appropriate times, the optimized parameters of the trained neural network can be transmitted to each of the plurality of neural networks at the clinical sites. The neural network at a site can thus assist a physician in reliably determining the nature and the likelihood of a medical condition even when it is dependent on a wide variety of patient data and even when the condition is relatively rare.", "Local diagnostic and remote learning neural networks for medical diagnosis "]
["A system and method for detecting behavior of a computing platform that includes obtaining platform data; for each data motif identifiers in a set data motif identifiers, performing data motif detection on data in an associated timescale, wherein a first data motif identifier operates on data in a first timescale, wherein a second data motif identifier operates on data in a second timescale, wherein the first timescale and second timescale are different; in a neural network model, synthesizing platform data anomaly detection with at least a set of features inputs from data motif detection of the set of motif identifiers; and signaling if a platform data anomaly is detected through the neural network model.", "System and method for detecting platform anomalies through neural networks "]
["A neural network is used to combine one or more estimates of a physiologic parameter with one or more associated signal quality metrics, creating a more accurate estimate of said physiologic parameter, as well as a second estimate of the accuracy of said physiologic parameter estimate.", "Estimation of a physiological parameter using a neural network "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for training a deep neural network. One of the methods includes generating a plurality of feature vectors that each model a different portion of an audio waveform, generating a first posterior probability vector for a first feature vector using a first neural network, determining whether one of the scores in the first posterior probability vector satisfies a first threshold value, generating a second posterior probability vector for each subsequent feature vector using a second neural network, wherein the second neural network is trained to identify the same key words and key phrases and includes more inner layer nodes than the first neural network, and determining whether one of the scores in the second posterior probability vector satisfies a second threshold value.", "Training multiple neural networks with different accuracy "]
["A digital neuron receiving inputs X1-Xn includes weighting elements W1-Wn, a processor P, and a non-linear compressing element C. Element C uses a piecewise linear approximation to the sigmoidal neuron activation function, which maps compactly into a digital integrated circuit realisation, and involves slopes of powers of two so that it may be implemented by shifting the lower order bits of the neuron output in dependence upon the higher order bits. <IMAGE>", "Digital neural networks "]
["The neural semiconductor chip first includes: a global register and control logic circuit block, a R/W memory block and a plurality of neurons fed by buses transporting data such as the input vector data, set-up parameters, etc., and signals such as the feed back and control signals. The R/W memory block, typically a RAM, is common to all neurons to avoid circuit duplication, increasing thereby the number of neurons integrated in the chip. The R/W memory stores the prototype components. Each neuron comprises a computation block, a register block, an evaluation block and a daisy chain block to chain the neurons. All these blocks (except the computation block) have a symmetric structure and are designed so that each neuron may operate in a dual manner, i.e. either as a single neuron (single mode) or as two independent neurons (dual mode). Each neuron generates local signals. The neural chip further includes an OR circuit which performs an OR function for all corresponding local signals to generate global signals that are merged in an on-chip common communication bus shared by all neurons of the chip. The R/W memory block, the neurons and the OR circuit form an artificial neural network having high flexibility due to this dual mode feature which allows to mix single and dual neurons in the ANN.", "Neural chip architecture and neural networks incorporated therein "]
["An air data sensing probe or MFP includes a barrel having multiple pressure sensing ports for sensing multiple pressures. Instrumentation coupled to the pressure sensing ports provides electrical signals related to the multiple pressures. A neural network, coupled to the instrumentation, receives as inputs the electrical signals related to the multiple pressures, and in response, the neural network provides, as an output, electrical signals indicative of at least one local air data parameter for the air data sensing probe.", "Multi-function air data probes employing neural networks for determining local air data parameters "]
["Power industry boiler tube failures are a major cause of utility forced outages in the United States, with approximately 41,000 tube failures occurring every year at a cost of $5 billion a year. Accordingly, early tube leak detection and isolation is highly desirable. Early detection allows scheduling of a repair rather than suffering a forced outage, and significantly increases the chance of preventing damage to adjacent tubes. The instant detection scheme starts with identification of boiler tube leak process variables which are divided into universal sensitive variables, local leak sensitive variables, group leak sensitive variables, and subgroup leak sensitive variables, and which may be automatically be obtained using a data driven approach and a leak sensitivity function. One embodiment uses artificial neural networks (ANN) to learn the map between appropriate leak sensitive variables and the leak behavior. The second design philosophy integrates ANNs with approximate reasoning using fuzzy logic and fuzzy sets. In the second design, ANNs are used for learning, while approximate reasoning and inference engines are used for decision making. Advantages include use of already monitored process variables, no additional hardware and/or maintenance requirements, systematic processing does not require an expert system and/or a skilled operator, and the systems are portable and can be easily tailored for use on a variety of different boilers.", "Artificial neural network and fuzzy logic based boiler tube leak detection systems "]
["Dynamically updating neural network systems may be implemented to generate, train, evaluate and update artificial neural network data structures used by content distribution networks. Such systems and methods described herein may include generating and training neural networks, using neural networks to perform predictive analysis and other decision-making processes within content distribution networks, evaluating the performance of neural networks, and generating and training pluralities of replacement candidate neural networks within cloud computing architectures and/or other computing environments.", "Dynamically updated neural network structures for content distribution networks "]
["Certain aspects of the present disclosure relate to a technique for adaptive structural delay plasticity applied in spiking neural networks. With the proposed method of structural delay plasticity, the requirement of modeling multiple synapses with different delays can be avoided. In this case, far fewer potential synapses should be modeled for learning.", "Method and apparatus for structural delay plasticity in spiking neural networks "]
["A plurality of neural networks or other models can be used in employee selection technologies. A hiring recommendation can be based at least on processing performed by a plurality of neural networks. For example, parallel or series processing by neural networks can be performed. A neural network can be coupled to one or more other neural networks. A binary or other n-ary output can be generated by one or more of the neural networks. In a series arrangement, candidates can be processed sequentially in multiple stages, and those surviving the stages are recommended for hire.", "Employee selection via multiple neural networks "]
["Different candidate windows in an image are identified, such as by sliding a rectangular or other geometric shape of different sizes over an image to identify portions of the image (groups of pixels in the image). The candidate windows are analyzed by a set of convolutional neural networks, which are cascaded so that the input of one convolutional neural network layer is based on the input of another convolutional neural network layer. Each convolutional neural network layer drops or rejects one or more candidate windows that the convolutional neural network layer determines does not include an object (e.g., a face). The candidate windows that are identified as including an object (e.g., a face) are analyzed by another one of the convolutional neural network layers. The candidate windows identified by the last of the convolutional neural network layers are the indications of the objects (e.g., faces) in the image.", "Object detection using cascaded convolutional neural networks "]
["An artificial neural network has a plurality of output circuits individually perturbable for memory modification or learning by the network. The network has a plurality of synapses individually connecting each of a plurality of inputs to each output circuit. Each synapse has a weight determining the effect on the associated output circuit of a signal provided on the associated input, and the synapse is addressable for selective variation of the weight. A perturbation signal is provided to one input, while data signals are provided to others of the inputs, so that perturbation of each output circuit may be controlled by varying the weights of a set of the synapses connecting the perturbation signal to the output circuits. An output circuit may be selected for perturbation by loading an appropriate weight in the synapse connecting the perturbation signal to the output circuit while zeroing the weights of the synapses connecting the perturbation signal to other output circuits. Where the weights are provided by devices incapable of repeated cycles of zeroing and reloading, each synapse connecting the perturbation intput to an output circuit has an addressable switch which is closed for perturbation of this output circuit and which is open at other times. Perturbations of different output circuits may be balanced by varying the weights of the set of synapses connected to the perturbation input or by varying the weights of another set of the synapses connected to one of inputs which receives a balancing signal.", "Method and circuits for neuron perturbation in artificial neural network memory modification "]
[null, "Cascaded depth neural network-based face attribute recognition method "]
["A neural network comprising a plurality of neurons in which any one of the plurality of neurons is able to associate with itself or another neuron in the plurality of neurons via active connections to a further neuron in the pluarlity of neurons.", "Neural networks with learning and expression capability "]
["A method and apparatus for color matching are provided, in which paint recipe neural networks are utilized. The color of a standard is expressed as color values. The neural network includes an input layer having nodes for receiving input data related to paint bases. Weighted connections connect to the nodes of the input layer and have coefficients for weighting the input data. An output layer having nodes are either directly or indirectly connected to the weighted connections and generates output data related to color values. The data to the input layer and the data from the output layer are interrelated through the neural network's nonlinear relationship. The paint color matching neural network can be used for, but not limited to, color formula correction, matching from scratch, effect pigment identification, selection of targets for color tools, searching existing formulas for the closest match, identification of formula mistakes, development of color tolerances and enhancing conversion routines.", "Computer-implemented neural network color matching formulation applications "]
["Embodiments of the invention provide neuromorphic-synaptronic systems, including neuromorphic-synaptronic circuits implementing spiking neural network with synaptic weights learned using simulation. One embodiment includes simulating a spiking neural network to generate synaptic weights learned via the simulation while maintaining one-to-one correspondence between the simulation and a digital circuit chip. The learned synaptic weights are loaded into the digital circuit chip implementing a spiking neural network, the digital circuit chip comprising a neuromorphic-synaptronic spiking neural network including plural synapse devices interconnecting multiple digital neurons.", "Neuromorphic and synaptronic spiking neural network with synaptic weights learned using simulation "]
["A method of operating a neural network for ecological and biological system modeling having a plurality of hidden layer neurons said method comprising: a plurality of network inputs and at least one network output, said plurality of neurons, each receiving a plurality of inputs applied to the network, reproduces the network using a regression model, and compares the output values with given target values, and using the comparison and goodness of fit to set the learning rules. The network does not require repetitive training and yields a global minimum for each given set of input variables.", "Neural network for modeling ecological and biological systems "]
["A neural network classifier provides the ability to separate and categorize multiple arbitrary and previously unknown audio sources down-mixed to a single monophonic audio signal. This is accomplished by breaking the monophonic audio signal into baseline frames (possibly overlapping), windowing the frames, extracting a number of descriptive features in each frame, and employing a pre-trained nonlinear neural network as a classifier. Each neural network output manifests the presence of a pre-determined type of audio source in each baseline frame of the monophonic audio signal. The neural network classifier is well suited to address widely changing parameters of the signal and sources, time and frequency domain overlapping of the sources, and reverberation and occlusions in real-life signals. The classifier outputs can be used as a front-end to create multiple audio channels for a source separation algorithm (e.g., ICA) or as parameters in a post-processing algorithm (e.g. categorize music, track sources, generate audio indexes for the purposes of navigation, re-mixing, security and surveillance, telephone and wireless communications, and teleconferencing).", "Neural network classifier for separating audio sources from a monophonic audio signal "]
["A neural network shell has a defined interface to an application program. By interfacing with the neural network shell, any application program becomes a neural network application program. The neural network shell contains a set of utility programs that transfers data into and out of a neural network data structure. This set of utility programs allows an application program to define a new neural network model, create a neural network data structure, train a neural network, and run a neural network. Once trained, the neural network data structure can be transported to other computer systems or to application programs written in different computing languages running on similar or different computer systems.", "Neural network shell for application programs "]
["A multi-layered pattern recognition neural network that comprises an input layer (28) that is operable to be mapped onto an input space comprising a scan window (12). Two hidden layers (30) and (32) map the input space to an output layer (16). The hidden layers utilize a local receptor field architecture and store representations of objects within the scan window (12) for mapping into one of a plurality of output nodes. Each of the plurality of output nodes and associated representations stored in the hidden layer define an object that is centered within the scan window (12). When centered, the object and its associated representation in the hidden layer result in activation of the associated output node. The output node is only activated when the character is centered in the scan window (12). As the scan window (12) scans a string of text, the output nodes are only activated when the associated character moves within the substantial center of the scan window. The network is trained by backpropagation through various letter string such that the letter by itself within the substantial center of the scan window (12) will be recognized, and also the letter with constraints of additional letters on either side thereof will also be recognized. In addition, the center between characters is recognized when it is disposed substantially in the center of scan window (12), and a space is recognized when it is disposed within the substantial center of the scan window (12).", "Pattern recognition neural network "]
["A neural network system and method of operating same wherein input data are initialized, then mapped onto a predetermined array for learning or recognition. The mapped information is divided into sub-input data or receptive fields, which are used for comparison of the input information with prelearned information having similar features, thereby allowing for correct classification of the input information. The receptive fields are shifted before the classification process, in order to generate a closest match between features which may be shifted at the time of input, and weights of the input information are updated based upon the closest-matching shifted position of the input information.", "Receptive field neural network with shift-invariant pattern recognition "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating parse trees for input text segments. One of the methods includes obtaining an input text segment, processing the input text segment using a first long short term memory (LSTM) neural network to convert the input text segment into an alternative representation for the input text segment, and processing the alternative representation for the input text segment using a second LSTM neural network to generate a linearized representation of a parse tree for the input text segment.", "Generating parse trees of text segments using neural networks "]
["Techniques for reconstructing a signal encoded with a time encoding machine (TEM) using a recurrent neural network including receiving a TEM-encoded signal, processing the TEM-encoded signal, and reconstructing the TEM-encoded signal with a recurrent neural network.", "Encoding and decoding machine with recurrent neural networks "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for identifying the language of a spoken utterance. One of the methods includes receiving input features of an utterance; and processing the input features using an acoustic model that comprises one or more convolutional neural network (CNN) layers, one or more long short-term memory network (LSTM) layers, and one or more fully connected neural network layers to generate a transcription for the utterance.", "Convolutional, long short-term memory, fully connected deep neural networks "]
["Automatic white balancing and/or autoexposure as useful in a digital camera extracts color channel gains from comparisons of image colors with reference colors under various color temperature illuminants and/or extracts exposure settings from illuminance mean, illuminance variance, illuminance minimum, and illuminance maximum in areas of an image with a trained neural network.", "Automatic white balancing via illuminant scoring autoexposure by neural network mapping "]
["Objects are classified using a normalized cross correlation (NCC) measure to compare two images acquired under non-uniform illumination conditions. An input pattern is classified to assign a tentative classification label and value. The input pattern is assigned to an output node in the radial basis function network having the largest classification value. If the input pattern and an image associated with the node, referred to as a node image, both have uniform illumination, then the node image is accepted and the probability is set above a user specified threshold. If the test image or the node image are not uniform, then the node image is not accepted and the classification value is kept as the value assigned by the classifier. If both the test image and the node image are not uniform, then an NCC measure is used and the classification value is set as the NCC value.", "Computer vision system and method employing illumination invariant neural networks "]
["A computing device, which may be implemented as an integrated circuit, is constructed of a microprocessor and one or more neural network co-processors. The microprocessor normally executes programs which transfer data to the neural network co-processors, which are used to compute complicated mathematical functions. Direct Memory Access (DMA) is also used to transfer data. Each neural network co-processor interfaces to the microprocessor in a manner substantially similar to that of a conventional memory device. The co-processor does not require any instructions and is configured to execute mathematical operations simply by being pre-loaded with gating functions and weight values. In addition, the co-processor executes a plurality of arithmetic operations in parallel, and the results of such operations are simply read from the co-processor.", "Computer utilizing neural network and method of using same "]
["The noise associated with conventional techniques for evolutionary improvement of neural network architectures is reduced so that of an optimum architecture can be determined more efficiently and more effectively. Parameters that affect the initialization of a neural network architecture are included within the encoding that is used by an evolutionary algorithm to optimize the neural network architecture. The example initialization parameters include an encoding that determines the initial nodal weights used in each architecture at the commencement of the training cycle. By including the initialization parameters within the encoding used by the evolutionary algorithm, the initialization parameters that have a positive effect on the performance of the resultant evolved network architecture are propagated and potentially improved from generation to generation. Conversely, initialization parameters that, for example, cause the resultant evolved network to be poorly trained, will not be propagated. In accordance with a second aspect of this invention, the encoding also includes parameters that affect the training process, such as the duration of the training cycle, the training inputs applied, and so on. In accordance with a third aspect of this invention, the same set of training or evaluation inputs are applied to all members whose performances are directly compared.", "Method for improving neural network architectures using evolutionary algorithms "]
[null, "Intelligent gearbox fault diagnosis method based on mixed inference and neural network "]
["A low-order model (LOM) of biological neural networks and its mathematical equivalents including the clusterer interpreter probabilistic associative memory (CIPAM) are disclosed. They are artificial neural networks (ANNs) organized as networks of processing units (PUs), each PU comprising artificial neuronal encoders, synapses, spiking/nonspiking neurons, and a scheme for maximal generalization. If the weights in the artificial synapses in a PU have been learned (and then fixed) or can be adjusted by the unsupervised accumulation rule and the unsupervised covariance rule (or supervised covariance rule), the PU is called unsupervised (or supervised) PU. The disclosed ANNs, with these Hebbian-type learning rules, can learn large numbers of large input vectors with temporally/spatially hierarchical causes with ease and recognize such causes with maximal generalization despite corruption, distortion and occlusion. An ANN with a network of unsupervised PUs (called clusterer) and offshoot supervised PUs (called interpreter) is an architecture for many applications.", "Artificial Neural Networks based on a Low-Order Model of Biological Neural Networks "]
["A multi-layered type neural network for a fuzzy reasoning in which an if-part of a fuzzy rule is expressed by a membership function and a then-part of the fuzzy rule is expressed by a linear expression, the network comprising an if-part neural network for receiving if-part variables of all the fuzzy rules and calculating if-part membership values of all the fuzzy rules, an intermediate neural network for calculating, as a truth value of the premise of each fuzzy rule, a product of the if-part membership values for all the if-part variables, and a then-part neural network for calculating a first sum of the truth values of the premise of all the fuzzy rules, a second sum of a product of the truth values of the premise of all the fuzzy rules and then-part outputs of all the fuzzy rules, and dividing the second sum by the first sum to obtain a quotient as an inferential result.", "Neural network for fuzzy reasoning "]
["The invention relates to an evolutionary optimization method. First, an initial population of individuals is set up and an original fitness function is applied. Then the offspring individuals having a high evaluated quality value as parents are selected. In a third step, the parents are reproduced to create a plurality of offspring individuals. The quality of the offspring individuals is evaluated selectively using an original fitness function or an approximate fitness function. Finally, the method returns to the selection step until a termination condition is met. The step of evaluating the quality of the offspring individuals includes grouping all offspring individuals in clusters, selecting for each cluster one or a plurality of offspring individuals, resulting in altogether selected offspring individuals, evaluating the selected offspring individuals by the original fitness function, and evaluating the remaining offspring individuals by means of the approximate fitness function.", "Reduction of fitness evaluations using clustering techniques and neural network ensembles "]
["A method for interpreting electromagnetic survey data includes acquiring electromagnetic survey data near a top of a portion of the Earth's subsurface. An initial model of the portion of the Earth's subsurface is generated. The model includes at least spatial distribution of formation resistivity within the portion. The initial model is applied to an artificial neural network trained to generate expected electromagnetic survey instrument response to the initial model. The acquired electromagnetic survey data are compared to an output of the artificial neural network. The initial model is adjusted, and the applying the model to the artificial neural network and the comparing are repeated until differences between the output of the network and the acquired survey data fall below a selected threshold.", "Fast 3D inversion of electromagnetic survey data using a trained neural network in the forward modeling branch "]
[null, "Intrusion detecting method based on semi-supervised neural network "]
["An artificial neural network is provided using a digital architecture having feedforward and feedback processors interconnected with a digital computation ring or data bus to handle complex neural feedback arrangements. The feedforward processor receives a sequence of digital input signals and multiplies each by a weight in a predetermined manner and stores the results in an accumulator. The accumulated values may be shifted around the computation ring and read from a tap point thereof, or reprocessed through the feedback processor with predetermined scaling factors and combined with the feedforward outcomes for providing various types neural network feedback computations. Alternately, the feedforward outcomes may be placed sequentially on a data bus for feedback processing through the network. The digital architecture includes a predetermined number of data input terminals for the digital input signal irrespective of the number of synapses per neuron and the number of neurons per neural network, and allows the synapses to share a common multiplier and thereby reduce the physical area of the neural network. A learning circuit may be utilized in the feedforward processor for real-time updating the weights thereof to reflect changes in the environement.", "Digital neural network computation ring "]
["A photo stimulated and controlled imaging neural network for providing self generating learning sets and associative memory and programmability. An image to be recognized or detected is transferred to an imaging plane, which can be as simple as a lens or as complicated as a cathode ray tube. The imaging plane whose contents forms the input for a photo receptor array transfers the stimulus from the object to the photoreceptor array. The photoreceptor array responds to the stimulus provided by the imaging plane with various couplings between an array of neuron amplifiers. The photo receptor array comprises a plurality of synaptic photo controlled resistors which respond to the stimulus provided by the imaging plane. The individual neuron amplifiers settle into a set of on or off binary states based on the couplings of the photo controlled resistors which comprise the receptor array. The output states are equally weighted and as a whole constitute a particular learning set which is then passed onto a gate array where it can be utilized to make various decisions.", "Photo stimulated and controlled imaging neural network "]
["Method intended for real-time modelling, by neural networks, of hydrodynamic characteristics of multiphase flows in transient phase in pipes. In order to specifically take account of the possible flow regimes of fluids in pipes, various neural or \u201cexpert\u201d models are formed for several flow regimes (or subregimes) in the whole of the variation range of the hydrodynamic characteristics of the flows (preferably for each one of them), as well as a neural model estimating the probability of belonging of the flows to each flow regime or subregime, knowing some of the characteristics thereof. The probabilities obtained are used for weighting the estimations delivered by each neural model, the result of the weighted sum being then the estimation eventually retained. Applications to various industries and notably for modelling of hydrocarbon flows in pipelines.", "Method for modelling hydrodynamic characteristics of multiphase flows using neuronal networks "]
[null, "Rapid target detection method based on convolutional neural network "]
["Certain aspects of the present disclosure provide methods and apparatus for a continuous-time neural network event-based simulation. This model is flexible, has rich behavioral options, can be solved directly, and is low complexity. One example method generally includes determining a first state of a neuron model at or shortly after a first event, wherein the neuron model has a closed-form solution in continuous time; and determining a second state of the neuron model at or shortly after a second event, based on the first state. Dynamics of the first and second states are coupled to the neuron model only at the first and second events, respectively, and are decoupled between the first and second events.", "Dynamical event neuron and synapse models for learning spiking neural networks "]
[null, "Sewage-disposal soft measurement method on basis of integrated neural network "]
["A method is described for improving the prediction accuracy and generalization performance of artificial neural network models in presence of input-output example data containing instrumental noise and/or measurement errors, the presence of noise and/or errors in the input-output example data used for training the network models create difficulties in learning accurately the nonlinear relationships existing between the inputs and the outputs, to effectively learn the noisy relationships, the methodology envisages creation of a large-sized noise-superimposed sample input-output dataset using computer simulations, here, a specific amount of Gaussian noise is added to each input/output variable in the example set and the enlarged sample data set created thereby is used as the training set for constructing the artificial neural network model, the amount of noise to be added is specific to an input/output variable and its optimal value is determined using a stochastic search and optimization technique, namely, genetic algorithms, the network trained on the noise-superimposed enlarged training set shows significant improvements in its prediction accuracy and generalization performance, the invented methodology is illustrated by its successful application to the example data comprising instrumental errors and/or measurement noise from an industrial polymerization reactor and a continuous stirred tank reactor (CSTR).", "Performance of artificial neural network models in the presence of instrumental noise and measurement errors "]
["The present invention relates to an artificial neural network (ANN) representation for system dynamics models (SDMs) and its applications in model construction and policy design. It first shows that, by a special design of the mapping scheme, a given flow diagram (FD) (i.e., traditional representation) can be transformed into a corresponding model in the representation of partial recurrent networks (PRNs) that will correctly behave like the one it mimics. The present invention shows the equivalence of the two types of representations, both structurally and mathematically. With the additional representation, an automatic learning method that can assist in the construction of SDMs is proposed, which starts from an initial skeleton of a PRN (mapping from an initial FD), identifies the cause-effect relationships within the SDM by neural learning, and then converts it back to the corresponding FD. The composite approach makes model construction simpler and more systematic. Similarly, by assigning an intended behavior pattern as a set of training examples for a given SDM, it can learn a new system structure with the PRN representation; the differences between the original and new structures lead to considerations of policy design. Besides, one can also allow the learning process to restart after some period of using a model so that it has a chance to evolve and adapt to temporal changes in the environment. This touches an area that has not yet been well solved; i.e., feedback to a system might change not only its behavior but also the internal system structure since, for example, a social system is usually organic.", "Neural network representation for system dynamics models, and its applications "]
["Methods are provided for developing medical diagnostic tests using decision-support systems, such as neural networks. Patient data or information, typically patient history or clinical data, are analyzed by the decision-support systems to identify important or relevant variables and decision-support systems are trained on the patient data. Patient data are augmented by biochemical test data, or results, where available, to refine performance. The resulting decision-support systems are employed to evaluate specific observation values and test results, to guide the development of biochemical or other diagnostic tests, to assess a course of treatment, to identify new diagnostic tests and disease markers, to identify useful therapies, and to provide the decision-support functionality for the test. Methods for identification of important input variables for medical diagnostic tests for use in training the decision-support systems to guide the development of the tests, for improving the sensitivity and specificity of such tests, and for selecting diagnostic tests that improve overall diagnosis of, or potential for, a disease state and that permit the effectiveness of a selected therapeutic protocol to be assessed are provided. The methods for identification can be applied in any field in which statistics are used to determine outcomes. A method for evaluating the effectiveness of any given diagnostic test is also provided.", "Method for selecting medical and biochemical diagnostic tests using neural network-related applications "]
["A method of accelerating the training of an artificial neural network uses a computer configured as an artificial neural network with a network input and a network output and having a plurality of interconnected units arranged in layers including an input layer and an output layer. Each unit has a multiplicity of unit inputs and a set of variables for operating upon a unit inputs to provide a unit output in the range between binary 1 and binary 0. A plurality of training examples is serially provided to the network input and the network output is observed. The computer is programmed with a back propagation algorithm for changing each set of variables in response to feedback representing differences between the network output for each example and the desired output. The examples are iterated while the output of a unit is observed. The feedback to a unit is adjusted so that a larger feedback is obtained when the output of the unit is near binary 1 or binary 0 than when the output is midrange between binary 1 or binary 0.", "Fast neural network training "]
["An optical proximity corrected mask design is generated from a given a target mask design by processing the target mask design through a feature trained neural network, configured to perform an optical proximity correction of geometric features, to obtain a representation of a first corrected mask design. The target mask design is processed in parallel through a rule processor, configured to perform placement of sub-resolution geometric features relative to geometric features in the target mask design, to obtain a representation of a second corrected mask design. A layout reassembler operates to generate a corrected mask design through an overlaid composition of said first and second corrected mask designs.", "Neural network-based system and methods for performing optical proximity correction "]
["A probabilistic neural network (PNN) comprises a layer L1 of input nodes, a layer L2 of exemplar nodes, a layer L3 of primary Parzen nodes, a layer L4 of sum nodes, and optionally a layer L5 of output nodes. Each exemplar node determines the degree of match between a respective exemplar vector and an input vector and feeds a respective primary Parzen node. The exemplar and primary Parzen nodes are grouped into design classes, with a sum node for each class which combines the outputs of the primary Parzen nodes for that class and feeds a corresponding output node. The network includes for each primary Parzen node (e.g. L3-2-3P) for the design classes a secondary Parzen node (L3-2-3S), the secondary Parzen nodes all feeding a null class sum node (L4-0). Each secondary Parzen node has a Parzen function with a lower peak amplitude and a broader spread than the corresponding primary Parzen node, and is fed from the exemplar node for that primary Parzen node. The secondary Parzen nodes in effect detect input vectors which are \"sufficiently different\" from the design classes--that is, null class vectors. The network is applicable to banknote recognition and authentication, the null class corresponding to counterfeit banknotes.", "Neural network for banknote recognition and authentication "]
[null, "Short-time wind speed forecasting method based on neural network "]
["An elevator control apparatus capable of predicting reversion floors of elevator cages accurately. The control apparatus comprises a neural network, in which traffic state data are fetched into the neural network, so that predicted values of floors where the moving direction of each cage is reversed are calculated as predicted reversion floors. In the elevator control apparatus, reversion floors near true reversion floors can be predicted flexibly correspondingly to traffic state and traffic volume.", "Elevator control apparatus using neural network to predict car direction reversal floor "]
["Genetically adaptive neural network systems and methods provide environmentally adaptable classification algorithms for use, among other things, in multi-static active sonar classification. Classification training occurs in-situ with data acquired at the onset of data collection to improve the classification of sonar energy detections in difficult littoral environments. Accordingly, in-situ training sets are developed while the training process is supervised and refined. Candidate weights vectors evolve through genetic-based search procedures, and the fitness of candidate weight vectors is evaluated. Feature vectors of interest may be classified using multiple neural networks and statistical averaging techniques to provide accurate and reliable signal classification.", "Genetically adaptive neural network classification systems and methods "]
["A visual information processing device has a pair of neural networks which respectively comprise an upper layer and a lower layer of the device. Each of the pair of neural networks comprises a semiconductor integrated circuit having a plurality of neuron circuit regions which are disposed in a matrix form, each of the neuron circuit regions performing a neuron function; a molecule film having a photoelectric function and provided on the semiconductor integrated circuit, the molecule film having (i) a plurality of Tij signal input sections each performing a wiring function among the plurality of neuron circuit regions, in each of which a Tij signal representing the bonding strength among the plurality of neuron circuit regions is optically written, and (ii) a plurality of video input sections each performing a sensor function of sensing a visual image in which one pixel corresponds to one neuron circuit region; and a wiring for electrically connecting the semiconductor integrated circuit and the molecule film. Each of the plurality of neuron circuit regions is bonded with the neighboring neuron circuit regions in each of the pair of neural networks comprising the upper and lower layers, and each of the plurality of neuron circuit regions is bonded with the corresponding one between the pair of neural networks.", "Neural network system for image processing "]
["A system and method identify the person who is using a keyboard based on keystroke latencies as the person types certain key combinations. In some embodiments the latencies are monitored as the person types a password, while in others they are monitored as the person types other information and continues to use the computer. In some embodiments the identification yields a binary result (whether the latency profile matches the profile stored for a particular user), while in others a confidence level is given. A mismatch, or a confidence level below a particular threshold, results in a request for further identity verification, creation of a log entry, immediate notification of responsible personnel, or denial of access (or continued access).", "Identity authentication based on keystroke latencies using a genetic adaptive neural network "]
["A speech recognition apparatus has: a speech input unit for inputting a speech; a speech analysis unit for analyzing the inputted speech to output the time series of a feature vector; a candidates selection unit for inputting the time series of a feature vector from the speech analysis unit to select a plurality of candidates of recognition result from the speech categories; and a discrimination processing unit for discriminating the selected candidates to obtain a final recognition result. The discrimination processing unit includes three components in the form of a pair generation unit for generating all of the two combinations of the n-number of candidates selected by said candidate selection unit a pair discrimination unit for discriminating which of the candidates of the combinations is more certain for each of all n C2 -number of combinations (or pairs) on the basis of the extracted result of the acoustic feature intrinsic to each of said candidate speeches and a final decision unit for collecting all the pair discrimination results obtained from the pair discrimination unit for each of all the n C2 -number of combinations (or pairs) to decide the final result. The pair discrimination unit handles the extracted result of the acoustic feature intrinsic to each of the candidate speeches as fuzzy information and accomplishes the discrimination processing on the basis of fuzzy logic algorithms, and the final decision unit accomplishes its collections on the basis of the fuzzy logic algorithms.", "Speech recognition apparatus using neural network and fuzzy logic "]
["A method is described for providing an estimate of the state of a moving contact. The method comprises providing a device for estimating the state of the contact, inputting information about a location of an observer platform at particular time intervals and information from at least one sensor about a position of the moving contact relative to the observer platform at each time interval into the device, transforming the inputted information into a series of geographical grids with one grid being formed for each reading of the at least one sensor; combining grids corresponding to similar time intervals into a series of consolidated grid representations; and analyzing the series of consolidated grid representations to produce an estimate of the state of the contact at a final point in time where an observation was made. The device of the present invention includes a grid stimulation block for forming the geographical grids, a fusion block for forming the consolidated grid representations, a correlation block for providing a path likelihood vector, and an estimation block for providing the desired estimate.", "Neural network based data fusion system for source localization "]
["A system permitting one or more users to navigate a wide-angle image, and in particular a panoramic image, using a neural network to correct distortion in that image. To train the neural network, a calibration pattern which defines an array of calibration points is disposed to occupy a field of view of a wide-angle imaging apparatus, and a calibration image is thereby captured by the apparatus. Respective view directions of the calibration points and the positions of these points in the calibration image are used as data for training the neural network to correctly match view directions to corresponding intra-image positions. Subsequently, to generate an undistorted sub-image of an arbitrary distorted wide-angle image, an array of display pixel positions are expressed as a corresponding set of view directions, and the neural network used to convert these to a corresponding set of positions within the wide-angle image, with the video attributes at these positions being then assigned to the corresponding pixels of the sub-image.", "Panoramic image navigation system using neural network for correction of image distortion "]
["A neural network development and data analysis tool provides significantly simplified network development through use of a scripted programming language, such as Extended Markup Language, or a project \u201cwizard.\u201d The system also provides various tools for analysis and use of a trained artificial neural network, including three-dimensional views, skeletonization, and a variety of output module options. The system also provides for the possibility of autonomous evaluation of a network being trained by the system and the determination of optimal network characteristics for a given set of provided data.", "Neural network development and data analysis tool "]
["The present invention provides an Internet search engine system and method that improves searching for documents or pages by processing the characteristics of a pool of data through a neural network governed by a set of rules and fuzzy logic applications. The rules and applications may be implemented at the input (or low) level or the computational/output (or high) level. Search terms and personal and situational data may activate various rule sets, and learning from human and machine feedback adjust and recombine the rule sets to improve accuracy for future searches as well as reduce computation time.", "Search engine with neural network weighting based on parametric user data "]
["A neural network device includes internal data input lines, internal data output lines, coupling elements provided at the connections of the internal data input lines and the internal data output lines, word lines each for selecting one row of coupling elements. The coupling elements couple, with specific programmable coupling strengths, the associated internal data input lines to the associated internal data output lines. In a program mode, the internal data output lines serve as signal lines for transmitting the coupling strength information. Each of the coupling elements includes memories constituted of cross-coupled inverters for storing the coupling strength information, first switching transistors responsive to signal potentials on associated word lines for connecting the memories to associated internal data output lines, second switching elements responsive to signal potentials on associated internal data input lines for transmitting the storage information in the memories to the associated internal data output lines. Each of the internal data output lines has a pair of first and second internal data output lines.", "Coupling element for semiconductor neural network device "]
["A system (1) for analysing the condition of biological organs is based on a set of parameters that have been identified as the factors that will determine the diagnosis of a particular organ. A neural network (4) is trained and used to analyse the values of these parameters as they relate to each other, and to first determine whether there is an error in the input data, and if not, then to generate a diagnosis from the analysis. In training the neural network (4), an initial set of training input/output pairs (101 and 102) are inputted into the neural network (4) that simulate parameters for correct diagnosis, and erroneous parameter combinations. The neural network (4), when presented with parameter data for making an actual diagnosis, is capable of either indicating that an error may have occurred in the measurements or test results obtained as input data, or correctly diagnosing the condition of the organ within a given tolerance level.", "A system for diagnosing biological organs using a neural network that recognizes random input error "]
["An artificial neuron for use in a neural processing network comprises a plurality of input signal lines, an arrangement for computing a nonlinear function of the sum of the inputs multiplied by associated weights, and a saturating delta-sigma modulator which oversamples the computed value and produces an encoded neuron output signal. Conversion of signals for use by these neurons preferably is performed by delta-sigma modulators at the inputs to the neurons, which may be incorporated directly into sensors. Processing of the output signals from the neuron includes low-pass filtering and decimation. The present invention may be used in many diverse areas. For example, arrays of sensors with delta signal modulators may be coupled with a network of the neurons to form an intelligent vision system. Linear signal processing, both conventional and adaptive, can be done by a simple neuronal system that operates linearly.", "Sensor for use in a neural network "]
["An outer product neural network provides a predetermined number of processing elements for extracting principal components of an input vector. The residual vector of the network when cascaded into a similar outer product neural network provides additional principal components defining a subspace orthogonal to the subspace defined by the principal components of the first network.", "Outer product neural network "]
["A signal processing method for efficiently searching an optimum solution in a neural network by including a term of a nonlinear resistance in an equation of motion and changing such nonlinear resistance periodically. According to the method, the range of absolute values of connection weights between units in the neural network is limited by the equation of motion, hence preventing a prolonged search time that may otherwise be caused by excessive extension of the search scope beyond the requisite. A plurality of patterns are previously embedded or stored in the neural network and, upon input of a predetermined key pattern, the nonlinear resistance is changed periodically to recall a pattern similar to the key pattern, whereby any desired pattern can be searched or retrieved with rapidity and facility out of the complicated patterns. A process of calculating the next position of an articulated robot corresponding to an optimum solution is repeated while periodically changing a nonlinear resistance included in another equation of the positional energy of the robot, thereby acquiring the data of the robot path up to a desired goal.", "Method of processing signals within a neural network to position arobot "]
["Bottleneck link speed, or the transmission speed of the slowest link within a path between two nodes, is determining by transmitting a sequence of ICMP ECHO data packets from the source node to the target node at a selected interval and measuring the return data packet intervals. Rather than using statistical analysis methods, the return data packet interval measurements are input into an adaptive resonance theory neural network trained with the expected interval for every known, existing network transmission speed. The neural network will then classify the return data packet interval measurements, indicating the bottleneck link speed. Since most of the computation\u2014that required to train the neural network\u2014may be performed before the data packet interval measurements are made rather than after, the bottleneck link speed may be determined from the return data packet interval measurements significantly faster and using less computational resources than with statistical analysis techniques. Moreover, fewer measurements are required to determine bottleneck link speed to the same degree of accuracy.", "Use of adaptive resonance theory (ART) neural networks to compute bottleneck link speed in heterogeneous networking environments "]
["A neural network learning system is applied to extensive use in applications such as pattern and character recognizing operations, various controls, etc. The neural network learning system operates on, for example, a plurality of neural networks each having a different number of intermediate layer units to efficiently perform a learning process at a high speed with a reduced amount of hardware. A neural network system having a plurality of hierarchical neural networks each having an input layer, one or more intermediate layers and output layers is formed from a common input layer shared among two or more neural networks, or the common input layer and one or more intermediate layers and a learning controller for controlling a learning process performed by a plurality of neural networks.", "Neural network learning system "]
["A face recognition method using artificial neural network and an apparatus thereof are provided. The apparatus comprises an eigenpaxel selection unit which generates eigenpaxels indicating characteristic patterns of a face and selects a predetermined number of eigenpaxels among the generated eigenpaxels; an eigenfiltering unit which filters an input image with the selected eigenpaxels; a predetermined number of neural networks, each of which corresponds to one of the selected eigenpaxels, receives an image signal which is filtered by the corresponding eigenpaxel, and output a face recognition result; and a determination unit which receives the recognition result from each of the neural networks and outputs a final face recognition result of the input image.", "Face recognition method using artificial neural network and apparatus thereof "]
["A system and method, which enable precise and automatic identification of characters, perform and calibrate data verification to ensure data reliability. The system can process these identified characters, such as override adverse conditions, adjusting and correcting unclear characters and their images.", "Multi-level neural network based characters identification method and system "]
["Fully automated methods and systems for processing complex data sets to identify abnormalities are described. In one embodiment, the system includes wavelet processing, recursive processing to determine prominent features, and then utilizing feed forward neural networks (FFNNs) to classify feature vectors generated in the wavelet and recursive processing. With respect to wavelet processing, multiresolution (five-level) and multidirection (two-dimensional) wavelet analysis with quadratic spline wavelets is performed to transform each image. The wavelets are a first-order derivative of a smoothing function and enhance the edges of image objects. Because two-dimensional wavelet transforms quantize an image in terms of space and spatial frequency and can be ordered linearly, the data is processed recursively to determine prominent features. A neural network approach derived from sequential recursive auto-associative memory is then used to parse the wavelet coefficients and hierarchy data. Since the wavelet coefficients are continuous, linear output instead of sigmoidal output is used. This variation is therefore referred to as linear output sequential recursive auto-associative memory, or LOSRAAM. The objective of training the LOSRAAM network is to have the output exactly match the input. Context units arising from serial evaluation of the wavelet coefficient triplets may be collected as vectors. These vectors are subjected to cluster analysis. This analysis yields a number of identifiable and discrete states. From these states, feature vectors are created. Each element in the feature vector represents the number of times the corresponding state from the above cluster analysis is found. Then, feed forward neural networks (FFNNs) are trained to classify feature vectors.", "Neural network based methods and systems for analyzing complex data "]
["A multifunctional neural network system for prediction which includes memory components to store previous values of data within a network. The memory components provide the system with the ability to learn relationships/patterns existent in the data over time.", "Multifunctional Neural Network System and Uses Thereof "]
["A prediction model is generated by training an ensemble of multiple neural networks, and estimating the performance error of the ensemble. In a subsequent stage a subsequent ensemble is trained using an adapted training set so that the preceding bias component of performance error is modelled and compensated for in the the new ensemble. In each successive stage the error is compared with that of all of the preceding ensembles combined. No further stages take place when there is no improvement in error. Within each stage, the optimum number of iterative weight updates is determined, so that the variance component of performance error is minimised.", "Neural network training "]
["The invention relates to a method for controlling process events of a technical plant. In order to permit a simultaneous and coherent assessment of relevant process variables of the plant, it is proposed to use a neural analysis on the basis of self-organizing neural maps to evaluate the relevant process variables in relation to one another by realizing a topology-maintaining nonlinear projection of data from the relevant process variables onto a multidimensional neural map.", "Method for controlling process events using neural network "]
["A system, method and computer program product for implementation of a Aggregate Neural Semantic Network, which stores the relationships and semantic connections between the key search words for each user. The Aggregate Neural Semantic Network processes the search results produced by a standard search engine such as, for example, Google or Yahoo!. The set of hits produced by the standard search engine is processed by the Aggregate Neural Semantic Network, which selects the hits that are relevant to a particular user based on the previous search queries made by the user. The Aggregate Neural Semantic Network can also use the connections between the terms (i.e., key words) that are most frequently used by all of the previous Aggregate Neural Semantic Network users. The Aggregate Neural Semantic Network is constantly updating and self-teaching. The more user queries are processed by the Aggregate Neural Semantic Network, the more comprehensive processing of search engine outputs is provided by the Aggregate Neural Semantic Network to the subsequent user queries.", "Semantic neural network for aggregating query searches "]
["A speech recognition system for recognizing the remote-controlling vocal commands of TV sets and VCRs comprises a microphone for receiving the speech pronounced by a user; a speech analyzer for analyzing the speech input via the microphone; circuitry for detecting a vocal section of the speech from the speech analyzer and performing a time-axis normalization and a binarization for the detected vocal section; and a multilayer neural network for receiving the binarization data from the aforementioned circuitry and then performing the learning, to thereby output the speech recognition result. Accordingly, the present invention can enhance the recognition ratio of speech.", "Speech recognition system utilizing a neural network "]
["A novel neural network implementation for logic systems has been developed. The neural network can determine whether a particular logic system and knowledge base are self-consistent, which can be a difficult problem for more complex systems. Through neural network hardware using parallel computation, valid solutions may be found more rapidly than could be done with previous, software-based implementations. This neural network is particularly suited for use in large, real-time problems, such as in a real-time expert system for testing the consistency of a programmable process controller, for testing the consistency of an integrated circuit design, or for testing the consistency of an \"expert system.\" This neural network may also be used as an \"inference engine,\" i.e., to test the validity of a particular logical expression in the context of a given logic system and knowledge base, or to search for all valid solutions, or to search for valid solutions consistent with given truth values which have been \"clamped\" as true or false. The neural network may be used with many different types of logic systems: those based on conventional \"truth table\" logic, those based on a truth maintenance system, or many other types of logic systems. The \"justifications\"  corresponding to a particular logic system and knowledge base may be permanently hard-wired by the manufacturer, or may be supplied by the user, either reversibly or irreversibly.", "Neural network logic system "]
["A learning method supervised by a binary teacher signal for a binary neural network comprises at least an error signal generator 10 for weighting factor updating, which generates an error signal for weighting factor updating having an opposite polarity to that of a difference signal between an output unit signal of the binary neural network and the binary teacher signal on an output unit whereat a binary output unit signal coincides with the binary teacher signal, and an amplitude which decreases by increase of distance from the binary teacher signal, when an absolute value of the difference signal is smaller than a threshold, generates an error signal which has the same polarity as that of the difference signal and an amplitude smaller than that of the difference signal, when the absolute value of the difference signal is larger than the threshold, or generates an error signal which has an amplitude equal to or smaller than that of the difference signal on an output unit providing a wrong binary output unit signal which is different from the binary teacher signal. Updating the weighting factors by the error signal which is optimally generated according to discrimination between the correct binary output unit signal and the wrong one, can provide a binary neural network which converges very quickly and reliably to obtain a desired binary output and also realizes a high generalization ability.", "Learning method for multi-level neural network "]
["A neural network IC 31 includes n dedicated processing elements (PEs) 62, an output register 66 for storing the PEs' outputs so that they are immediately accessible to all of the PEs, a number of output circuits 78 that are connected to selected PEs to provide binary outputs, and a timing circuit 74. Each of the PEs includes a weight memory 90 for storing input, output and bias weight arrays, a first in first out (FIFO) memory 88 for storing input data, a dot product circuit 92 and an activation circuit 94. The dot product circuit computes a dot product of the input weight array and the contents of the FIFO memory, a dot product of the output weight array and the contents of the output register, a dot product of the bias value and a constant, and sums the three results. The activation circuit maps the output of the dot product circuit through an activation function to produce the PE's output. The inclusion of a memory 90 that stores both input and output weight arrays in conjunction with the output register 66 allows the PEs to be configured to implement arbitrary feed-forward and recurrent neural network architectures.", "Configurable neural network integrated circuit "]
["A self adaptive hierarchical target identification neural network pattern recognition system (10) is constructed utilizing four basic modules. The first module is a segmenter and preprocessor (14) which accepts gray level image data (12) and is based on the Boundary Contour System neural network. The segmenter and preprocessor (14) output is fed to a feature extractor (16) which comprises a first layer of a neocognitron. The feature extractor (16) output is fed to a pattern recognizer (18) which comprises two layers of the neocognitron. The pattern recognizer (18) produces as output a real valued vector representation which encodes the object to be identified. This vector representation is fed to a classifier (20) which comprises a back propagation neural network. The pattern recognition system (10) can classify large numbers of objects from raw sensor data and is relatively translation, rotation and scale invariant", "Self adaptive hierarchical target indentification and recognition neural network "]
["A method is described for providing an estimate of the state of a moving  tact in a three dimensional ocean. The method comprises the steps of providing a device for estimating the state of the contact, inputting into the device information about a location of an observer during a sequence of time, information from at least one sensor about the position of the contact relative to the observer during the time sequence, and a knowledge vector, transforming the inputted information into a series of three dimensional geographical grids, and analyzing the geographical grids to produce an estimate of the state of the contact with respect to the location of the observer. The device for providing the estimate of the state of the moving contact is a neurally inspired contact estimation device. The device includes a grid stimulation block for transforming the inputted information into the three dimensional geographical grids, a fusion block where grids corresponding to similar time intervals are combined or fused, a correlation block for providing constraints such as constant speed and heading and for producing a path likelihood vector, and an estimate block for providing the estimate of the state of the moving contact. The device further includes a controller for providing knowledge to the aforementioned blocks.", "Neural network based three dimensional ocean modeler "]
["A plant (72) is operable to receive control inputs c(t) and provide an output y(t). The plant (72) has associated therewith state variables s(t) that are not variable. A control network (74) is provided that accurately models the plant (72). The output of the control network (74) provides a predicted output which is combined with a desired output to generate an error. This error is back propagated through an inverse control network (76), which is the inverse of the control network (74) to generate a control error signal that is input to a distributed control system (73) to vary the control inputs to the plant (72) in order to change the output y(t) to meet the desired output. The control network (74) is comprised of a first network NET 1 that is operable to store a representation of the dependency of the control variables on the state variables. The predicted result is subtracted from the actual state variable input and stored as a residual in a residual layer (102). The output of the residual layer (102) is input to a hidden layer (108) which also receives the control inputs to generate a predicted output in an output layer (106). During back propagation of error, the residual values in the residual layer (102) are latched and only the control inputs allowed to vary.", "Residual activation neural network "]
["Disclosed is an image processing apparatus having an input device for inputting binary image data comprising a plurality of pixels which include a pixel of interest that is to be subjected to multivalued conversion, the plurality of pixels being contained in an area that is asymmetrical with respect to the position of the pixel of interest, and an multivalued converting device for executing processing, by a neural network, to restore the input binary image data to multivalued image data for the pixel of interest, whereby multivalued image data is estimated from binarized image data. It is possible to reduce the number of pixels referred to in arithmetic operations performed in the neural network.", "Image processing apparatus using neural network "]
["Certain aspects of the present disclosure support a technique for updating the state of an artificial neuron. A first state of the artificial neuron can be first determined, wherein a neuron model for the artificial neuron has a closed-form solution in continuous time and wherein state dynamics of the neuron model are divided into two or more regimes. An operating regime for the artificial neuron can be determined based, at least in part, on the first state. The state of the artificial neuron can be updated based, at least in part, on the first state of the artificial neuron and the determined operating regime.", "Dynamical event neuron and synapse models for learning spiking neural networks "]
["A control device for controlling the learning of a neural netowrk includes a monitor for monitoring weight values of synapse connections between units of the neural netowrk during learning of the neural network so as to update these weight values. When one of the weight values satisfies a preset condition, the weight value is updated to a predetermined value such that configuration of the neural network is determined in an optimum manner.", "Control device for controlling learning of a neural network "]
["A neuron (100) has a null-inhibiting function so that null inputs do not affect the output of the neuron (100) or updating of its weights. The neuron (100) provides a net value based on a sum of products of each of several inputs, and corresponding weight and null values, and provides an output in response to the net value. A neural network (40) which uses such a neuron (100) has a first segmented layer (41) in which each segment (50-52) corresponds to a manufacturing process step (60-62). Each segment of the first layer (41) receives as inputs measured values associated with the process step (60-62). A second layer (42) connected to the first layer (4l), is non-segmented to model the entire manufacturing process (80). The first (41) and second (42) layers are both unsupervised and competitive. A third layer (43) connected to the second layer (42) then estimates parameters of the manufacturing process (80) and is unsupervised and noncompetitive.", "Neural network, neuron, and method for recognizing a missing input valve "]
["The invention relates to a method and a neural network for computer-assisted knowledge management, based on a neural network (1) that is formed by a computer in its memory location. The invention method and neural network are especially for use in a decentralized, computer-assisted patent system that can be used via the Internet system, in the broad sense. The neural network (1) forms a system of artificial intelligence (KI), covering a fundamental knowledge base in the form of computer-readable texts. The neural network (1) consists of elements (2) that are associated with each other and weighted in relation to each other so that the sets of knowledge available can be managed and analyzed in relation to each other by computer means.", "Neural network for computer-aided knowledge management "]
["A neural network is trained and used to reduce artifacts in spatial domain representations of images that were compressed by a transform method and then decompressed. For example, the neural network can be trained and used to reduce artifacts such as blocking and ringing artifacts in JPEG images.", "Image artifact reduction using a neural network "]
[null, "Distribution network fault location method utilizing natural frequency and artificial neural network "]
[null, "Neural network generalized inverse permanent magnetism synchronous machine decoupling controller structure method without bearing "]
["A neural network for classifying input vectors to an outcome class under the assumption that the classes are characterized by mixtures of component populations having a multivariate Gaussian likelihood distribution. The neural network comprises an input layer for receiving components of an input vector, two hidden layers for generating a number of outcome class component values, and an output layer. The first hidden layer includes a number of first layer nodes each connected receive input vector components and generate a first layer output value representing the absolute value of the sum of a function of the difference between each input vector component and a threshold value. The second hidden layer includes a plurality of second layer nodes, each second layer node being connected to the first layer nodes and generating an outcome class component value representing a function related to the exponential of the negative square of a function of the sum of the first layer output values times a weighting value. The output layer includes a plurality of output nodes, each associated with an outcome class, for generating a value that represents the likelihood that the input vector belongs to that outcome class.", "Neural network architecture for gaussian components of a mixture density function "]
["A method of allocating subscriber lines in a telecommunications network into speed bins. With the method, more intelligent business actions can then be taken in the provision of high-speed data services over the subscriber lines. For example, only qualified lines might be used for high-speed data services, with the other lines being allocated to POTS service. The lines are divided into speed bins using a pair of neural networks, with one predicting upstream speed and one predicting downstream speed. The combined predictions are then mapped to a speed bin, which is the basis for further business actions. The disclosure describes that the neural networks are created using conditional fuzzy logic to precondition the neural networks by line speed.", "Speed binning by neural network "]
[null, "Adaptive driver monitoring and warning system for motor vehicle - uses neural nets, image acquisition and processing to determine approach to safety limits in order to generate warnings "]
["An optical image transmitted through a photographing lens is incident on a light-receiving unit of a two-dimensional matrix. An output from the light-receiving unit is input to a first arithmetic logic unit, and the first arithmetic logic unit calculates actual object brightness values in consideration of an aperture value of an aperture. An output from the first arithmetic logic unit is supplied to a multiplexer and a neural network. The neural network determines a main part of the object from a pattern of brightness values of the respective photoelectric transducer elements and outputs a position signal of the main part. The multiplexer selectively passes the brightness value of the photoelectric transducer element corresponding to the main part of the object from the outputs generated by the first arithmetic logic unit. An output from the multiplexer is supplied to a second arithmetic logic unit. The second arithmetic logic unit performs a focus detection calculation based on only the brightness of the main part. The photographing lens is moved along the optical axis, thereby performing a focusing operation.", "Focus detection apparatus using neural network means "]
[null, "Deepening controlling method of underactuated automatic underwater vehicle based on neural network back stepping method "]
["A system and method for automatically selecting a color space for use in compressing and decompressing a texture image that automatically determines a compression color space for each texture image. The invention selects a compression color space manually, or preferable, using a neural network algorithm. The invention initializes the neural network that includes an input layer of neurons and a hidden layer of neurons. Each input layer neuron has an associated weight that is equal to the combination of the weights of a Y neuron, an A neuron, and a B neuron that is associated with each input layer neuron. The texel image is reduced into a representative sample of colors and input vectors from the texel image are randomly selected. For each input vector, the invention determines the two input layer neurons that most closely match the input vector and modifies the weights of the two input layer neurons to more closely match the value of the input vector. The weights of the hidden layer neurons represent the Y, A, and B-channel values of the optimal compression color space.", "System and method for selecting a color space using a neural network "]
[null, "Intelligent alarm monitoring method of neural network "]
[null, "Chinese question-answering system based on neural network "]
["A hybrid analyzer having a data derived primary analyzer and an error correction analyzer connected in parallel is disclosed. The primary analyzer, preferably a data derived linear model such as a partial least squares model, is trained using training data to generate major predictions of defined output variables. The error correction analyzer, preferably a neural network model is trained to capture the residuals between the primary analyzer outputs and the target process variables. The residuals generated by the error correction analyzer is summed with the output of the primary analyzer to compensate for the error residuals of the primary analyzer to arrive at a more accurate overall model of the target process. Additionally, an adaptive filter can be applied to the output of the primary analyzer to further capture the process dynamics. The data derived hybrid analyzer provides a readily adaptable framework to build the process model without requiring up-front knowledge. Additionally, the primary analyzer, which incorporates the PLS model, is well accepted by process control engineers. Further, the hybrid analyzer also addresses the reliability of the process model output over the operating range since the primary analyzer can extrapolate data in a predictable way beyond the data used to train the model. Together, the primary and the error correction analyzers provide a more accurate hybrid process analyzer which mitigates the disadvantages, and enhances the advantages, of each modeling methodology when used alone.", "Hybrid linear-neural network process control "]
["An Artificial Neural Network (ANN) based search method and system for enhancing and assisting the task of specifying the required information in the query by combining the user's original query with additional information previously provided by the expert users. That is, the ANN based search system utilizes the expert community feedback in predicting the relevance of particular documents and dynamically builds statistical associations between the queries and known solutions, i.e., relevant documents, identified by the expert users.", "Neural network feedback for enhancing text search "]
["A malfunction diagnostic and repair guidance system and method wherein a matrix of numbers indicating the state of a complex binary system is used as an input vector for a neural network pattern processing capability that is focused to distinguish malfunction types of patterns. The neural network capability provides two complementary network types to classify and generalize the binary matrix. An interactive operator interface is updated with each repair after the root cause and is proposed repair of a malfunction is identified.", "Neural network diagnostic classification of complex binary systems "]
["A neural network component includes a plurality of inputs, at least one processing element, at least one output, and a digital memory storing values at addresses respectively corresponding to the at least one processing element, wherein the at least one processing element is arranged to receive a value from the digital memory in response to an input signal, and is instructed to execute one of a plurality of operations by the value that is received from the digital memory.", "Neural network component "]
["A method is disclosed for computer-based control of the timing and level of gear shifts in a multi-speed automatic transmission operated in combination with an internal combustion engine and interposed fluid torque converter. The computer containing power control module signals gear shifts in response to its repeated cyclic processing of engine and transmission operation parameters including torque converter slippage. Here, such slippage is estimated using a neural network with suitable such parameters as input data. In preferred modes of operation, different neural networks are available for selection and use by the computer in different modes of engine-transmission operation.", "Neural network-based virtual sensor for automatic transmission slip "]
["A system and method for processing patient polysomnograph data are provided. An abstractor obtains raw patient polysomnograph data and generates a subset of the data to include selected factors, including data clusters. The subset of the patient polysomnograph data is transferred to two or more neural networks that process the data and generate sleep classification data. An integrator obtains the sleep classification data from the two or more neural networks by integrating the sleep classification data from each neural network. A cumulative sleep stage score is generated including confidence values and accuracy estimations for review.", "System and method for processing patient polysomnograph data utilizing multiple neural network processing "]
["A system and method for semantic extraction using a neural network architecture includes indexing each word in an input sentence into a dictionary and using these indices to map each word to a d-dimensional vector (the features of which are learned). Together with this, position information for a word of interest (the word to labeled) and a verb of interest (the verb that the semantic role is being predicted for) with respect to a given word are also used. These positions are integrated by employing a linear layer that is adapted to the input sentence. Several linear transformations and squashing functions are then applied to output class probabilities for semantic role labels. All the weights for the whole architecture are trained by backpropagation.", "Fast semantic extraction using a neural network architecture "]
["A method and system for beta-token partitioning a target expert system program. The target expert system program is first compiled to form a RETE network for execution on a single processor, the compilation including directives for collecting selected processing statistics. The target expert system program is then executed on a single processor, generating during execution processing statistics in connection with each node of the RETE network. The processing statistics are then applied to a programmed neural network to identify nodes in the RETE network for beta-token partitioning, and the target expert system program is then recompiled to form a RETE network for execution on multiple processors, the RETE network being beta-token partitioned at nodes identified by the neural network.", "Neural network for performing beta-token partitioning in a RETE network "]
[null, "Access network selection method based on neural network and fuzzy logic "]
["A self managed ad hoc communications network nodes and node mobility management. Nodes include an Artificial Neural Network (ANN) that determines connection to other network nodes. The ANN may use free space propagation link life estimation, inverse modeling for partition prediction, Stochastic Approximation, and/or coarse estimation. The node includes storage storing network tables and matrices indicating network connectivity and connection to other nodes. Also, a wireless communications unit provides for wireless communicating with other nodes.", "Neural network-based node mobility and network connectivty predictions for mobile ad hoc radio networks "]
[null, "Soft-sensing modeling method and soft meter of multi-model neural network in biological fermentation process "]
["Logging systems and methods are disclosed to reduce usage of radioisotopic sources. Some embodiments comprise collecting at least one output log of a training well bore from measurements with a radioisotopic source; collecting at least one input log of the training well bore from measurements by a non-radioisotopic logging tool; training a neural network to predict the output log from the at least one input log; collecting at least one input log of a development well bore from measurements by the non-radioisotopic logging tool; and processing the at least one input log of the development well bore to synthesize at least one output log of the development well bore. The output logs may include formation density and neutron porosity logs.", "Neural network based well log synthesis with reduced usage of radioisotopic sources "]
["An automatic machine health monitoring system which combines the use of vibration analysis and self organising map neural networks to facilitate fault detection and diagnosis with training examples taken only from the machine when in a good condition. Component specification data is used to determine a set of key frequencies, the amplitudes of which are used as the input parameters of the self organising map network. The networks outputs are in the form of individual distance values from each of the key frequencies. The control computer uses this information to detect and diagnose faults in the machine under examination. <IMAGE>", "Machine monitoring using neural network "]
["A method for training a neural network includes receiving labeled training data at a master node, generating, by the master node, partitioned training data from the labeled training data and a held-out set of the labeled training data, determining a plurality of gradients for the partitioned training data, wherein the determination of the gradients is distributed across a plurality of worker nodes, determining a plurality of curvature matrix-vector products over the plurality of samples of the partitioned training data, wherein the determination of the plurality of curvature matrix-vector products is distributed across the plurality of worker nodes, and determining, by the master node, a second-order optimization of the plurality of gradients and the plurality of curvature matrix-vector products, producing a trained neural network configured to perform a structured classification task using a sequence-discriminative criterion.", "Training Deep Neural Network Acoustic Models Using Distributed Hessian-Free Optimization "]
[null, "Neural network control system for nonlinear process "]
["System and method for optimization of a design associated with a response function, using a hybrid neural net and support vector machine (NN/SVM) analysis to minimize or maximize an objective function, optionally subject to one or more constraints. As a first example, the NN/SVM analysis is applied iteratively to design of an aerodynamic component, such as an airfoil shape, where the objective function measures deviation from a target pressure distribution on the perimeter of the aerodynamic component. As a second example, the NN/SVM analysis is applied to data classification of a sequence of data points in a multidimensional space. The NN/SVM analysis is also applied to data regression.", "Hybrid neural network and support vector machine method for optimization "]
[null, "Neural network nonuniformity correction method based on scene statistics "]
["A circuit for implementing a neural network comprises a one dimensional systolic array of processing elements controlled by a microprocessor. The one dimensional systolic array can implement weighted sum and radial based type networks including neurons with a variety of different activation functions. Pipelined processing and partitioning is used to optimize data flows in the systolic array. Accordingly, the inventive circuit can implement a variety of neural networks in a very efficient manner.", "One dimensional systolic array architecture for neural network "]
[null, "Method and device for classifying images on basis of convolutional neural network "]
[null, "Neural network modeling method and system "]
["A Forward Feed Neural Network is disclosed using data flow techniques on a data flow microprocessor. As a result of this invention, a neural network is provided that has the capacity of \"learning\" to distinguish among patterns of data which may differ recognizably from idealized cases, and is able to perform pattern recognition faster while utilizing less memory and fewer clock cycles than neural networks implemented on sequential processors. This implementation is simpler and faster because of an inherent similarity between the flow of information in the brain and in data flow architecture.", "Feed-forward neural network "]
["The operation of an electronic neural computer is described. This electronic neural computer solves for the optimal path in a space of \"cost functions\" which are represented as delays at the nodes of a grid (in two, three, four, or more dimensions). Time gating by delays lets the optimal solution thread the maze of the network first. The neural computer starts to compute all possible paths through the cost function field and shuts down after the first (optimal solution) emerges at the target node. The cost function delays are set from outside the neural computer architecture.", "Path cost computing neural network "]
["A method for representing an input image includes the steps of applying a trained neural network on the input image, selecting a plurality of feature maps, determining a location of each of the plurality of feature maps in an image space of the input image, defining a plurality of interest points of the input image, and employing the plurality of interest points for representing the input image for performing a visual task. The plurality of feature maps are selected of an output of at least one selected layer of the trained neural network according to values attributed to the plurality of feature maps by the trained neural network. The plurality of interest points of the input image are defined based on the locations corresponding to the plurality of feature maps.", "Neural network image representation "]
["An efficient neural network computing technique capable of synthesizing two sets of output signal data from a single input signal data set. The method and device of the invention involves a unique integration of autoassociative and heteroassociative neural network mappings, the autoassociative neural network mapping enabling a quality metric for assessing the generalization or prediction accuracy of the heteroassociative neural network mapping.", "Autoassociative-heteroassociative neural network "]
["The present invention is predicated upon the fact that an emission trace from a plasma glow used in fabricating integrated circuits contains information about phenoma which cause variations in the fabrication process such as age of the plasma reactor, densities of the wafers exposed to the plasma, chemistry of the plasma, and concentration of the remaining material. In accordance with the present invention, a method for using neural networks to determine plasma etch end-point times in an integrated circuit fabrication process is disclosed. The end-point time is based on in-situ monitoring of the optical emission trace. The back-propagation method is used to train the network. More generally, a neural network can be used to regulate control variables and materials in a manufacturing process to yield an output product with desired quality attributes. An identified process signature which reflects the relation between the quality attribute and the process may be used to train the neural network.", "Active neural network control of wafer attributes in a plasma etch process "]
["A dosimetry system and method characterized by use a plurality of radiation sensitive elements to monitor exposure to a radiation field composed of one or more types of radiation at one or more different energies; reading the radiation sensitive elements in a reader after irradiation by the radiation field to obtain element outputs; and supplying the element outputs to a trained neural network computer apparatus wherein the element outputs are analyzed to provide an output indicative of the radiation field.", "Multi-element dosimetry system using neural network "]
[null, "Pulmonary nodule benignity and malignancy predicting method based on convolutional neural networks "]
[null, "Real-time face recognition method based on deep neural network "]
["A neural network determines optimal control inputs for a linear quadratic discrete-time process at M sampling times, the process being characterized by a quadratic cost function, p state variables, and r control variables. The network includes N=(p+r)M neurons, a distinct neuron being assigned to represent the value of each state variable at each sampling time and a distinct neuron being assigned to represent the value of each control variable at each sampling time. An input bias connected to each neuron has a value determined by the quandratic cost function for the variable represented by the neuron. Selected connections are provided between the output of each neuron and the input of selected other neurons in the network, each such connection and the strength of each such connection being determined by the relationship in the cost function between the variable represented by the connected output neuron and the variable represented by the connected input neuron, such that running the neural network for a sufficient time to minimize the cost function will produce optimum values for each control variable at each sampling time.", "Discrete-time optimal control by neural network "]
["An optical information processor for use as a matrix vector multiplier comprises a vector input spatial light modulator (1) and an optically addressed weight matrix spatial light modulator (3). A read beam (10) passes through the input modulator (1) and the weight modulator and onto a combined output transducer and error spatial light modulator (5). The error modulator (5) is then controlled in accordance with the difference between a target output vector and the output vector from the transducer (5), and modulates an update beam (11) which then passes through the input modulator (1) and onto the weight modulator (3). The weight modulator (3) represents a two-dimensional array of optical attenuation values which are updated in accordance with the optical radiation incident thereon during updating.", "3 layer liquid crystal neural network with output layer displaying error value for optical weight updating "]
["A pattern recognition apparatus and a method for operating same. The apparatus includes a volume holographic medium (4) having a plurality of Fourier-space volume holograms representing pattern templates stored within. The apparatus further includes a spatial light modulator (1) and a phase encoder (2). The phase encoder has an output optically coupled to the medium by a first Fourier transform lens (3). The spatial light modulator spatially modulates a spatially uniform laser beam (7) in accordance with an unknown pattern. The two-dimensional phase encoder causes the spatially modulated laser beam to be spatially distributed prior to application to the medium. The apparatus also includes a detector (6, 11) having an input optically coupled by a second Fourier transform lens (5) means to an angular spectrum of plane waves generated by the medium in response to the output of the spatial modulator, phase encoder, and first Fourier lens. The detector detects plane waves that correspond to vector inner products generated within medium (4) in response to the unknown pattern. The apparatus further contains a means (12) for nonlinearly processing the output of detector (6, 11) and a means (13) by which the output of nonlinear processing means (12) may be temporarily stored.", "Multi-layer opto-electronic neural network "]
["A method of adapting a neural network of an automatic speech recognition device, includes the steps of: providing a neural network including an input stage, an intermediate stage and an output stage, the output stage outputting phoneme probabilities; providing a linear stage in the neural network; and training the linear stage by means of an adaptation set; wherein the step of providing the linear stage includes the step of providing the linear stage after the intermediate stage.", "Method of adapting a neural network of an automatic speech recognition device "]
["Methods, apparatus, and other embodiments associated with detecting mitosis in breast cancer pathology images by combining handcrafted (HC) and convolutional neural network (CNN) features in a cascaded architecture are described. One example apparatus includes a set of logics that acquires an image of a region of tissue, partitions the image into candidate patches, generates a first probability that the patch is mitotic using an HC feature set and a second probability that the patch is mitotic using a CNN-learned feature set, and classifies the patch based on the first probability and the second probability. If the first and second probabilities do not agree, the apparatus trains a cascaded classifier on the CNN-learned feature set and the HC feature set, generates a third probability that the patch is mitotic, and classifies the patch based on a weighted average of the first probability, the second probability, and the third probability.", "Automatic Detection Of Mitosis Using Handcrafted And Convolutional Neural Network Features "]
["In selected embodiments a recommendation generator builds a network of interrelationships between venues, reviewers and users based on their attributes and reviewer and user reviews of the venues which are enhanced by dynamic resonance between source sites. The recommendation engine in certain embodiments determines recommended venues based on user attributes and venue preferences by performing geometric contextualization on generated recommendation sets and determining recommendation resonance with past recommendations. Remote businesses may also link with the recommendation generator to receive recommendations custom-tailored to their business. In selected embodiments, interconnectivity augmentation provides for enhanced neural network topology and recommendations for foreign locales. Various user interfaces are also contemplated thereby providing users with a view of the neural network topology as well as the ability to collaboratively determine meeting places.", "Systems and methods for providing enhanced neural network genesis and recommendations "]
["A multi-layer electrically trainable analog neural network employing multiplexed output neurons having inputs organized into two groups, external and recurrent (i.e., feedback). Each layer of the network comprises a matrix of synapse cells which implement a matrix multiplication between an input vector and a weight matrix. In normal operation, an external input vector coupled to the first synaptic array generates a Sigmoid response at the output of a set of neurons. This output is then fed back to the next and subsequent layers of the network as a recurrent input vector. The output of second layer processing is generated by the same neurons used in first layer processing. Thus, the neural network of the present invention can handle N-layer operation by using recurrent connections and a single set of multiplexed output neurons.", "Multi-layer neural network employing multiplexed output neurons "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating representations of input sequences. One of the methods includes receiving a grapheme sequence, the grapheme sequence comprising a plurality of graphemes arranged according to an input order; processing the sequence of graphemes using a long short-term memory (LSTM) neural network to generate an initial phoneme sequence from the grapheme sequence, the initial phoneme sequence comprising a plurality of phonemes arranged according to an output order; and generating a phoneme representation of the grapheme sequence from the initial phoneme sequence generated by the LSTM neural network, wherein generating the phoneme representation comprises removing, from the initial phoneme sequence, phonemes in one or more positions in the output order.", "Generating representations of input sequences using neural networks "]
["A data processing system is provided that consists of a connection of a first neural network (N1) with at least one other neural network (N21, N22, . . . , N2n). The first neural network (N1) and the at least one other neural network (N21, N22, . . . , N2n) is an associative memory. First input data (E0) are supplied to both the first neural network (N1) and also to at least the one other neural network (N21, N22, . . . , N2n) Data (E11, E12, . . . , E1n) which are evaluated by at least the one other neural network (N21, N22, . . . , N2n), are supplied as further input data (E1) to the first neural network (N1).", "Parallel neural networks having one neural network providing evaluated data to another neural network "]
["A neural network controller in parallel with a proportional-plus-integral (PI) feedback controller in a control system. At least one input port of the neural network for receiving an input signal representing a condition of a process is included. A first set of data is obtained that includes a plurality of output values of the neural network obtained during a training period thereof using a plurality of first inputs representing a plurality of conditions of the process. The process/plant condition signals generally define the process/plant, and may include one set-point as well as signals generated using measured systems variables/parameters. In operation, the neural network contributes to an output of the PI controller only upon detection of at least one triggering event, at which time a value of the first set of data corresponding with the condition deviation is added-in thus, contributing to the proportional-plus-integral feedback controller. The triggering event can be characterized as (a) a change in any one of the input signals greater-than a preselected amount, or (b) a detectable process condition deviation greater-than a preselected magnitude, for which an adjustment is needed to the process/plant being controlled. Also a method for controlling a process with a neural network controller operating in parallel with a IP controller is included.", "Combined proportional plus integral (PI) and neural network (nN) controller "]
["Long and short term memory equations for neural networks are implemented by means of exchange of signals which carry information in the form of both binary and continuously modulated energy emissions. In one embodiment, array of parallel processors exhibits behavior of cooperative-competitive neural networks. Parallel bus interconnections and digital and analog processing of analog information contained in the exchanged energy emissions are employed with generally local synchronization of the processors. Energy emission and detection is modulated as a function of a random code.", "Neural network using random binary code "]
[null, "Improved positioning method of indoor fingerprint based on clustering neural network "]
["A method for configuring long short-term memory (LSTM) in a spiking neural network includes decoding input spikes into analog values within the LSTM. The method further includes implementing the LSTM based on an encoded representation of the analog values. The implementing can include encoding the analog values using base expansive coding, rate coding, latency coding or synaptic weight coding.", "Long short-term memory using a spiking neural network "]
["A memory device includes a nonlinear electric conductivity element, a charge accumulation element, and a switching element. The nonlinear electric conductivity element has an insulating layer having opposite surfaces, and first and second conductive layers respectively formed on the opposite surfaces of the insulating layer. The nonlinear electric conductivity element receives an external write signal applied to one of the first and second conductive layers, and outputs a signal having nonlinear electric conductivity characteristics from the other of the first and second conductive layers. The charge accumulation element has charge accumulation characteristics and is connected to receive and store the signal output from the other of the first and second conductive layers. The switching element is ON/OFF-controlled upon reception of the signal charge stored in the charge accumulation element. The switching element receives an external read voltage to read out the signal charge stored in the charge accumulation element as storage data. A memory apparatus includes a plurality of memory devices each having the nonlinear electric conductivity element the charge accumulation element and the switching element. The plurality of memory devices are connected in a matrix form such that the switching elements in at least two memory devices can commonly receive the read voltage and can commonly read out the storage data.", "Memory device and memory apparatus using the same suitable for neural network "]
["AVO anomalies are classified in near-offset and far-offset seismic data volumes, by first calculating a plurality of initial AVO seismic attributes representative of the offset seismic data volumes. A probabilistic neural network is constructed from the calculated initial AVO seismic attributes. AVO anomaly classifications are calculated in a portion of the offset seismic data volumes. The preceding steps are repeated until the calculated AVO anomaly classifications in the portion of the offset seismic data volumes are satisfactory. AVO anomaly classifications are calculated throughout the offset seismic data volumes using the constructed probabilistic neural network.", "Method for classifying AVO data using an interpreter-trained neural network "]
["An air data sensing probe such as a multi-function probe includes a barrel having multiple pressure sensing ports for sensing multiple pressures. Instrumentation coupled to the pressure sensing ports provides electrical signals indicative of the pressures. An inertial navigation system input of the probe receives electrical signals indicative of inertial navigation data for the aircraft. A neural network of the probe receives as inputs the electrical signals indicative of the multiple pressures and the electrical signals indicative of the inertial navigation data. The neural network is trained or configured to provide as an output, electrical signals indicative of an air data parameter.", "Multi-function air data probes using neural network for sideslip compensation "]
["An Artificial Neural Network (110) includes a hidden layer (209) of distance metric computer nodes (210, 214, 218) that evaluate distances of a input vector from metric space centers, an additional layer of adaptable infinite logic aggregators (236, 240, 244) that combine the per-unit distance output values by the distance metric computer nodes (210, 214, 218) using adaptable infinite logic. In certain embodiments the adaptable infinite logic aggregators include veracity signal pre-processors (602, 702) that can be configured to make inferences in a continuum from positive to negative including no inference from each distance and infinite logic connective signal processors (604, 702) that can implement a continuum of functions covering the range of fuzzy logic union operators, fuzzy logic intersection operators, and all linear and nonlinear averaging operators between them. Control parameters (e.g., \u03b1i, \u03b2i, \u03bbA, \u03bbD, wi) of the distance metric computer nodes and adaptable infinite logic aggregators can be determined by direct search optimization, using training data.", "Artificial neural network with adaptable infinite-logic nodes "]
["Processing gamma count rate decay curves using neural networks. At least some of the illustrative embodiments are methods comprising obtaining a gamma count rate decay curve one each for a plurality of gamma detectors of a nuclear logging tool (the gamma count rate decay curves recorded at a particular borehole depth), applying the gamma count rate decay curves to input nodes of a neural network, predicting by the neural network a geophysical parameter of the formation surrounding the borehole, repeating the obtaining, applying and predicting for a plurality of borehole depths, and producing a plot of the geophysical parameter of the formation as a function of borehole depth.", "Method and System of Processing Gamma County Rate Curves Using Neural Networks "]
["Liquid gauging apparatus using a time delay neural network for determining a quantity of liquid in a container that is not directly measurable by sensors is disclosed. The apparatus comprises a plurality of sensors and a processor. Each of the sensors are capable of measuring a respective parameter of the liquid and for producing a time varying sensor output signal representative of the respective parameter measured thereby. The processor is programmed to process the sensor output signals by a time delay neural network algorithm to determine a current quantity of the liquid in the container based on current and past parameter measurements of the sensor output signals. Also disclosed is a method of training a time delay neural network algorithm for computing a quantity of liquid in a container from current and past liquid parameter sensor measurements. The method comprises the steps of: establishing a dynamic model of liquid behavior in the container and parameter measurements of the liquid behavior sensed by a plurality of sensors; deriving from the dynamic model training data sets for a plurality of liquid quantity values, each data set comprising current and past liquid parameter sensor measurement values corresponding to a liquid quantity value of the plurality, and the corresponding liquid quantity value; and training the time delay neural network algorithm with the derived training data sets.", "Liquid gauging apparatus using a time delay neural network "]
["A method for a computer-aided control of a technical system is provided. The method involves use of a cooperative learning method and artificial neural networks. In this context, feed-forward networks are linked to one another such that the architecture as a whole meets an optimality criterion. The network approximates the rewards observed to the expected rewards as an appraiser. In this way, exclusively observations which have actually been made are used in optimum fashion to determine a quality function. In the network, the optimum action in respect of the quality function is modeled by a neural network, the neural network supplying the optimum action selection rule for the given control problem. The method is specifically used to control a gas turbine.", "Method for computer-aided control and/or regulation using neural networks "]
[null, "Method for controlling bearing-less AC asynchronous motor neural network inverse decoupling controller "]
[null, "Non-restricted environment face verification method based on block depth neural network "]
[null, "Image transmitter and neural network "]
["A motion video data system includes a compression system, including an image compressor, an image decompressor correlative to the image compressor having an input connected to an output of the image compressor, a feedback summing node having one input connected to an output of the image decompressor, a picture memory having an input connected to an output of the feedback summing node, apparatus for comparing an image stored in the picture memory with a received input image and deducing therefrom pixels having differences between the stored image and the received image and for retrieving from the picture memory a partial image including the pixels only and applying the partial image to another input of the feedback summing node, whereby to produce at the output of the feedback summing node an updated decompressed image, a subtraction node having one input connected to received the received image and another input connected to receive the partial image so as to generate a difference image, the image compressor having an input connected to receive the difference image whereby to produce a compressed difference image at the output of the image compressor.", "Motion video compression system with neural network having winner-take-all function "]
["The invention concerns a classifying apparatus, used in particular for recognising or characterising odours, applying a combination of statistical methods and neuronal networks to classify into the appropriate class instances of a plurality of different classes presented to the apparatus. The apparatus comprises a processing unit for determining for each class (j) a subspace (SEj) wherein the instances of said class are best separated from instances of other classes, said subspace being defined by synthetic variables (VDj), and for determining a discriminating subspace (SED) defined by the whole set of synthetic variables identified for the whole set of classes. Each neuron in the neuronal input layer corresponds to one of the variables defining the discriminating space (SED) and each neuron of the output layer corresponds to one of the classes.", "Classifying apparatus using a combination of statistical methods and neuronal networks, designed in particular for odour recognition "]
["A method and system for approximating a deep neural network for anatomical object detection is discloses. A deep neural network is trained to detect an anatomical object in medical images. An approximation of the trained deep neural network is calculated that reduces the computational complexity of the trained deep neural network. The anatomical object is detected in an input medical image of a patient using the approximation of the trained deep neural network.", "Method and System for Approximating Deep Neural Networks for Anatomical Object Detection "]
["A model based controller system and method is disclosed. The system and method includes at least one model including at least one process step, at least one controller that generates at least one control command, at least one component responsive to the at least one control command, wherein the at least one component receives the at least one control command from the at least one controller, and wherein the at least one component sends at least one component information element to the at least one controller, and at least one communicative coordination that communicatively coordinates the at least one model with the at least one controller, wherein the at least one control command is generated in accordance with the at least one process step, and wherein at least one of the at least one process step is varied in accordance with the at least one component information element.", "System and method for model based control of a neural network "]
["The characteristic data for determining the characteristics of the transfer functions (for example, sigmoid functions) of the neurons of the hidden layer and the output layer (the gradients of the sigmoid functions) of a neural network are learned and corrected in a manner similar to the correction of weighting data and threshold values. Since at least one characteristic data which determines the characteristics of the transfer function of each neuron is learned, the transfer function characteristics can be different for different neurons in the network independently of the problem and/or the number of neurons, and be optimum. Accordingly, a learning with high precision can be performed in a short time.", "Neural network having an optimized transfer function for each neuron "]
["A neutral net in which new nodes and connections are created in both input and intermediate layers during training, which is by punishment, reward and teaching. This can use a small increase in memory requirement to preclude the necessity for long training times applicable problems in speech and natural language processing, video recognition and simple logic functions.", "Trainable neural network having short-term memory for altering input layer topology during training "]
["A method for stimulation of hydrocarbon production via imbibition by utilization of surfactants. The method includes use of fuzzy logic and neural network architecture constructs to determine surfactant use.", "Imbibition well stimulation via neural network design "]
["An arrangement of data cells which stores at least one matrix of data words which are arranged in rows and columns, the matrix being distributed in the arrangement in order to deliver/receive, via a single bus, permuted data words which correspond either to a row or to a column of the matrix. Each data cell is connected to the single bus via series-connected switches which are associated with a respective addressing mode, the switches which address a same word of a same mode being directly controlled by a same selection signal. Circulation members enable the original order of the data on the bus to be restored. An arrangement of this kind is used in a layered neural network system for executing the error backpropagation algorithm.", "Arrangement of data cells and neural network system utilizing such an arrangement "]
["A signal processing system for a video camera uses a single neural network to implement multiple nonlinear signal processing functions. In one example, the neural network implements gamma correction, contrast compression, color correction, high pass filtering and aperture correction as a combined function which is emulated by the network. The network is trained off-line using back propagation to emulate the entire composite function for a set of parameters which results in multiple sets of weighting factors. Then, using the stored multiple sets of weighting factors as initial values, the neural network is \"re-trained\" on-line for each new parameter setting. The use of a single neural network in place of the multiple dedicated processing functions reduces engineering effort to develop the product and may reduce the cost of the total system.", "Neural network video image processor "]
[null, "Robot reinforced learning initialization method based on neural network "]
["A heuristic control system and method for use with a computer-aided design, capable of learning complicated control and achieving an optimizing task with a fewer number of iteration. The heuristic control system includes: rule-based system for choosing and evaluating a rule among a plurality of given rules; training system for choosing strongly a rule whose evaluation result is favorable based on a predetermined value which evaluates an evaluation result of the rule-based system, and for generating a learning pattern; and neural network for designing an optimized circuit based on a signal fed from the training system and for sending a resultant signal to the rule-based system for another iteration of heuristic control. The learning method includes the steps of: choosing and evaluating a rule among a plurality of given rules; choosing strongly a rule whose evaluation result is favorable based on a predetermined value; generating a learning pattern which brings a desirable result based on the evaluating step; designing an optimized circuit based on the learning pattern generated; and choosing and evaluating iteratively a rule among a plurality of given rules.", "Heuristic control system employing expert system, neural network and training pattern generating and controlling system "]
[null, "Noise classification method of Gaussian Mixture Model based on neural network "]
["A neural network solution for routing calls through a three stage interconnection network selects an open path through the interconnection network if one exists. The neural network solution uses a neural network with a binary threshold. The weights of the neural network are fixed for all time and therefore are independent of the current state of the interconnection network. Preferential call placement strategies are implemented by selecting appropriate external inputs to the neural network. An interconnection network controller stores information reflecting the current usage of the interconnection network and interfaces between the interconnection network and the neural network.", "Neural network solution for interconnection apparatus "]
["A processing element utilizing the learning algorithm", "Adaptive weight adjusting circuit for an neural network "]
["An artificial neural network (ANN) classifier provides a series of outputs indicative of a series of classes to which input feature vectors are classified. The ANN provides only one output for each input feature vector to partition said input into one class. The one output of the classifier is coupled to the interrupt input of an associated digital computer or CPU. Upon receipt of the output, the CPU immediately interrupts a main program and executes an interrupt service routine which is triggered by the output of the classifier. In this manner, the CPU is immediately accessed in the interrupt mode by the transition of one of the output class processing elements when activated.", "Artificial neural network (ANN) classifier apparatus for selecting related computer routines and methods "]
[null, "Short-term load forecast method based on artificial neural network "]
[null, "Rotary-table servo system neural network control method "]
["An artificial neural network, which has a plurality of neurons each receiving a plurality of inputs whose effect is determined by adjust able weights at synapses individually connecting the inputs to the neuron to provide a sum signal to a sigmoidal function generator determining the output of the neuron, undergoes memory modification by a steepest-descent method in which individual variations in the outputs of the neurons are successively generated by small perturbations imposed on the sum signals. As each variation is generated on the output of a neuron, an overall error of all the neuron outputs in relation to their desired values is measured and compared to this error prior to the perturbation. The difference in these errors, with adjustments which may be changed as the neuron outputs converge toward their desired values, is used to modify each weight of the neuron presently subjected to the perturbation.", "Artificial neural network system for memory modification "]
["A hierarchical neural network for monitoring network functions and that functions as a true anomaly detector is disclosed. Detection of an anomaly is achieved by monitoring selected areas of network behavior, such as protocols, that are predictable in advance. Combining outputs of neural networks within the hierarchical network yields satisfactory anomaly detection.", "A hierarchial neural network intrusion detector "]
["A self managed ad hoc communications network and method of managing the network. The network includes wireless devices or nodes that include a neural network element and the ad hoc network operates as a neural network. One of the nodes is designated as a Network Management System (NMS) that provides overall network management. Clusters of nodes are organized around cluster leaders. Each cluster leader manages a cluster of nodes and communications between node clusters. Each cluster may also have other nodes identified as lower order cluster leaders.", "Neural network-based mobility management for mobile ad hoc radio networks "]
["A recognition apparatus and method using a neural network is provided. A neuron-like element stores a value of its inner condition. The neuron-like element also updates a values of its internal status on the basis of an output from the neuron-like element itself, outputs from other neuron-like elements and an external input, and an output value generator a value of its internal status into an external output. Accordingly, the neuron-like element itself can retain the history of input data. This enables the time series data, such as speech, to be processed without providing any special devices in the neural network.", "Recognition apparatus using neural network, and learning method therefor "]
["The present invention relates to a method and apparatus, based on the use of a neural network (NN), for (a) predicting important groundwater/surface water output/state variables, (b) optimizing groundwater/surface water control variables, and/or (c) sensitivity analysis, to identify physical relationships between input and output/state variables used to model the groundwater/surface water system or to analyze the performance parameters of the neural network.", "Neural network based predication and optimization for groundwater / surface water system "]
["A synapse for neural network applications providing four quadrant feed-forward and feed-back modes in addition to an outer-product learning capability allowing learning in-situ. The invention, in its preferred embodiment, utilizes a novel two-transistor implementation which permits each synapse to be built in an integrated circuit chip surface area of only 20 by 20 micrometers. One of the two transistors at each synapse of the present invention comprises a floating gate structure composed of a floating gate electrode and a control electrode which permits learning upon application of incident ultraviolet light. During ultraviolet light application, a floating gate electrode voltage may be altered to modify the weight of each synapse in accordance with preselected criteria, based upon the input and output weight change vector elements corresponding to that particular matrix element. The second transistor corresponding to each synapse of the present invention provides a novel method for applying a voltage to the control electrode of the aforementioned floating gate structure of the first transistor. The voltage applied to the control electrode and thus the proportionate change in the floating gate electrode of the first transistor may be made proportional to the product of the corresponding input weight change vector element and the corresponding output weight change vector element, by using slope controllable ramp generators and phase controllable pulse generators, only one set of which must be provided for the entire matrix of synapses herein disclosed.", "Programmable synapse for neural network applications "]
["A neural network construct is trained according to sets of input signals (descriptors) generated by conducting a first experiment. A genetic algorithm is applied to the construct to provide an optimized construct and a CHTS experiment is conducted on sets of factor levels proscribed by the optimized construct.", "Neural network method and system "]
["Technologies pertaining to slot filling are described herein. A deep neural network, a recurrent neural network, and/or a spatio-temporally deep neural network are configured to assign labels to words in a word sequence set forth in natural language. At least one label is a semantic label that is assigned to at least one word in the word sequence.", "Assignment of semantic labels to a sequence of words using neural network architectures "]
["The neural network of the invention, of the type with a Cartesian matrix, has a first column of addition of the input signals, and at each intersection of the M lines and N columns it comprises a synapse constituted of a simple logic gate.", "Programmable analog neural network "]
["A neural network integrated circuit comprises many neuron circuits each with a distance resister that is compared in a competition for the closest-hit with all the other neurons. Such closest-hit comparison is conducted bit-by-bit over the many bit positions of a distance measure in binary format each time after the neurons fire. A single-wire AND-bus interconnects every neuron in a whole system. Each neuron drives the single-wire AND-bus with an open-collector buffer. All neurons press the single-wire AND-bus with their respective distance measures in successive cycles, starting with the most significant bit. For example, a fourteen-bit binary distance word requires fourteen comparison cycles. Any neuron that sees a \u201c0\u201d on the single-wire AND-bus when its own corresponding bit in its distance measure is a \u201c1\u201d, automatically drops from the competition. By the time the least significant bit cycle is run, a single closest distance will have been determined. Such neuron that wins announces itself with an identifying code.", "Neural network integrated circuit with fewer pins "]
["A semiconductor neural network includes a plurality of data input line pairs to which complementary input data pairs are transmitted respectively, data output line pairs respectively deriving complementary output data pairs and a plurality of coupling elements arranged at respective crosspoints of the data input lines and the data output lines. The coupling elements are programmable in states, and couple corresponding data output lines and corresponding data input lines in accordance with the programmed states thereof. Differential amplifiers formed by cross-coupled inverting amplifiers are provided in order to detect potentials on the data output lines. The differential amplifiers are provided for respective ones of the data output line pairs.", "Semiconductor neural network and method of driving the same "]
[null, "Neural Network Modeling Method "]
["A learning process for a neural network for open-loop or closed-loop control of an industrial process with time-variable parameters. The neural network is configured either as an open-loop or closed-loop-control network with which the process is controlled. The neural network is trained with the current process data so that it builds a model of the current process. The neural network can also be configured as a background network which is trained during operation with representative process data so that it builds an averaged model of the process over a longer period of time. After a certain learning time or upon the occurrence of an external event, the open-loop or closed-control network is replaced by the background network.", "Learning process for a neural network "]
["Higher operational speed is obtained without sacrificing computational accuracy and reliability in a neural network by interchanging a computationally complex nonlinear function with a similar but less complex nonlinear function in each neuron or computational element after each neuron of the network has been trained by an appropriate training algorithm for the classifying problem addressed by the neural network. In one exemplary embodiment, a hyperbolic tangent function is replaced by a piecewise linear threshold logic function.", "Operational speed improvement for neural network "]
["An object, such as a robot, is located at an initial state in a finite state space area and moves under the control of the unsupervised neural network model of the invention. The network instructs the object to move in one of several directions from the initial state. Upon reaching another state, the model again instructs the object to move in one of several directions. These instructions continue until either: a) the object has completed a cycle by ending up back at a state it has been to previously during this cycle, or b) the object has completed a cycle by reaching the goal state. If the object ends up back at a state it has been to previously during this cycle, the neural network model ends the cycle and immediately begins a new cycle from the present location. When the object reaches the goal state, the neural network model learns that this path is productive towards reaching the goal state, and is given delayed reinforcement in the form of a \"reward\". Upon reaching a state, the neural network model calculates a level of satisfaction with its progress towards reaching the goal state. If the level of satisfaction is low, the neural network model is more likely to override what has been learned thus far and deviate from a path known to lead to the goal state to experiment with new and possibly better paths.", "Neural network model for reaching a goal state "]
["A neural network is disclosed in which communication between processing elements occurs by radio waves in a waveguide. Radio wave communication using common carrier signals by transceivers in a waveguide allows processing elements to communicate wirelessly and simultaneously. Each processing element includes a radio frequency transceiver and an accompanying antenna which performs the neuron summing operation because input signals simultaneously received from plural processing elements by the antenna add. The weights on each input are provided by different spatial relationships between the transmitting processing elements and the receiving processing element which causes signal strength loses through the waveguide to be different. Each receiving processing element performs a neural threshold or sigmoid operation on the summed signal received from the transceiver and then a strength (amplitude scaling) can be applied to the output before the processing element transmits that output to the other processing elements in the system. Processing elements are grouped, allowing one group to transmit while the other group is receiving. Wafer scale electronics including transceivers and analog processing elements are combined with a comparably sized waveguide to produce a compact device.", "Wireless neural network and a wireless neural processing element "]
["A trainable artificial neural network includes a computer configured as a plurality of interconnected neural units arranged in a layered network. An input layer has a network input and an output layer has a network output. A neural unit has a first subunit and a second subunit, with the first subunit having one or more first inputs and a corresponding first set of variables for operating upon the said first inputs to provide a first output. The first set of variables can change in response to feedback representing differences between desired network outputs and actual network outputs. The second subunit has a plurality second inputs, and a corresponding second set of variables for operating upon said second inputs to provide a second output. The second set of variables can change in response to differences between desired network outputs for selected network inputs and actual network outputs. The computer provides an activating variable representing the difference between current second output and previous second outputs, and adds the activating variable to said feedback to accelerate the change of the first set of variables.", "Trainable neural network "]
["A compact neural network architecture is trainable to sense and classify an optical image directly projected onto it. The system is based upon the combination of a two-dimensional amorphous silicon photoconductor array and a liquid-crystal spatial light modulator. Appropriate filtering of the incident optical image upon capture is incorporated into the net work training rules, through a modification of the standard backpropagation training algorithm. Training of the network on two image classification problems is described: the recognition of handprinted digits, and facial recognition. The network, once trained is capable of standalone operation, sensing an incident image and outputting a final classification signal in real time.", "Neural network that incorporates direct optical imaging "]
["A electronic engine control (EEC) module executes a generic neural network processing program to perform one or more neural network control funtions. Each neural network funtion is defined by a unitary data structure which defines the network architecture, including the number of node layers, the number of nodes per layer, and the interconnections between nodes. In addition, the data structure holds weight values which determine the manner in which network signals are combined. The network definition data structures are created by a network training system which utilizes an external training processor which employs gradient methods to derive network weight values in accordance with a cost function which quantitatively defines system objectives and an identification network which is pretrained to provide gradient signals representative the behavior of the physical plant. The training processor executes training cycles asynchronously with the operation of the EEC module in a representative test vehicle.", "Generic neural network training and processing system "]
["A signal processing system for a video camera uses a single neural network to implement multiple nonlinear signal processing functions. In one example, the neural network implements gamma correction and contrast compression, in another example, color correction and aperture correction are added to the combined function emulated by the network. The network is trained using back propagation to emulate one function then a combination of two functions, then a combination of three functions, and so on. The programmed neural network replaces multiple pipelined signal processors in the video camera. The use of a single neural network in place of the multiple dedicated processing functions reduces engineering effort to develop the product and may reduce the cost of the total system.", "Neural network video image processor "]
[null, "Device for analyzing time sequence based on recurrent neural network and its method "]
["Described is a system for searching a continuous data stream for exact matches with a priori stored data sequences. The system includes a neural network with an input and an output layer. The input layer has one neuron for each possible character or number in the data stream, and the output layer has one neuron for each stored pattern. Importantly, the delays of the connections from input to output layer are engineered to match the temporal occurrence of an input character within a stored sequence. Thus, if an input sequence has the proper time gaps between characters, matching a stored pattern, then the delayed neural signals result in a simultaneous activation at the receiving neuron, which indicates a detected pattern. For storing a pattern, only one connection for each pair of input character and output neuron has to be specified resulting in sparse coding and quick storage.", "Neural network device with engineered delays for pattern storage and matching "]
["Method and apparatus for regenerating function in the nervous system. The method includes implanting in a central or peripheral nervous system environment neurons programmed for a selected function in the implant environment. The neurons are programmed using a multi-electrode device or micro-patterning. A suitable implantable neuronal network construct includes a conductive polymer substrate and neurons programmed for a selected function residing on the substrate.", "Implantable neuronal networks "]
["An artificial neural network is provided using a digital architecture having feedforward and feedback processors interconnected with a digital computation ring or data bus to handle complex neural feedback arrangements. The feedforward processor receives a sequence of digital input signals and multiplies each by a weight in a predetermined manner and stores the results in an accumulator. The accumulated values may be shifted around the computation ring and read from a tap point thereof, or reprocessed through the feedback processor with predetermined scaling factors and combined with the feedforward outcomes for providing various types neural network feedback computations. Alternately, the feedforward outcomes may be placed sequentially on a data bus for feedback processing through the network. The digital architecture includes a predetermined number of data input terminals for the digital input signal irrespective of the number of synapses per neuron and the number of neurons per neural network, and allows the synapses to share a common multiplier and thereby reduce the physical area of the neural network. A learning circuit may be utilized in the feedforward processor for real-time updating the weights thereof to reflect changes in the environment.", "Digital processing element in an artificial neural network "]
["A tensor deep stacked neural (T-DSN) network for obtaining predictions for discriminative modeling problems. The T-DSN network and method use bilinear modeling with a tensor representation to map a hidden layer to the predication layer. The T-DSN network is constructed by stacking blocks of a single hidden layer tensor neural network (SHLTNN) on top of each other. The single hidden layer for each block then is separated or divided into a plurality of two or more sections. In some embodiments, the hidden layer is separated into a first hidden layer section and a second hidden layer section. These multiple sections of the hidden layer are combined using a product operator to obtain an implicit hidden layer having a single section. In some embodiments the product operator is a Khatri-Rao product. A prediction is made using the implicit hidden layer and weights, and the output prediction layer is consequently obtained.", "Tensor deep stacked neural network "]
[null, "Dissolved oxygen control method based on dynamic radial basis function neural network "]
[null, "Convolution neural network parallel processing method based on large-scale high-performance cluster "]
["A method and system for training a computer classification system which can be defined by a network of a number of n-tuples or Look Up Tables (LUTs), with each n-tuple or LUT including a number of rows corresponding to at least a subset of possible classes and further including a number of columns being addressed by signals or elements of sampled training input data examples, each column being defined by a vector having cells with values, wherein the column vector cell values are determined based on one or more training sets of input data examples for different classes so that at least part of the cells comprise or point to information based on the number of times the corresponding cell address is sample from one or more sets of training input examples, and weight cell values are determined, corresponding to one or more column vector cells being addressed or sampled by the training examples.", "N-tuple or RAM based neural network classification system and method "]
["The technology described herein can be embodied in a method that includes receiving an audio signal encoding a portion of an utterance, and providing, to a first neural network, data corresponding to the audio signal. The method also includes generating, by a processor, data representing a transcription for the utterance based on an output of the first neural network. The first neural network is trained using features of multiple context-dependent states, the context-dependent states being derived from a plurality of context-independent states provided by a second neural network.", "Context-dependent state tying using a neural network "]
[null, "Neural network and fuzzy control fused electrical fire intelligent alarm method "]
["A system for measuring the value of a parameter, e.g., structural strain, includes an optical waveguide, a laser or equivalent light source for launching coherent light into the waveguide to propagate therein as multi modes, an array of a plurality of spaced apart photodetectors each comprising a light receptor surface and signal output, said array being arranged to have light emitted from said waveguide output portion irradiate said light receptor surfaces, an artificial neural network formed of a plurality of spaced apart neurons, connectors to impose weighted portions of signal outputs from the photodetectors upon the neurons which register the parameter value on a meter or like output device.", "Parameter measurement systems and methods having a neural network comprising parameter output means "]
["The device includes a neural network with an input layer 3, an internal layer 4, and an output layer 5. This network is designed to classify data vectors to classes, the synaptic weights in the network being determined through programming on the basis of specimens whose classes are known. Each class is defined during programming as corresponding to a set of neurons of which each represents a domain which contains a fixed number of specimens. The network includes a number of neurons and synaptic weights which have been determined as a function of the classes thus defined.", "Method for training a neural network for classifying an unknown signal with respect to known signals "]
["Methods and apparatuses for balancing computing workload via migrating computing tasks are disclosed. An artificial neural network (ANN) is trained based on the workload distribution over time for a host. The ANN predicts the workload for the host, and an indication may be sent to migrate at least one computing task away from the host. The indication is sent when the method is operating in a proactive mode and when the predicted workload is outside of a desired operating range. Some embodiments monitor the workload; and automatically switch the method to the proactive mode, when a difference between the monitored workload and the predicted workload is small. Other embodiments monitor the workload; and automatically switch the method to a reactive mode, when the monitored workload is outside of a failsafe operating range for the particular host.", "Artificial neural network for balancing workload by migrating computing tasks across hosts "]
["A neural network system includes a qualitative evaluation section, a neural network section, a quantifying section and a display section. The qualitative evaluation section qualitatively analyzes an unknown data supplied thereto, and normalizes the result of analysis within a predetermined range. The neural network section having a neural network with plural neurons computes the network output data from the normalized unknown data produced by the qualitative evaluation section. Each neuron is connected to plural other neurons through synapses, each of which is assigned an individual weight coefficient. Each neuron is adapted to output an output function value assigned thereto associated with the total sum of the products of the output from the neurons connected thereto and the synapse weight coefficient. The quantifying section quantifies the network output data to produce desired data. The desired data thus produced is displayed on the display section.", "Neural network system adapted for non-linear processing "]
["Classification method implemented in a layered neural network, comprising learning steps during which at least one layer is constructed by the addition of the successive neurons necessary for operating, by successive dichotomies, a classification of examples distributed over classes. In order to create at least one layer starting with a group of examples distributed over more than two classes, each successive neuron tends to distinguish its input data according to two predetermined sub-groups of classes peculiar to the said neuron according to a principal components analysis of the distribution of the said input data subjected to the learning of the neuron of the layer in question.", "Classification method implemented in a layered neural network for multiclass classification and layered neural network "]
["A pattern recognition method and apparatus utilizes a neural network to recognize input images which are sufficiently similar to a database of previously stored images. Images are first processed and subjected to a Fourier transform which yields a power spectrum. An in-class to out-of-class study is performed on a typical collection of images in order to determine the most discriminatory regions of the Fourier transform. A feature vector consisting of the (most discriminatory) information from the power spectrum of the Fourier transform of the image is formed. Feature vectors are input to a neural network having preferably two hidden layers, input dimensionality of the number of elements in the feature vector and output dimensionality of the number of data elements stored in the database. Unique identifier numbers are preferably stored along with the feature vector. Application of a query feature vector to the neural network results in an output vector. The output vector is subjected to statistical analysis to determine if a sufficiently high confidence level exists to indicate a successful identification whereupon a unique identifier number may be displayed.", "Method and apparatus for determining and organizing feature vectors for neural network recognition "]
["A neural network employing absolute difference calculating synapse cells comprising a pair of floating gate devices coupled in parallel between an internal cell node and column line of the network. The network further includes a switched-capacitor circuit for summing all of the charges generated by all of the synapse cells within a column of the network. The circuit operates in response to a sequence of applied voltage pulses such that each cell generates a charge representing either the input, the weight, or the minimum/maximum of either the weight or the input. The accumulation of these charges represents the sum of the absolute value difference between the input voltages and the stored weights for a single column of the array.", "Neural network employing absolute value calculating synapse "]
["A method, apparatus, and article of manufacture for performing data mining applications in a relational database management system. At least one analytic algorithm for enhanced back-propagation neural network processing is performed by a computer, wherein the analytic algorithm for enhanced back-propagation neural network processing includes SQL statements performed by the relational database management system directly against the relational database and programmatic iteration. The analytic algorithm for enhanced back-propagation neural network processing operates on data in the relational database that has been partitioned into training, testing and validation data sets. The analytic algorithm for enhanced back-propagation neural network processing maps the data in the training data sets to nodes in the neural network wherein the data is processed as it moves from an input node of the neural network through a hidden node of the neural network to an output node of the neural network. In addition, the analytic algorithm for enhanced back-propagation neural network processing determines an error difference between the output node's value and a target value as the data is mapped to the output node in the neural network, and changes a weight value for one or more of the nodes based on an accumulation of the error difference for the node, in order to get the neural network to converge on a solution. Finally, the analytic algorithm for enhanced back-propagation neural network processing cross-validates the changed weight value to prevent overfitting the node.", "Analytic algorithm for enhanced back-propagation neural network processing "]
["Designs for cognitive memory systems storing input data, images, or patterns, and retrieving it without knowledge of where stored when cognitive memory is prompted by query pattern that is related to sought stored pattern. Retrieval system of cognitive memory uses autoassociative neural networks and techniques for pre-processing query pattern to establish relationship between query pattern and sought stored pattern, to locate sought pattern, and to retrieve it and ancillary data. Cognitive memory, when connected to computer or information appliance introduces computational architecture that applies to systems and methods for navigation, location and recognition of objects in images, character recognition, facial recognition, medical analysis and diagnosis, video image analysis, and to photographic search engines that when prompted with a query photograph containing faces and objects will retrieve related photographs stored in computer or other information appliance, and will identify URL's of related photographs and documents stored on the World Wide Web.", "System and method for cognitive memory and auto-associative neural network based pattern recognition "]
[null, "2.5D location method based on neural network and wireless LAN infrastructure "]
["A deep tensor neural network (DTNN) is described herein, wherein the DTNN is suitable for employment in a computer-implemented recognition/classification system. Hidden layers in the DTNN comprise at least one projection layer, which includes a first subspace of hidden units and a second subspace of hidden units. The first subspace of hidden units receives a first nonlinear projection of input data to a projection layer and generates the first set of output data based at least in part thereon, and the second subspace of hidden units receives a second nonlinear projection of the input data to the projection layer and generates the second set of output data based at least in part thereon. A tensor layer, which can converted into a conventional layer of a DNN, generates the third set of output data based upon the first set of output data and the second set of output data.", "Computer-implemented deep tensor neural network "]
["When it is determined that a minimum value of a cost function of a candidate structure obtained by a training process of a specified-number sequence is equal to or higher than that of the cost function of the candidate structure obtained by the first step of a previous sequence immediately before the specified-number sequence, a method performs, as a random removal step of the specified sequence, a step of randomly removing at least one unit from the candidate structure obtained by the first step of the previous sequence again. This gives a new generated structure of the target neural network based on the random removal to the first step as the input structure of the target neural network. The method performs the specified-number sequence again using the new generated structure of the target neural network.", "Method and system for obtaining improved structure of a target neural network "]
["A signal processing apparatus using a neural network according to this invention includes a reference signal generating section for generating a plurality of reference signals having different signal values, a complement signal generating section for receiving the reference signals and an unknown input signal as an object to be processed, and generating a plurality of complement signals, indicating complement values of the corresponding reference signals with respect to a signal value obtained by multiplying the unknown input signal with a natural number, a multiplication section for receiving the reference signals and the complement signals, and multiplying the reference signals with the corresponding complement signals, and a neural network, in which a plurality of neurons are reciprocal-inhibition-coupled, the neurons receive the products obtained by the multiplication section, and the neuron, which receives the product having a largest value, outputs a spark signal.", "Apparatus including a neural network used for signal processing, such as signal clustering, signal identification, and A/D conversion "]
["A system for generating annotations of a document, including a plurality of neurons connected as a neural network, the neurons being associated with words, sentences and documents. An activity regulator regulates a minimum and/or maximum number of neurons of the neural network that are excited at any given time. The neurons are displayed to a user and identify the neurons that correspond to sentences containing a predetermined percentage of document meaning. The annotations can be also based on a context of the user's search query. The query can include keywords, documents considered relevant by the user, or both. Positions of the neurons relative to each other can be changed on a display device, based on input from the user, with the change in position of one neuron changing the resulting annotations. The input from the user can also include changing a relevance of neurons relative to each other, or indicating relevance or irrelevance of a document or sentence.", "System and method for using a bidirectional neural network to identify sentences for use as document annotations "]
["Certain aspects of the present disclosure provide methods and apparatus for a continuous-time neural network event-based simulation that includes a multi-dimensional multi-schedule architecture with ordered and unordered schedules and accelerators to provide for faster event sorting; and a formulation of modeling event operations as anticipating (the future) and advancing (update/jump ahead/catch up) rules or methods to provide a continuous-time neural network model. In this manner, the advantages include faster simulation of spiking neural networks (order(s) of magnitude); and a method for describing and modeling continuous time neurons, synapses, and general neural network behaviors.", "Continuous time spiking neural network event-based simulation "]
[null, "Crowd density estimation method based on cascaded multilevel convolution neural network "]
["A sample and hold circuit for introducing delayed feedback into an associative memory is described. The circuit continuously samples an output sequence derived from a neural network; then, in response to a clock signal, it holds that output sequence until the next clock signal. The held sequence is coupled back to the input of the network so that the present output sequence becomes some function of the past output sequence. This delayed feedback enables the associative recall of a memorized sequence from the neural network.", "Sample and hold circuit for temporal associations in a neural network "]
["A system and method for solving a problem using a genetic algorithm technique is disclosed. A population of chromosomes that is representative of a set of candidate solutions of the problem is created and subjected to simulated evolution. A neural network is trained and employed to evaluate the fitness of the population of chromosomes. Based on the neural network evaluation, the population of chromosomes is updated.", "System and method for solving an optimization problem using a neural-network-based genetic algorithm technique "]
["Neural networks may be used in certain automatic speech recognition systems. To improve performance of these neural networks, they may be updated/retrained during run time by training the neural network based on the output of a speech recognition system or based on the output of the neural networks themselves. The outputs may include weighted outputs, lattices, weighted N-best lists, or the like. The neural networks may be acoustic model neural networks or language model neural networks. The neural networks may be retrained after each pass through the network, after each utterance, or in varying time scales.", "Adaptive neural network speech recognition models "]
["A method and apparatus of training a neural network. The method and apparatus include creating a model for a desired function as a multi-dimensional function, determining if the created model fits a simple finite geometry model, and generating a Radon transform to fit the simple finite geometry model. The desired function is fed through the Radon transform to generate weights. A multilayer perceptron of the neural network is trained using the weights.", "Method and apparatus of using a neural network to train a neural network "]
["A \"Barometer\" Neuron enhances stability in a Neural Network System that, when used as a track-while-scan system, assigns sensor plots to predicted track positions in a plot/track association situation. The \"Barometer\" Neuron functions as a bench-mark or reference system node that equates a superimposed plot and track to a zero distance as a \"perfect\" pairing of plot and track which has a measured/desired level of inhibition. The \"Barometer\" Neuron responds to the System inputs, compares these inputs against the level of inhibition of the \"perfect\" pair, and generates a supplied excitation or inhibition output signal to the System which adjusts the System to a desired value at or near 1.0; this the reference level of inhibition of the \"perfect\" pair.", "\"Barometer\" neuron for a neural network "]
["A method and system for recognizing user input information including cursive handwriting and spoken words. A time-delayed neural network having an improved architecture is trained at the word level with an improved method, which, along with preprocessing improvements, results in a recognizer with greater recognition accuracy. Preprocessing is performed on the input data and, for example, may include resampling the data with sample points based on the second derivative to focus the recognizer on areas of the input data where the slope change per time is greatest. The input data is segmented, featurized and fed to the time-delayed neural network which outputs a matrix of character scores per segment. The neural network architecture outputs a separate score for the start and the continuation of a character. A dynamic time warp (DTW) is run against dictionary words to find the most probable path through the output matrix for that word, and each word is assigned a score based on the least costly path that can be traversed through the output matrix. The word (or words) with the overall lowest score (or scores) are returned. A DTW is similarly used in training, whereby the sample ink only need be labeled at the word level.", "Handwriting and speech recognizer using neural network with separate start and continuation output scores "]
["A system for detecting a network intrusion includes a first neural network for determining a first plurality of weight values corresponding to a plurality of vectors of an input data, a second neural network for updating the first plurality of weight values received from the first neural network to a second plurality of weight values based on the plurality of vectors of the input data, a third neural network for updating the second plurality of weight values received from the second neural network to a third plurality of weight values based on the plurality of vectors of the input data, and a classification module for classifying the plurality of vectors under at least one of a plurality of intrusions based on the third plurality of weight values received from the third neural network.", "Feature Based Three Stage Neural Network Intrusion Detection "]
["Device comprising storage means (51) for storing a genotypic representation (3) of a spiking neural network comprising spiking neurons (1) and input neurons (2) connected by synapses, and computer program portions for performing the steps of mutating said genotypic representation (3) and computing a fitness value associated to said mutated genotypic representation (3). The spiking neural network implemented in this device can thus be trained and used for various control systems, achieving better results, thanks to its highly non-linear behavior, than standard prior art neural networks.", "Spiking neural network device "]
["A method for increasing the accuracy of an analog neural network which computers a sum-of-products between an input vector and a stored weight pattern is described. In one embodiment of the present invention, the method comprises initially training the network by programming the synapses with a certain weight pattern. The training may be carried out using any standard learning algorithm. Preferably, a back-propagation learning algorithm is employed.", "Method of increasing the accuracy of an analog neural network and the like "]
["A difference calculating neural network is disclosed having an array of synapse cells arranged in rows and columns along pairs of row and column lines. The cells include a pair of floating gate devices which have their control gates coupled to receive one of a pair of complementary input voltages. The floating gate devices also have complementary threshold voltages such that packets of charge are produced from the synapse cells that are proportional to the difference between the input and voltage threshold. The charge packets are accumulated by the pairs of column lines in the array.", "Difference calculating neural network utilizing switched capacitors "]
["A self-extending shape neural-network is capable of a self-extending operation in accordance with the studying results. The self-extending shape neural-network has initially minimum number of the intermediate layers and the number of the nodes (units) within each layer by the self-extension of the network construction so as to shorten the studying time and the discriminating time. This studying may be effected efficiently by the studying being directed towards the focus when the studying is not focused.", "Self-extending neural-network "]
["A learning method of a neural network, in which from a set of learning patterns belonging to one category, specific learning patterns located at a region close to learning patterns belonging to another category are selected and learning of the neural network is performed by using the specific learning patterns so as to discriminate the categories from each other.", "Learning method of neural network "]
["A neural network comprises trained interconnected neurons. The neural network is configured to constrain the relationship between one or more inputs and one or more outputs of the neural network so the relationships between them are consistent with expectations of the relationships; and/or the neural network is trained by creating a set of data comprising input data and associated outputs that represent archetypal results and providing real exemplary input data and associated output data and the created data to neural network. The real exemplary output data and the created associated output data is compared to the actual output of the neural network, which is adjusted to create a best fit to the real exemplary data and the created data.", "Method of training a neural network and a neural network trained according to the method "]
["A predictive global model for modeling a system includes a plurality of local models, each having: an input layer for mapping into an input space, a hidden layer and an output layer. The hidden layer stores a representation of the system that is trained on a set of historical data, wherein each of the local models is trained on only a select and different portion of the set of historical data. The output layer is operable for mapping the hidden layer to an associated local output layer of outputs, wherein the hidden layer is operable to map the input layer through the stored representation to the local output layer. A global output layer is provided for mapping the outputs of all of the local output layers to at least one global output, the global output layer generalizing the outputs of the local models across the stored representations therein.", "Neural network model with clustering ensemble approach "]
["A system and method for determining events in a system or process, such as predicting fault events. The method includes providing data from the process, pre-processing data and converting the data to one or more temporal spike trains having spike amplitudes and a spike train length. The spike trains are provided to a dynamical neural network operating as a liquid state machine that includes a plurality of neurons that analyze the spike trains. The dynamical neural network is trained by known data to identify events in the spike train, where the dynamical neural network then analyzes new data to identify events. Signals from the dynamical neural network are then provided to a readout network that decodes the states and predicts the future events.", "Spiking dynamical neural network for parallel prediction of multiple temporal events "]
["A learning neural network (30) implements a random weight change learning algorithm within a weight adjustment mechanism (28) for manipulating the weights applied to inputs of the network (30) in order to achieve a desired functionality for the network (30). Weights are changed randomly from an initial state with a small incremental weight change of either +\u03b4 or -\u03b4. If the overall network output decreases by the weight change, the same weight change is iterated until the error increases. If, however, the overall network error increases, the weights are changed randomly again. After iterating the foregoing methodology, the network error gradually decreases and finally reaches approximately zero. Furthermore, a shift mechanism (36) and a multiplier (38) are employed as a weight application mechanism (16). The shift mechanisms (36) are connected in series with a random line (35) and are connected in parallel with a shift line (44). A random direction is successively channelled through the shift mechanisms (36) via the random line (35) under the control of the shift line ( 44) so that only a single random number need be generated for all of the shift mechanisms (36) within the neural network (30).", "System and method for a learning neural network for generating random directions for weight changes "]
["Approaches for accurate neural network training for library-based critical dimension (CD) metrology are described. Approaches for fast neural network training for library-based CD metrology are also described.", "Accurate and Fast Neural network Training for Library-Based Critical Dimension (CD) Metrology "]
["A system and method for applying a convolutional neural network (CNN) to speech recognition. The CNN may provide input to a hidden Markov model and has at least one pair of a convolution layer and a pooling layer. The CNN operates along the frequency axis. The CNN has units that operate upon one or more local frequency bands of an acoustic signal. The CNN mitigates acoustic variation.", "System and method for applying a convolutional neural network to speech recognition "]
["Systems for audio and image processing using bio-inspired neural network are proposed. The first system allows separating a specific sound in a mixture of audio sources. The second system allows performing visual pattern processing and recognition robust to affine transforms and noise. The neural network system comprises first and second layers of spiking neurons, each neurons being configured for respectively first and second internal connections to other neurons from the same layer or for external connections to neurons from the other layer for receiving extra-layer stimuli therefrom and for receiving external stimuli from external signals; and global controllers connected to all neurons to allow inhibiting the neurons. In operation, upon receiving stimuli from said the first and second external signals, the internal connections are promoted, and synchronous spiking from neurons from the first and second layers are promoted by the external connections when some of the stimuli from the first external signals are similar to some of the stimuli from the second external signals. There is no need to tune the neural network when changing the signal nature. Furthermore, the proposed neural network is autonomous and there is neither training nor recognition phase.", "Spiking neural network and use thereof "]
["In a neural network having neurons connected in a multi-layer, firstly, input signal sets are sequentially entered to statistically process the outputs of hidden neurons and determine the optimum number of hidden neurons. Secondly, while changing the input signal entered to each input neuron to the maximum change limit, the change of output values of the other input neurons are checked to thereby determine an unnecessary input neuron. Thirdly, the weights between input neurons and hidden neurons are set to be in correspondence with a hyperplane to enable pattern recognition.", "Apparatus for configuring neural network and pattern recognition apparatus using neural network "]
["A neural net signal processor provided with a single layer neural net constituted of N neuron circuits which sums the results of the multiplication of each of N input signals Xj(j=1 to N) by a coefficient mij to produce a multiply-accumulate value ##EQU1## thereof, in which input signals Xj(j=1 to N) for input to the single layer neural net are input as serial input data, comprising: a multiplicity of systolic processor elements SPE-1(i=1 to M), each comprised of a two-state input data delay latch; a coefficient memory; means for multiplying and summing for multiply-accumulate output operations; an accumulator; a multiplexor for selecting a preceding stage multiply-accumulate output Sk(k=1 to i-1) and the multiply-accumulate product Si computed by the said circuit; wherein the multiplicity of systolic processor elements are serially connected to form an element array and element multiply-accumulate output operations are executed sequentially to obtain the serial multiply-accumulate outputs Si(i=1 to M) of one layer from the element array.", "Systolic processor elements for a neural network "]
[null, "Grain sorting apparatus utilizing neural network "]
["A method of training a multilayer perceptron type neural network to provide a processor for fusion of target angle data detected by a plurality of sensors. The neural network includes a layer of input neurons at least equal in number to the number of sensors plus the maximum number of targets, at least one layer of inner neurons, and a plurality of output neurons forming an output layer. Each neuron is connected to every neuron in adjacent layers by adjustable weighted synaptic connections. The method of training comprises the steps of (a) for each sensor, designing a plurality of the input neurons for receiving any target angle data, the number of designated input neurons for each sensor being at least as large as the maximum number of targets to be detected by the sensor; (b) for a known set of targets having a known target angle for each sensor, applying a signal related to each known target angle to the designated input neurons for each of the sensors, wherein the output neurons will produce an initial output; (c) for a selected one of the sensors, designating a plurality of the output neurons to correspond to the input neurons designated for the selected sensor and applying the signal related to the known target angles for the selected sensor to the designated output neurons to provide a designated output signal wherein the difference between the initial output and the designated output signal is used to adapt the weights throughout the neural network to provide an adjusted output signal; and (d) repeating steps (a)-(c) until the adjusted output signal corresponds to a desired output signal.", "Training of neural network for multi-source data fusion "]
[null, "Traffic flow prediction method based on quick learning neural network with double optimal learning rates "]
["A method for testing an audio device with a trained neural network includes a loopback connector connecting the output port of the audio device to the input port of the audio device. A test signal is transmitted through the audio port and received at the input port. The test signal is converted into a frequency spectrum for analysis. The frequency spectrum is provided as input to a trained neural network, the neural network being previously trained to recognize the frequency spectrum pattern created by a properly working, or ideal, audio device. The neural network is trained by connecting the input port to the output port of an audio device from which the training is to occur. Prior to converting signals to a frequency spectrum, the waveform characteristics of the signal may be iteratively evaluated and recording levels adjusted so that the signal received has characteristics that can be tested by the neural network. The analyzed signal may be a portion of the signal received after analog to digital converters in the audio device have stabilized. The neural network generates a confidence level based on comparing the pattern of the tested audio device's frequency spectrum to the frequency spectrum of a working audio device. A pass value may be predetermined so that the tested audio device is reported as passing or failing the test by comparing the confidence level value generated by the system with the predetermined pass value.", "Audio diagnostic system and method using frequency spectrum and neural network "]
[null, "Predicting device using neural network "]
["A fuzzy-neural network system includes: an input layer outputting values of input parameters; a membership layer wherein a multiple number of regions for each of the input parameters are formed by dividing the probable range of the input parameter and a membership function is defined for each of the regions, the membership layer producing membership values as to the regions for each of the input parameters, in accordance with the output values from the input layer; a rule layer wherein specific rules are formed between regions belonging to different input parameters, the rule layer outputting a suitability for each of the rules; an outputting layer producing an output parameter or parameters in accordance with the output values from the rule layer; and a membership value setup means which, if some of the input parameters are unknown, sets up prescribed values as membership values corresponding to the unknown parameters.", "Fuzzy-neural network system and a learning method therein "]
["A signal phase pattern sensitive neural network system can discern  persist patterns of phase in a time varying or oscillatory signal. The system employs duplicate inputs from each of its sensors to the processing elements of a first layer of its neural network, with the exception that one input is phase shifted relative to the other. The system also employs a modification of a conventional Kohonen competitive learning rule which is applied by the processing and learning elements of a second layer of its neural network.", "Signal phase pattern sensitive neural network system and method "]
["A method and apparatus for training and operating a neural network using gated data. The neural network is a mixture of experts that performs \u201csoft\u201d partitioning of a network of experts. In a specific embodiment, the technique is used to detect malignancy by analyzing skin surface potential data. In particular, the invention uses certain patient information, such as menstrual cycle information, to \u201cgate\u201d the expert output data into particular populations, i.e., the network is soft partitioned into the populations. An Expectation-Maximization (EM) routine is used to train the neural network using known patient information, known measured skin potential data and correct diagnosis for the particular training data and patient information. Once trained, the neural network parameters are used in a classifier for predicting breast cancer malignancy when given the patient information and skin potentials of other patients.", "Method and apparatus for training and operating a neural network for detecting breast cancer "]
["A learning control method reduces overall learning time by displaying data related to an appropriate determination of learning protraction and a proper restoring method. Prior to initiating the learning, the user is inquired about the current problem and a problem data set representing items associated with the problem is obtained. Evaluation data indicating a state of learning obtained during the learning on the current problem is sequentially stored and displayed. When there is a high possibility of learning protraction during the learning, a message informing the user is displayed. When the learning is stopped by the user in this case, the problem data set and evaluation data set are stored. Then, a list of restoring methods is displayed and a particular restoring method is selected by the user once the learning is stopped. The learning is restarted on the current problem in accordance with the selected restoring method.", "Method of and system for controlling learning in neural network "]
["Embodiments of the present invention comprise methods and systems for automatically adjusting images to conform to preference data.", "Methods and systems for digital image characteristic adjustment using a neural network "]
["The speech recognition apparatus recognizes a frequency of successively input identical speech data sequences. The speech recognition apparatus includes a speech recognition non-layered neural network unit. Speech data sequence is inputted as feature vectors from a feature extracting unit. The neural network performs speech recognition and determines whether the input speech data sequence matches at least one predetermined speech data sequence. The neural network generates a speech recognition signal when the input speech data sequence matches the at least one predetermined speech data sequence. A recognition signal detecting unit outputs a reset instruction signal each time the neural network generates the speech recognition signal. An internal state value setting unit resets the neural network unit to an initial state each time the recognition signal detecting unit outputs the reset instruction signal. Since the neural network unit is reset each time the speech recognition signal is outputted, accurate detection can be achieved even when speech data sequence to be recognized is inputted successively.", "Neural network speech recognition apparatus recognizing the frequency of successively input identical speech data sequences "]
["A computer simulator is provided for displaying the state of an artificial neural network in a simplified yet meaningful manner on a computer display terminal. The user may enter commands to select one or more areas of interest within the neural network for further information regarding its state of learning and operation. One display mode illustrates the output activity of each neuron as representatively sized and shaded boxes within the border of the neuron, while another display mode shows the connectivity as weighted synapses between a user-selected neuron and the remaining neurons of the network in a similar manner. A third display mode provides a tuning curve wherein the synapses associated with each of the neurons are represented within the borders of the same. Both grid block and line graph type characterization are supported. The methodology allows large neural networks on the order of thousands of neurons to be displayed in a meaningful manner.", "Method of displaying the state of an artificial neural network "]
["A multi-criteria event detection system, comprising a plurality of sensors, wherein each sensor is capable of detecting a signature characteristic of a presence of an event and providing an output indicating the same. A processor for receiving each output of the plurality of sensors is also employed. The processor includes a probabilistic neural network for processing the sensor outputs. The probabilistic neural network comprises a nonlinear, nor-parametric pattern recognition algorithm that operates by defining a probability density function for a plurality of data sets that are each based on a training set data and an optimized kernel width parameter. The plurality of data sets includes a baseline, non-event, first data set; a second, event data set; and a third, nuisance data set. The algorithm provides a decisional output indicative of the presence of a fire based on recognizing and discrimination between said data sets, and whether the outputs suffice to substantially indicate the presence of an event, as opposed to a non-event or nuisance situation.", "Probabilistic neural network for multi-criteria event detector "]
[null, "Information processor and neural network circuit using the same "]
["A neural-simulating system for processing input stimuli includes a plurality of layers, each layer receives layer input signals and generates layer output signals, the layer input signals include signals from the input stimuli and ones of the layer output signals from only previous layers within the plurality of layers. Each of the plurality of layers includes a plurality of neurons operating in parallel on the layer input signals applied to the plurality of layers. Each of the neurons derives neuron output signals from a continuously differentiable transfer function for each of the neurons based upon a combination of sets of weights associated with the neurons and the layer input signals. An adaptive network is associated with each neuron for generating weight correction signals based upon gradient estimate signals and convergence factors signals of each neuron and for processing the weight correction signals to thereby modify the weights associated with each neuron. An error measuring circuit generates relative powered error signals for use in generating the gradient estimate signals and the convergence factors signals.", "Adaptive neural network image processing system "]
["Methods of training neural networks (100, 600) that include one or more inputs (102-108) are provided, and a sequence of processing nodes (110, 112, 114, 116) in which each processing node may be coupled to one or more processing nodes that are closer to an output node. The methods include establishing an objective function that preferably includes a term related to differences between actual and expected output for training data, and a term related to the number of weights of significant magnitude. Training involves optimizing the objective function in terms of weights that characterized directed edges of the neural network. The objective function is optimized using algorithms that employ derivatives of the objective function. Algorithms for evaluating closed-form derivatives of the summed input to output processing nodes of the neural network with respect to the weights of the neural network are provided.", "Neural network and method of training "]
["Initially, customer time series credit files are acquired. The credit files are organized in a data mart environment for supporting a query system. Time series utilization attributes are created and a neural network time series segmentation process is applied and N\u00d7N dimension segments are generated for analysis. The chart may be modified to more accurately depict profitable credit revolvers. Credit data from each potential new customer is processed in a similar fashion by the neural network segmentation process. Profitable credit revolvers are identified by having credit utilization patterns belonging to profitable segments previously identified.", "Method and system for identifying consumer credit revolvers with neural network time series segmentation "]
["This invention is a novel high-speed neural network based processor for solving the \"traveling salesman\" and other global optimization problems. It comprises a novel hybrid architecture employing a binary synaptic array whose embodiment incorporates the fixed rules of the problem, such as the number of cities to be visited. The array is prompted by analog voltages representing variables such as distances. The processor incorporates two interconnected feedback networks, each of which solves part of the problem independently and simultaneously, yet which exchange information dynamically.", "Electronic neural network for solving \"traveling salesman\" and similar global optimization problems "]
["A neural network with a high recognition rate when applied to static patterns is made applicable to dynamic time series patterns such as voice signals. Plural units with one or more inputs and outputs are interconnected, and a unique load coefficient is assigned to each connection to weight the signals flowing through that connection. The neural network includes an input unit group to which are input the components of plural vectors included in the input feature vector series {y(t)}; an output unit which outputs the converted vectors, which are produced by passing the input vectors through each unit and the associated connections; and J paths from input unit group to the output unit group. The units are connected to form a Hidden Markov Model wherein each signal path identified as j=1, 2, . . . , J corresponds to the same state.", "Time series signal analyzer including neural network having path groups corresponding to states of Markov chains "]
["Apparatus and methods for event based communication in a spiking neuron network. The network may comprise units communicating by spikes via synapses. The spikes may communicate a payload data. The data may comprise one or more bits. The payload may be stored in a buffer of a pre-synaptic unit and be configured to accessed by the post-synaptic unit. Spikes of different payload may cause different actions by the recipient unit. Sensory input spikes may cause postsynaptic response and trigger connection efficacy update. Teaching input spikes trigger the efficacy update without causing the post-synaptic response.", "Apparatus and methods for event-based communication in a spiking neuron networks "]
["To enable the pattern matching between a shifted input pattern and the standard pattern, a plurality of standard patterns are stored in a standard pattern associative memory network 12. A pattern shifted relative to the standard pattern is inputted to the input pattern network 11 and a restriction condition of when the input pattern is shifted relative to the standard pattern is stored in a coordinate associated network 14. In an association network 13, weights and biases are determined so that the respective units of the network 13 are activated most intensely when the input pattern and the standard pattern match correctly each other in response to the signals from the respective networks 11, 12, and 14.", "Neural network "]
[null, "Bearing-free asynchronous motor control method based on neural network inverse system theory "]
[null, "Method and device for data identification based on multitask deep neural network "]
[null, "Wind power combination predicting method based on fuzzy neural network and support vector machine "]
["In accordance with the present invention, a neural network comprising an array of neurons (i.e. processing nodes) interconnected by synapses (i.e. weighted transmission links) is utilized to carry out a probabilistic relaxation process. The inventive neural network is especially suited for carrying out a variety of image processing tasks such as thresholding.", "Neural network for performing a relaxation process "]
["A coin validation system for determining if a coin moving along a coin rail is a valid coin, and if so, its denomination the system including a rail along which coins move, at least one optical sensor located along the rail to sense the presence or absence of a coin moving therealong, at least one magnetic sensor associated with each optical sensor located in the vicinity of the respective optical sensor, each of the magnetic sensors including an inductive element and a circuit for exciting the magnetic sensor to produce a field that is coupled to the coin moving past so that the coin and the inductive element have mutual inductance therebetween, the circuit ringing the magnetic sensor a predetermined number of times while the coin is adjacent to the magnetic sensor whereby the magnetic sensor generates a damped wave signal having characteristics representative of the physical and magnetic characteristics of the coin, a signal preprocessor operatively connected to the magnetic sensor for producing output responses representative of distinguishing characteristics of the coin, a feature extraction circuit for extracting from the output responses of the signal preprocessor signal portions representative of predetermined distinguishing characteristics of the coin, a circuit for producing a multi-dimensional representation of the extracted features and for comparing the multi-dimensional representation with the center of an established ellipsoidal cluster of selected coin denominations to determine the extent of the comparison therebetween and to be used to determine whether the coin is an acceptable coin or not, and an artificial neural network classifier circuit having connections to the preprocessor and to the comparator circuit, the neural network classifier circuit having an output which identifies the denomination of coins that are determined by the comparator circuit to be acceptable.", "Pattern recognition using artificial neural network for coin validation "]
["The present invention provides an event-driven universal neural network circuit. The circuit comprises a plurality of neural modules. Each neural module comprises multiple digital neurons such that each neuron in a neural module has a corresponding neuron in another neural module. An interconnection network comprising a plurality of digital synapses interconnects the neural modules. Each synapse interconnects a first neural module to a second neural module by interconnecting a neuron in the first neural module to a corresponding neuron in the second neural module. Corresponding neurons in the first neural module and the second neural module communicate via the synapses. Each synapse comprises a learning rule associating a neuron in the first neural module with a corresponding neuron in the second neural module. A control module generates signals which define a set of time steps for event-driven operation of the neurons and event communication via the interconnection network.", "Event-driven universal neural network circuit "]
["A neural network architecture is provided for optical character recognition from an input image in which the target character may be rotated in the image plane. The architecture includes hidden units whose inputs receive image information from portions of the image which are rotationally distributed. That is, the local link between input units and the hidden units is adequate for rotation of the character in the image. Therefore, regardless of the orientation of the image, it is right side up, or approximately so, with respect to one of the hidden units. The hidden units have corresponding inputs with corresponding weight factors, i.e., symmetric weight sharing. Thus, regardless of the orientation of the image, one of the hidden units will produce a high output value indicative of an upright character. Alternatively, a single hidden unit has groups of inputs, each group having a corresponding set of weight factors. The groups are coupled to input units for rotationally distributed portions of the image. Therefore, for any orientation of the image corresponding with one of the groups of inputs of the hidden unit, the hidden unit produces the same output value. In an preferred embodiment, feature information such as local contour direction information is provided to the input units. The feature information is provided with respect to slices of the image taken in different directions.", "Neural network architecture for recognition of upright and rotated characters "]
["In a segmentation method and system, a plurality of digitized images having different optical bands are acquired for the same micrographic scene of a biological sample. Each digitized image has a plurality of pixels having values from each digitized image. The pixel values are processed to identify nuclear or cytoplasmic material utilizing previously developed classification information developed from at least one cell having known regions of nuclear or cytoplasmic material. In a preferred embodiment, the neural network comprises a hardware-encoded algorithm in the form of a look-up table.", "Neural network assisted multi-spectral segmentation system "]
["A neuron for use in a self-learning neural network comprises a current input node at which a plurality of synaptic input currents are summed using Kirchoff's current law. The summed input currents are normalized using a coarse gain current normalizer. The normalized summed inputs current is then converted to a voltage using a current to voltage converter. This voltage is then amplified by a gain controlled cascode output amplifier. Gain control inputs are provided in the output amplifier so that the neuron can be settled by the Mean Field Approximation. A noise input stage is also connected to the output amplifier so that the neuron can be settled using simulated annealing. The resulting neuron is a variable gain, bi-directional current transimpedance neuron with a controllable noise input.", "Neuron for use in self-learning neural network "]
["A method involving the supply of data (20) to input nodes (24a-e) of a neural network with respect to the breathing function. Alarm conditions of critical events are identified and associated with coordinated images generated at the output of the neural network (29m-q) as the system is trained. In such training, the neural network and associated system are subjected to the alarm condition to produce the desired coordinated image. This coordinated image is correlated with an alarm activating signal and the specific alarm condition to which it is to be identified in connection with future comparisons. During subsequent use in clinical applications, the monitoring system, (23, 50-52, 55-59, 63-68 and 70-71) screens data generated for each breath and compares the corresponding coordinated image with previous images generated during training sessions for alarm conditions to be monitored. Upon occurrence of a similar image, the system signals and alarm (71) and identifies the cause for the critical event.", "Device and method for neural network breathing alarm "]
["Neuronal networks of electronic neurons interconnected via electronic synapses with synaptic weight normalization. The synaptic weights are based on learning rules for the neuronal network, such that a synaptic weight for a synapse determines the effect of a spiking source neuron on a target neuron connected via the synapse. Each synaptic weight is maintained within a predetermined range by performing synaptic weight normalization for neural network stability.", "Synaptic weight normalized spiking neuronal networks "]
["A method of managing an ad hoc communications network of wireless devices or nodes. The network is connected if all nodes can communicate with each other and otherwise partitioned. Partitions are identified by recursively applying a connectivity function to a connectivity matrix representative of the network. The number of times the connectivity function is recursively applied is determined by the network diameter. If the result of the recursive application is a unity matrix, the network is connected; otherwise it is disconnected. Also, if the network diameter exceeds a selected maximum length, the network may be voluntarily partitioned into connected sub-networks by recursively applying the connectivity function a lesser number of times to the connectivity matrix. The lesser number of times is determined by the selected maximum length or maximum allowable number of hops.", "Neural network-based mobility management for self-partition detection and identification of mobile ad hoc radio networks "]
[null, "Robust neural network control system for micro-electro-mechanical system (MEMS) gyroscope based on sliding mode compensation and control method of control system "]
["A computer implemented method, data processing system, and computer program product for monitoring system events and providing real-time response to security threats. System data is collected by monitors in the computing system. The expert system of the present invention compares the data against information in a knowledge base to identify a security threat to a system resource in a form of a system event and an action for mitigating effects of the system event. A determination is made as to whether a threat risk value of the system event is greater than an action risk value of the action for mitigating the system event. If the threat risk value is greater, a determination is made as to whether a trust value set by a user is greater than the action risk value. If the trust value is greater, the expert system executes the action against the security threat.", "Method for controlling risk in a computer security artificial neural network expert system "]
[null, "Contactless distance measurement arrangement using pulse=echo signals - uses neural network to evaluate transition time of transmitted and reflected signal "]
["An arrangement of data cells which stores at least one matrix of data words which are arranged in rows and columns, the matrix being distributed in the arrangement in order to deliver/receive, via a single bus, permuted data words which correspond either to a row or to a column of the matrix. Each data cell is connected to the single bus via series-connected switches which are associated with a respective addressing mode, the switches which address a same word of a same mode being directly controlled by a same selection signal. Circulation members enable the original order of the data on the bus to be restored. An arrangement of this kind is used in a layered neural network system for executing the error backpropagation algorithm.", "Arrangement of data cells and neural network system utilizing such an arrangement "]
["Various technologies described herein pertain to conservatively adapting a deep neural network (DNN) in a recognition system for a particular user or context. A DNN is employed to output a probability distribution over models of context-dependent units responsive to receipt of captured user input. The DNN is adapted for a particular user based upon the captured user input, wherein the adaption is undertaken conservatively such that a deviation between outputs of the adapted DNN and the unadapted DNN is constrained.", "Conservatively adapting a deep neural network in a recognition system "]
[null, "WLAN indoor positioning method for neural network regional training "]
["This invention has an object to provide a practical neural network device. The first neural network device of this invention comprises an input circuit for performing predetermined processing of external input information and generating an input signal, an arithmetic processing circuit for performing an arithmetic operation of the input signal in accordance with a plurality of control parameters and generating an output signal, and a control circuit for controlling the control parameters of the arithmetic processing circuit so that the output signal is set to satisfy a predetermined relationship with the input signal, the control circuit including a first cumulative adder for performing cumulative summation of updating amounts of the control parameters for a plurality of proposition patterns supplied as the input signal during learning, and a second cumulative adder for adding currently used control parameter values to values obtained by the first cumulative adder to obtain new control parameter values.", "Neural network device "]
["A variable weight optical interconnector is disclosed to include a projecting device and an interconnection weighting device remote from the projecting device. The projecting device projects a distribution of interconnecting light beams when illuminated by a spatially-modulated light pattern. The weighting device includes a photosensitive screen provided in optical alignment with the projecting device to independently control the intensity of each projected interconnecting beam to thereby assign an interconnection weight to each such beam. Further in accordance with the present invention, a highly-interconnected optical neural network having learning capability is disclosed as including a spatial light modulator, a detecting device, an interconnector according to the present invention, and a device responsive to detection signals generated by the detecting device to modify the interconnection weights assigned by the photosensitive screen of the interconnector.", "Optical interconnector and highly interconnected, learning neural network incorporating optical interconnector therein "]
["A data transmission method includes a step which modulates maximal-sequence codes which are phase shifted by different phase shift quantities based upon plural data for transmission, a step which then convolutes the modulated maximal-sequence codes to obtain transmission data, and afterwards, a step which receives the transmission data and obtains a cross-correlation of the transmission data with a maximal-sequence code which has been phase shifted by the same as the maximal-sequence code which corresponds to the data for transmission. A data transmission method also includes a step which modulates maximal-sequence codes which are phase shifted by different phase shift quantities based on plural data for transmission, then a step which convolutes the modulated maximal-sequence codes to obtain transmission data. The method previously obtains a time sequence code based on a weighting factor for all data and maximal-sequence codes which are phase shifted with corresponding phase shifting quantities, then obtains a cross-correlation of the transmission data and the time sequence code.", "Data transmission and apparatus, data processing apparatus and a neural network which utilize phase shifted, modulated, convolutable pseudo noise "]
["A method of obtaining one or more components from an image may include normalizing and pre-processing the image to obtain a processed image. Features may be extracted from the processed image. Neural-network-based regression may then be performed on the set of extracted features to predict the one or more components. These techniques may be applied, for example, to the problem of extracting and removing bone components from radiographic images, which may be thoracic (lung) images.", "Feature based neural network regression for feature suppression "]
[null, "Neural network short-term electric load prediction based on sample dynamic organization and temperature compensation "]
["A multiplier and a neural network synapse capable of removing nonlinear current using current mirror circuits. The multiplier produces a linear current by using MOS transistors operating in a nonsaturation region. The multiplier includes a first current mirror including a plurality of MOS transistors to form a first current and a second current mirror including a plurality of MOS transistors to form a second current, wherein the second current mirror is coupled in parallel to the first current mirror. As a result, the multiplier outputs an output current by subtracting a second current from said first current.", "Multiplier and neural network synapse using current mirror having low-power mosfets "]
["A pattern recognition device has a DP matching section. The DP matching section performs frequency expansion DP matching to a standard pattern and a characteristic pattern obtained from input voice waveform to obtain a DP score and DP path pattern. It is determined by means of a category identification neural network using the DP path pattern obtained from the DP matching section whether a category of the standard pattern and a category of the characteristic pattern are the same, and a determination result corresponding to the degree of identification is obtained. A normalized DP score, which is the DP score normalized for individual differences within a required range, is then obtained in a divider by compensating the DP score using the determination result.", "Pattern recognition device using a neural network "]
[null, "Automatic insect image identification method based on depth convolutional neural network "]
["The adaptation and personalization of a deep neural network (DNN) model for automatic speech recognition is provided. An utterance which includes speech features for one or more speakers may be received in ASR tasks such as voice search or short message dictation. A decomposition approach may then be applied to an original matrix in the DNN model. In response to applying the decomposition approach, the original matrix may be converted into multiple new matrices which are smaller than the original matrix. A square matrix may then be added to the new matrices. Speaker-specific parameters may then be stored in the square matrix. The DNN model may then be adapted by updating the square matrix. This process may be applied to all of a number of original matrices in the DNN model. The adapted DNN model may include a reduced number of parameters than those received in the original DNN model.", "Low-footprint adaptation and personalization for a deep neural network "]
[null, "Adaptive control system based on radial basis function (RBF) neural network sliding mode control for micro-electromechanical system (MEMS) gyroscope "]
["A divider using neural network configurations comprises a subtractor, a selecting means, a first latch means, a second latch means, a shift register and a control means. The subtractor of the divider comprises plural inverters and plural 3-bit full-adders which are composed of four output lines, an input synapse group, a first bias synapse group, a second bias synapse group, a feedback synapse group, a neuron group and an inverter group.", "Divider using neural network "]
[null, "Trademark detection method based on convolutional neural network "]
["A robot controller for an articulated robot, in which a joint angle vector indicating target joint angles of respective joints of the robot is calculated based on a target position matrix indicating a desired target position, and the robot is controlled ill accordance with the joint angle vector. The robot controller is further provided with a neural network for compensating the target position so as to reduce the positioning error. The learning operation of the neural network is repeatedly carried out at predetermined intervals during the operation of the robot. The target position may be preliminarily compensated using a mathematical model.", "Method and apparatus for controlling a robot using a neural network "]
["A recognition system is disclosed, including a representation of an object in terms of its constituent parts that is translationally invariant, and which provides scale invariant recognition. The system further provides effective recognition of patterns that are partially present in the input signal, or that are partially occluded, and also provides an effective representation for sequences within the input signal. The system utilizes dynamically determined, context based expectations, for identifying individual features/parts of an object to be recognized. The system is computationally efficient, and capable of highly parallel implementation, and further includes a mechanism for improving the preprocessing of individual sections of an input pattern, either by applying one or more preprocessors selected from a set of several preprocessors, or by changing the parameters within a single preprocessor.", "Feed forward feed back multiple neural network with context driven recognition "]
[null, "Analogue circuit fault diagnosis neural network method based on particle swarm algorithm "]
["A self-organizable neural network expressing unit includes a plurality of neuron units electronically expressing nerve cell bodies, and a plurality of synapse expressing units electronically expressing synapses for coupling neuron units through programmed coupling strengths represented by synapse load values, and a control circuit for supplying a pattern of random number data as an educator data. When the pattern of random number data is generated, the neural network expressing unit carries out correction of synapse load values as in a learning mode of operation using the pattern of random number data as an educator data. The memorized internal states in the neural network expressing unit is reinforced based on a faded memory thereof, and the synapse load values are precisely maintained for a long time, resulting in a reliable neural network expressing unit.", "Neural network expressing apparatus including refresh of stored synapse load value information "]
["A cellular automata neural network method for process modeling of film-substrate interactions utilizes a cellular automaton system having variable rules for each cell. The variable rules describe a state change algorithm for atoms or other objects near a substrate. The state change algorithm is used to create a training set of solutions for training a neural network. The cellular automaton system is run to model the film-substrate interactions with the neural network providing the state change solutions in place of the more computationally complex state change algorithm to achieve real-time or near real-time simulations.", "Cellular automata neural network method for process modeling of film-substrate interactions and other dynamic processes "]
["A camera system has a neural network for calibration of image distortion. The neural network learns the conversion from image coordinates with distortion to image coordinates with substantially reduced distortion, whereby the neural network provides image coordinates having substantially reduced distortion. In a learning process of the neural network, a relatively simple camera model is used to provide an instruction signal to the neural network according to sample data provided from the real camera.", "Camera system with neural network compensator for measuring 3-D position "]
["The sinter production in compliance with the standards is of a fundamentally economical importance for the steel industry because the blast furnace productivity depends on it and, consequently, for the whole plant productivity. Although the improvements of the sinterings have significant savings of great economical and ecological importance, such as the use of the mine rejects and the viabilization of the mines whose ores tend to produce great numbers of strips in their processes of milling and stonebreaking, the thermodynamics of the sinter process require the pellet layers to be sintered have the level kept within strict limits, which, if not obeyed, causes stops of slow recuperation and material non-compliance, implying in reprocessing and a series of productivity losses. The big problem of the State of the Art that this patent comes to advance is that the traditional controls of the hopper (5) level, the sinter machine (6) feeder, has a response time of about 250 seconds, whfch is too long for a continuous and safe operation. The 'PROCESS USING ARTIFICIAL NEURAL NETWORK FOR PREDICTIVE CONTROL IN SINTER MACHINE', object of this patent, has a specific software as its neuro-fuzzy artificial intelligence core supported by preferably the tools MATLAB and ADALINE, being able, however, to use countless other tools and platforms of the ANN, as the ANN is trained to predict the filling level of the hopper (5) 250 seconds or more ahead, for the case of its specific application. The Artificial Neural Network was trained with the pieces of information of the process such as the weight of the materials (10) fed by the feeder silos of the pellets (2), the material density (11 ), the volume of the production by time unit (12), which, as they are sent to the specific software, allow the control of the system with an advance of 250 seconds, or more, and this specific software (9) provides the interfaces (13) to the control panels and it relates with the database to allow a continuous learning, since the ANN can operate with values of variables that have not been provided to it during its training process.", "Process using artificial neural network for predictive control in sinter machine "]
["A sensor arrangement (1) comprising at least one measuring coil (2), at least one voltage source (3) for the measuring coil (2), and an evaluation unit (4) with means for detecting, processing, and evaluating measured signals. This sensor arrangement (1) is used to measure distances and thicknesses substantially independently of the material involved, without the user having to know the physicomathematical relations between the influencing quantities and the measured values. In order to evaluate the measured signals, the evaluation unit (4) of the sensor arrangement comprises a neural network (5) with an input layer, at least one hidden layer, an output layer, and connection weights for the individual layers. The connection weights are determined and stored in a learning phase by measurements taken on a plurality of different suitable learning objects with known actual values.", "Sensor arrangement including a neural network and detection method using same "]
["A spectrophotometric neural network assists in the color matching process for pigments. The self-teaching system provides an accurate means of automating the determination of pigment recipes for color matching. The neural network uses the reflectivity values (R1-R5) obtained through a spectrophotometric scan as input to the process. Full automation of such a system employs the interface of a neural network, a computer, a spectrophotometer, and pigment-mixing equipment.", "Spectrophotometric neural network "]
["In an apparatus for multiplexed operation of multi-cell neural network, the reference vector component values are stored as differential values in pairs of floating gate transistors. A long-tail pair differential transconductance multiplier is synthesized by selectively using the floating gate transistor pairs as the current source. Appropriate transistor pairs are multiplexed into the network for forming a differential output current representative of the product of the input vector component applied to the differential input and the stored reference vector component stored in the multiplexed transistor pair that is switched into the multiplier network to function as the differential current source. Pipelining and output multiplexing is also described in other preferred embodiments for increasing the effective output bandwidth of the network.", "Neural network with multiplexed snyaptic processing "]
["Automatic white balancing and/or autoexposure as useful in a digital camera extracts color channel gains from comparisons of image colors with reference colors under various color temperature illuminants and/or extracts exposure settings from illuminance mean, illuminance variance, illuminance minimum, and illuminance maximum in areas of an image with a trained neural network.", "Automatic white balancing via illuminant scoring autoexposure by neural network mapping "]
["A system for controlling operation of engine fuel injectors that includes a feed-forward control unit responsive to signals from sensors on the engine for supplying a basic electronic control signal for the injectors. A neural network is connected in parallel with the feed-forward control unit for receiving the sensor signals and multiplying the sensor signals by associated weighting factors. The sensor signals multiplied by the weighting factors are combined to produce a network output signal, which in turn is combined with the basic control signal from the feed-forward control unit to control operation of the fuel injectors. The weighting factors in the neural network are modified as a function of inputs from the engine sensors so as to reduce any errors in the sensor output signals as compared with desired values.", "Control of engine fuel delivery using an artificial neural network in parallel with a feed-forward controller "]
[null, "Artificial neural network-based multi-source gait feature extraction and identification method "]
[null, "Analogue circuit fault diagnosis method based on neural network "]
[null, "Image classification method capable of effectively preventing convolutional neural network from being overfit "]
[null, "Neural network-based method for identifying and classifying visible components in urine "]
["A data processing system based on the concept of a neural network includes a normalizing circuit and driving elements. Each driving element has an output inductor magnetically coupled to an input inductor of the normalizing circuit. In the normalizing circuit, the input inductor is coupled to receive an input signal. The circuit also has a switching circuit responsive to the input signal and a switched inductor energized in response to the switched signal. The switched inductor comprising either a hooked or a spiral inductor.", "Inductively coupled neural network "]
["For inputting a two-dimensional image into an optical neural network apparatus, a primary processing device is used to extract the characteristic feature of an object pattern. Thereafter, compressed information as a result of the above processing is inputted into the input of the all-optical type optical neural network apparatus, that implements parallel processings adaptively through optical computing, at individual points on the input of the same. Therefore, the primary processing device that was capable of dealing with only logical input information until now can process even vague input information by the use of the optical neural network apparatus located on the later stage. On the other hand, the use of the primary processing device on the previous stage of the optical neural network apparatus enables a limited input range of the optical neural network apparatus to be expanded together with the assurance of higher degree processing by inputting into the optical neural network apparatus results of the characteristic feature extraction from an original image.", "Optical neural network apparatus using primary processing "]
[null, "Wind power forecasting method based on genetic algorithm optimization BP neural network "]
["Methods, systems, and apparatus, including computer programs encoded on computer storage media, for training a deep neural network. One of the methods includes training a deep neural network with a first training set by adjusting values for each of a plurality of weights included in the neural network, and training the deep neural network to determine a probability that data received by the deep neural network has features similar to key features of one or more keywords or key phrases, the training comprising providing the deep neural network with a second training set and adjusting the values for a first subset of the plurality of weights, wherein the second training set includes data representing the key features of the one or more keywords or key phrases.", "Transfer learning for deep neural network based hotword detection "]
["A channel equalizer is formed using a self-learning neural network. During a training period, the neural network is taught the channel response function. The network is then used to equalize distortions introduced into signals by the channel. The neural network may be a Boltzmann Machine type of neural network comprising neurons arranged in an input layer, a hidden layer, and an output layer. The neurons are interconnected by bidirectional symmetric weighted synapses. Each neuron is preferably implemented by an analog integrated circuit. Direct communication between the input and output layers helps in faster channel acquisition. The scheme can very easily be extended to multilevel and multisymbol modulation schemes such as QAM and PSK.", "Adaptive equalizer using self-learning neural network "]
["A method of creating and using a neural network model for wellbore operations is disclosed. The method, in one aspect, may include defining a plurality of a wellbore parameter; calculating a plurality of output values of a tool operating parameter using the plurality of values of the wellbore parameter as input to a preexisting model; and obtaining a neural network model by using the plurality of values of the wellbore parameter and the calculated plurality of output values of the tool operating parameter. The neural network may be utilized for any suitable wellbore operation, including in conjunction with a drilling assembly for drilling a wellbore.", "Method of Training Neural Network Models and Using Same for Drilling Wellbores "]
[null, "Network resource personalized recommended method based on ultrafast neural network "]
["A waveform equalizer for reducing distortion of a digital signal produced from a digital data recording and playback system or transmission system is formed of a neural network having fixed weighting coefficients. Respective values for the coefficients are established by generating a corresponding simulated neuron network, by software implementation using a computer, and by executing a neuron network learning operation using input values obtained from a distorted digital signal and teaching values obtained from an original digital signal which resulted in the distorted digital signal.", "Waveform equalizer apparatus formed of neural network, and method of designing same "]
["A learning framework and methods of machine learning are disclosed. Specifically, an Analytical Neural Network Intelligent Interface (ANNII) is disclosed that includes the ability to analyze incoming data in substantially real-time and determine whether or not the data is statistically anomalous data. Learning models can then be updated depending upon whether or not the data is determined to be statistically anomalous data or not.", "Analytical neural network intelligent interface machine learning method and system "]
["Handwriting recognition techniques employing a personalized handwriting recognition engine. The recognition techniques use examples of an individual's previous writing style to help recognize new pen input from that individual. The techniques also employ a shape trainer to select samples of an individual's handwriting that accurately represent the individual's writing style, for use as prototypes to recognize subsequent handwriting from the individual. The techniques also alternately or additionally employ an intelligent combiner to combine the recognition results from the personalized recognition engine and the conventional recognition engine (or engines). The combiner may use a comparative neural network to combine the recognition results from multiple recognition engines. The combiner alternately may use a rule-based system based on prior knowledge of different recognition engines.", "Handwriting recognition using a comparative neural network "]
["A simple format is disclosed and referred to as Elementary Network Description (END). The format can fully describe a large-scale neuronal model and embodiments of software or hardware engines to simulate such a model efficiently. The architecture of such neuromorphic engines is optimal for high-performance parallel processing of spiking networks with spike-timing dependent plasticity. The software and hardware engines are optimized to take into account short-term and long-term synaptic plasticity in the form of LTD, LTP, and STDP.", "Systems and methods for providing a neural network having an elementary network description for efficient implementation of event-triggered plasticity rules "]
["A technique for machine learning, such as supervised artificial neural network learning includes receiving data and checking the dimensionality of the read data and reducing the dimensionality to enhance machine learning performance using Principal Component Analysis methodology. The technique further includes specifying the neural network architecture and initializing weights to establish a connection between read data including the reduced dimensionality and the predicted values. The technique also includes performing supervised machine learning using the specified neural network architecture, initialized weights, and the read data including the reduced dimensionality to predict values. Predicted values are then compared to a normalized system error threshold value and the initialized weights are revised based on the outcome of the comparison to generate a learnt neural network having a reduced error in weight space. The learnt neural network is validated using known values and is then used for predicting values.", "Combinatorial approach for supervised neural network learning "]
