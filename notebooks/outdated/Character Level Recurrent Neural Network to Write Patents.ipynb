{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Writing Patents Using a Recurrent Neural Network\n",
    "\n",
    "The purpose of this notebook is to develop a recurrent neural network which can be used to write patent abstracts. Although this is mostly meant as a simple example, the idea of recurrent neural networks is powerful and can be usde for real purposes such as generating text similar to a corpus, machine translation, and supervised learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "CHUNK_SIZE = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6382"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in os.listdir('../data/patents_parsed/'):\n",
    "    with open(f'../data/patents_parsed/{file}', 'rt') as fin:\n",
    "        data.append([json.loads(l) for l in fin])\n",
    "        \n",
    "        \n",
    "data = list(chain(*data))\n",
    "data = [r for r in data if r[0] is not None]\n",
    "data = [r for r in data if len(r[0]) >= 200]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = [len(x[0]) for x in data]\n",
    "min(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Artificial intelligence system for item analysis for rework shop orders ',\n",
       " 'A computer implemented method facilitates the capability for shop re-work orders to be effectively scheduled, knowing the time and location of item availability that is needed to correct the problem found in the re-work shop orders. The system automatically identifies alternate components or items that can be used in the shop orders and provides realistic shipping dates so that the re-work shop orders can be scheduled. If components or items are not available, the system provides feedback to the material planning system to re-plan items using traditional material planning systems such as the MRP (material requirement planning) systems and provide projected shipping dates so that re-work orders can be scheduled.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][1], data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts = [d[0] for d in data]\n",
    "titles = [d[1] for d in data]\n",
    "\n",
    "chars = []\n",
    "for abstract in abstracts:\n",
    "    for ch in abstract:\n",
    "        chars.append(ch)\n",
    "        \n",
    "chars = set(chars)\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 'V')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "char_to_idx['a'], idx_to_char[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_rnn_model(num_chars, num_layers, num_nodes = 512, dropout = 0.1):\n",
    "    # Take in a sequence of one-hot encoded characters\n",
    "    input_layer = Input(shape = (None, num_chars), name = 'input')\n",
    "    prev = input_layer\n",
    "    \n",
    "    # Add an LSTM cell for each layer\n",
    "    for i in range(num_layers):\n",
    "        lstm = LSTM(num_nodes, return_sequences = True, name = f'lstm_layer_{i}')(prev)\n",
    "        if dropout:\n",
    "            prev = Dropout(dropout)(lstm)\n",
    "        else:\n",
    "            prev = lstm\n",
    "            \n",
    "    # For each time step find the most likely character - one time step considers up to current character\n",
    "    # Time Distributed applies same layer to all time steps (first dimension)\n",
    "    dense = TimeDistributed(Dense(num_chars, name = 'dense',\n",
    "                             activation = 'softmax'))(prev)\n",
    "    model = Model(inputs = [input_layer], outputs = [dense])\n",
    "    \n",
    "    # Compile with categorical loss\n",
    "    model.compile(loss = 'categorical_crossentropy', \n",
    "                  optimizer = RMSprop(lr=0.01), \n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, None, 147)         0         \n",
      "_________________________________________________________________\n",
      "lstm_layer_0 (LSTM)          (None, None, 512)         1351680   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "lstm_layer_1 (LSTM)          (None, None, 512)         2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 147)         75411     \n",
      "=================================================================\n",
      "Total params: 3,526,291\n",
      "Trainable params: 3,526,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = char_rnn_model(len(chars), 2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Avatars, methods, apparatuses, computer program products, devices and systems are described that carry out identifying at least one instance of media content as a prospective cohort-linked attribute; presenting to at least one member of a population the at least one instance of media content; measuring at least one physiologic activity of the at least one member of the population, the at least one physiologic activity proximate to the at least one instance of media content; associating the at least one physiologic activity with at least one mental state; and specifying at least one population cohort based on the at least one mental state.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(abstracts, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def data_generator(text, char_to_idx, batch_size, chunk_size):\n",
    "    X = np.zeros((batch_size, chunk_size, len(char_to_idx)))\n",
    "    y = np.zeros((batch_size, chunk_size, len(char_to_idx)))\n",
    "    \n",
    "    chunk_size_original = chunk_size\n",
    "    \n",
    "    # Generator yields samples\n",
    "    while True:\n",
    "        # Batch size is number of samples to use\n",
    "        for row in range(batch_size):\n",
    "            \n",
    "            # Choose a random abstract\n",
    "            sample = random.sample(text, 1)[0]\n",
    "            \n",
    "            # Choose a random starting index\n",
    "            idx = random.randrange(len(sample) - chunk_size - 1)\n",
    "\n",
    "            # Empty array to hold a chunk, chunk size is number of characters to extract\n",
    "            chunk = np.zeros((chunk_size + 1, len(char_to_idx)))\n",
    "            \n",
    "            # Need to find one more than chunk size to make labels\n",
    "            for i in range(chunk_size + 1):\n",
    "                chunk[i, char_to_idx[sample[idx + i]]] = 1\n",
    "                \n",
    "            # Features are all characters except for last\n",
    "            X[row, :, :] = chunk[:chunk_size]\n",
    "            # Labels are all characters except for first\n",
    "            y[row, :, :] = chunk[1:]\n",
    "            \n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, ys = next(data_generator(abstracts, char_to_idx, 512, chunk_size = 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 80, 147)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 147)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = Xs[1]\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'private system or a residential system. A second step is executed upon a termina'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "\n",
    "for row in sample:\n",
    "    x.append(idx_to_char[np.argmax(row)])\n",
    "''.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'private system or a residential system. A second step is executed upon a termina'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = []\n",
    "\n",
    "for row in sample:\n",
    "    y.append(idx_to_char[np.argmax(row)])\n",
    "''.join(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label is a shifted forward version of the features. At each feature, we are teaching the network to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor = 'loss', min_delta = 0.03, patience = 5),\n",
    "             ModelCheckpoint(filepath = '../models/first_rnn.h5', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5332623"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "all_text = list(chain(*abstracts))\n",
    "len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      " - 154s - loss: 3.3441 - acc: 0.1105\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/callbacks.py:432: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 151s - loss: 3.0639 - acc: 0.1302\n",
      "Epoch 3/40\n",
      " - 151s - loss: 3.0454 - acc: 0.1379\n",
      "Epoch 4/40\n",
      " - 151s - loss: 2.8410 - acc: 0.2041\n",
      "Epoch 5/40\n",
      " - 151s - loss: 1.7560 - acc: 0.5275\n",
      "Epoch 6/40\n",
      " - 151s - loss: 1.5383 - acc: 0.6020\n",
      "Epoch 7/40\n",
      " - 151s - loss: 1.4581 - acc: 0.6260\n",
      "Epoch 8/40\n",
      " - 151s - loss: 1.4636 - acc: 0.6303\n",
      "Epoch 9/40\n",
      " - 151s - loss: 1.3711 - acc: 0.6517\n",
      "Epoch 10/40\n",
      " - 151s - loss: 1.3461 - acc: 0.6582\n",
      "Epoch 11/40\n",
      " - 151s - loss: 1.4086 - acc: 0.6494\n",
      "Epoch 12/40\n",
      " - 151s - loss: 1.3879 - acc: 0.6543\n",
      "Epoch 13/40\n",
      " - 151s - loss: 1.4522 - acc: 0.6433\n",
      "Epoch 14/40\n",
      " - 151s - loss: 1.4008 - acc: 0.6540\n"
     ]
    }
   ],
   "source": [
    "train_gen = data_generator(abstracts, char_to_idx, 256, chunk_size=CHUNK_SIZE)\n",
    "\n",
    "h = model.fit_generator(generator=train_gen, epochs = 40, callbacks = callbacks,\n",
    "                        steps_per_epoch = 2 * len(all_text) / (BATCH_SIZE * CHUNK_SIZE),\n",
    "                        verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/first_rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randrange(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def generate_output(model, text, start_index = 2, diversity = None, amount = 400):\n",
    "    \n",
    "    if start_index is None:\n",
    "        start_index = random.randint(0, CHUNK_SIZE)\n",
    "        \n",
    "    sample = random.sample(text, 1)[0]\n",
    "    generated = sample[start_index: start_index + CHUNK_SIZE]\n",
    "    yield generated + '#'\n",
    "    \n",
    "    for i in range(amount):\n",
    "        x = np.zeros((1, len(generated), len(chars)))\n",
    "        for t, char in enumerate(generated):\n",
    "            x[0, t, char_to_idx[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x, verbose = 0)[0]\n",
    "    \n",
    "        if diversity is None:\n",
    "            next_index = np.argmax(preds[len(generated) - 1])\n",
    "            \n",
    "        else:\n",
    "            preds = np.array(preds[len(generated) - 1]).astype(np.float64)\n",
    "            preds = np.log(preds) / diversity\n",
    "            exp_preds = np.exp(preds)\n",
    "            preds = exp_preds / np.sum(exp_preds)\n",
    "            probas = np.random.multinomial(1, preds, 1)\n",
    "            next_index = np.argmax(preds)\n",
    "            \n",
    "        next_char = idx_to_char[next_index]\n",
    "        yield next_char\n",
    "        \n",
    "        generated += next_char\n",
    "    return generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method and apparatus for speeding and enhancing the \"learning\" function of a computer configured as a multilayered, feed format artificial neural network using logistic functions a#nd a set of patterns of the set of training data and the set of training data is determined based on the training data sets and the set of training data and the second set of network parameters are de"
     ]
    }
   ],
   "source": [
    "for ch in generate_output(model, abstracts, diversity = 100, amount = 200):\n",
    "    sys.stdout.write(ch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "development platform for developing a skill for a persistent companion device (PCD) includes an asset development library having an application programming interface (API) configur#ed to provide a communication component and a computer system and a processing system for controlling a processing device for controlling a processing device for controlling a processing device and a computer system in a computer system in a computer system in a computer system in a computer system and a processing system for computing a processing system including a plurality of sensors and a sec"
     ]
    }
   ],
   "source": [
    "for ch in generate_output(model, abstracts):\n",
    "    sys.stdout.write(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we got a small glimpse at the abilities of recurrent neural networks. Using just a few thousands patents and a basic neural network, we were able to teach a machine to produce reasonable outputs of patent abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
